# sft_tester2.py
# -*- coding: utf-8 -*-
import os
import sys
import io
import logging
import inspect
from typing import Optional, Dict, Any, List, Union, Tuple
import re

# ç¡®ä¿stdoutå’Œstderrä½¿ç”¨UTF-8ç¼–ç 
os.environ['PYTHONIOENCODING'] = 'utf-8'

# å¦‚æœstdout/stderrä¸æ˜¯UTF-8ï¼Œåˆ™é‡æ–°åŒ…è£…
if hasattr(sys.stdout, 'buffer') and (not hasattr(sys.stdout, 'encoding') or sys.stdout.encoding != 'utf-8'):
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)
    except (AttributeError, ValueError):
        pass

import numpy as np
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM


# ==========================================
# 1. è‡ªå®šä¹‰é¢œè‰²æ ¼å¼åŒ–å™¨
# ==========================================
class ColoredFormatter(logging.Formatter):
    """æ—¶é—´è“è‰²ï¼Œçº§åˆ«/åç§°å˜è‰²ï¼Œæ¶ˆæ¯åŸè‰²"""
    blue = "\x1b[34;20m"    # è“è‰²ç”¨äºæ—¶é—´
    green = "\x1b[32;20m"   # ç»¿è‰²ç”¨äº INFO
    yellow = "\x1b[33;20m"  # é»„è‰²ç”¨äº WARNING
    red = "\x1b[31;20m"     # çº¢è‰²ç”¨äº ERROR
    bold_red = "\x1b[31;1m" # ç²—ä½“çº¢ç”¨äº CRITICAL
    reset = "\x1b[0m"
    
    LEVEL_COLORS = {
        logging.INFO: green,
        logging.WARNING: yellow,
        logging.ERROR: red,
        logging.CRITICAL: bold_red
    }

    def format(self, record):
        level_color = self.LEVEL_COLORS.get(record.levelno, self.reset)
        # æå–æœ€åçš„ç±»å/æ¨¡å—å
        short_name = record.name.split('.')[-1]
        
        # æ„é€ æ ¼å¼ï¼š[æ—¶é—´](è“è‰²) çº§åˆ« [åç§°](å˜è‰²): æ¶ˆæ¯(åŸè‰²)
        log_fmt = (
            f"{self.blue}[%(asctime)s]{self.reset} "
            f"{level_color}%(levelname)s [{short_name}]:{self.reset} "
            f"%(message)s"
        )
        
        formatter = logging.Formatter(log_fmt, datefmt='%Y-%m-%d %H:%M:%S')
        return formatter.format(record)

# ==========================================
# 2. UTF-8 ç¼–ç å®‰å…¨å¤„ç†å™¨
# ==========================================
class UTF8StreamHandler(logging.StreamHandler):
    def __init__(self, stream=None):
        super().__init__(stream or sys.stdout)
    
    def emit(self, record):
        try:
            msg = self.format(record)
            if hasattr(self.stream, 'buffer'):
                self.stream.buffer.write(msg.encode('utf-8', errors='replace'))
                self.stream.buffer.write(b'\n')
                self.flush()
            else:
                self.stream.write(msg + '\n')
                self.flush()
        except Exception:
            self.handleError(record)

def init_logger(logger_name="SFT-Tester"):
    """
    åˆå§‹åŒ–å…¨å±€æ—¥å¿—ç³»ç»Ÿï¼Œå¹¶è¿”å› SFT-Tester ä¸“ç”¨çš„ logger
    """
    # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨ (Root Logger) ä»¥æ•è·æ‰€æœ‰æ¨¡å—çš„è¾“å‡º
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    
    # æ¸…ç†æ—§çš„ Handlersï¼Œé˜²æ­¢é‡å¤æ‰“å°
    if root.hasHandlers():
        root.handlers.clear()
        
    # åˆ›å»ºå¹¶æ·»åŠ å½©è‰² UTF-8 å¤„ç†å™¨
    console_handler = UTF8StreamHandler(sys.stdout)
    console_handler.setFormatter(ColoredFormatter())
    root.addHandler(console_handler)
    
    # å±è”½ç¬¬ä¸‰æ–¹åº“çš„å¹²æ‰°
    logging.getLogger("rdkit").setLevel(logging.ERROR)
    logging.getLogger("transformers").setLevel(logging.WARNING)
    
    return logging.getLogger(logger_name)

logger = init_logger()


from modules.mol_aware_lm import MolAwareCausalLM

# LDMol æ”¯æŒ
LDMOL_ENABLED = True # LDMol ä½¿ç”¨å¼€å…³ 
from modules.ldmol_component import LDMolInferer # LDMol çš„é»˜è®¤é…ç½®ä½äº modules/ldmol_component/ldmol_config.yaml 

def _get_model_device(model_llm: torch.nn.Module) -> torch.device:
    """å…¼å®¹ device_map="auto" çš„åœºæ™¯ï¼šè¾“å…¥æ”¾åˆ°ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨ device"""
    return next(model_llm.parameters()).device

def _encode_with_chat_template(
    tokenizer,
    system_msg: str,
    user_msg: str,
    enable_thinking: bool = False,
):
    """
    å®˜æ–¹é£æ ¼ï¼šä¼˜å…ˆ apply_chat_template(tokenize=True, return_tensors="pt")
    å¦‚æœ tokenizer ä¸æ”¯æŒï¼Œå°±è¿”å› None è®©å¤–å±‚ fallbackã€‚
    """
    if not hasattr(tokenizer, "apply_chat_template") or getattr(tokenizer, "chat_template", None) is None:
        return None

    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ]

    # ä¸è¦ç¡¬å¡ enable_thinkingï¼›åªæœ‰ç­¾åæ”¯æŒæ‰ä¼ 
    extra_kwargs = {"enable_thinking": enable_thinking}

    try:
        enc = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
            **extra_kwargs
        )
        # HF çš„è¡Œä¸ºï¼šå¯èƒ½ç›´æ¥è¿”å› input_idsï¼Œä¹Ÿå¯èƒ½è¿”å› BatchEncoding
        if isinstance(enc, torch.Tensor):
            return {"input_ids": enc, "attention_mask": torch.ones_like(enc)}
        return enc
    except Exception:
        return None


def _encode_fallback_plain(tokenizer, system_msg: str, user_msg: str):
    """æœ€é€šç”¨çš„å…œåº•ï¼šçº¯æ–‡æœ¬å¯¹è¯ï¼Œä¸æ‹¼ä»»ä½•ç‰¹æ®Š token"""
    text = f"System: {system_msg}\n\nUser: {user_msg}\n\nAssistant:"
    return tokenizer(text, return_tensors="pt", add_special_tokens=True)

class MolAwareGenerator2:
    """
    åŸºäº tester.py çš„ç»“æ„ï¼Œæ•´åˆ sft_tester.py çš„åŠ è½½é€»è¾‘å’Œ mlp_inference.py çš„ token åˆ†ç±»åŠŸèƒ½ã€‚
    """

    def __init__(self):
        self.model: Optional[MolAwareCausalLM] = None
        self.tokenizer: Optional[AutoTokenizer] = None
        self.device: str = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.loaded_cfg: Dict[str, Any] = {}
        self.use_multi_gpu: bool = False  # æ˜¯å¦ä½¿ç”¨å¤šGPU
        # æ˜¯å¦ä½¿ç”¨ Llama 3.x é£æ ¼çš„å¯¹è¯æ¨¡æ¿ï¼ˆé€šè¿‡ vocab ä¸­æ˜¯å¦å«æœ‰ header token ç²—ç•¥åˆ¤æ–­ï¼‰
        self.is_llama_chat_format: bool = False
        # æ˜¯å¦å¯ç”¨ thinking æ¨¡å¼ï¼ˆé»˜è®¤å…³é—­ï¼‰
        self.enable_thinking: bool = False
        # LDMol ç»„ä»¶
        self.ldmol = None   

    # ------------------------ å†…éƒ¨å·¥å…· ------------------------
    def _ensure_special_tokens(self):
        """
        åªæ ¡éªŒç‰¹æ®Š token æ˜¯å¦å­˜åœ¨ï¼›æ¨ç†æœŸç¦æ­¢æ–°å¢ï¼Œä»¥å…æ’•è£‚ embedding æƒé‡ã€‚
        """
        assert self.tokenizer is not None
        vocab = self.tokenizer.get_vocab()

        # æ ‡è®°å½“å‰ tokenizer æ˜¯å¦æ”¯æŒ Llama é£æ ¼çš„ header tokenï¼Œç”¨äºåç»­é€‰æ‹©å¯¹è¯æ¨¡æ¿
        self.is_llama_chat_format = (
            "<|start_header_id|>" in vocab and "<|end_header_id|>" in vocab
        )

        # åªéœ€è¦æ£€æŸ¥ <mol> tokenï¼Œå…¶ä»–ç‰¹æ®Šå¯¹è¯ token äº¤ç»™å„æ¨¡å‹è‡ªèº«çš„ tokenizer/chat_template å¤„ç†
        needed = ["<mol>"]
        missing = [t for t in needed if t not in vocab]
        # if missing:
        #     raise RuntimeError(
        #         f"[vocab-mismatch] æ¨ç†æœŸç¦æ­¢æ–°å¢ tokenã€‚ç¼ºå¤±: {missing}ã€‚"
        #         f"è¯·ç¡®ä¿å¯¼å‡ºçš„ tokenizer å·²åŒ…å«è¿™äº› tokenã€‚"
        #     )

        # å…œåº• eos/bos/pad
        if getattr(self.tokenizer, "eos_token_id", None) is None:
            eot_id = self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
            if eot_id is not None and eot_id >= 0:
                self.tokenizer.eos_token = "<|eot_id|>"

        if getattr(self.tokenizer, "eos_token_id", None) is None:
            try_ids = [
                getattr(self.tokenizer, "eos_token_id", None),
                getattr(self.tokenizer, "sep_token_id", None),
                getattr(self.tokenizer, "cls_token_id", None),
                getattr(self.tokenizer, "bos_token_id", None),
            ]
            try_ids = [t for t in try_ids if t is not None]
            self.tokenizer.eos_token_id = try_ids[0] if try_ids else 0

        if self.tokenizer.pad_token is None:
            if isinstance(self.tokenizer.eos_token, str):
                self.tokenizer.pad_token = self.tokenizer.eos_token
            else:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        # Llama ç­‰ decoder-only æ¨¡å‹åœ¨æ¨ç†/æ‰¹é‡åœºæ™¯ä¸‹éœ€è¦å·¦ä¾§ padding
        self.tokenizer.padding_side = "left"

    def _sync_vocab_and_embeddings(self, strict: bool = True):
        """
        æ ¡éªŒ tokenizer ä¸æ¨¡å‹çš„è¯è¡¨å¤§å°æ˜¯å¦ä¸€è‡´ã€‚
        """
        assert self.model is not None and self.tokenizer is not None
        v_tok = len(self.tokenizer)
        v_model = self.model.llm.get_input_embeddings().weight.size(0)

        if v_tok == v_model:
            self.model.llm.config.pad_token_id = self.tokenizer.pad_token_id
            self.model.llm.config.eos_token_id = self.tokenizer.eos_token_id
            self.model.llm.config.bos_token_id = self.tokenizer.bos_token_id
            return

        if strict:
            # å¯¹äºéƒ¨åˆ†æ¨¡å‹ï¼ˆå¦‚ Qwen ç³»åˆ—ï¼‰ï¼ŒHF å®˜æ–¹æƒé‡ä¸­ tokenizer ä¸ config.vocab_size
            # æœ¬èº«å°±å¯èƒ½ä¸ä¸€è‡´ï¼Œè¿™é‡Œè‡ªåŠ¨å¯¹é½è€Œä¸æ˜¯ç›´æ¥æŠ¥é”™ã€‚
            try:
                print(f"[vocab-mismatch] tokenizer({v_tok}) != model-emb({v_model})ï¼Œå°è¯•è‡ªåŠ¨ resize_token_embeddings...")
                self.model.llm.resize_token_embeddings(v_tok)
                self.model.llm.config.vocab_size = v_tok
                self.model.llm.config.pad_token_id = self.tokenizer.pad_token_id
                self.model.llm.config.eos_token_id = self.tokenizer.eos_token_id
                self.model.llm.config.bos_token_id = self.tokenizer.bos_token_id
                print(f"[vocab-mismatch] âœ… å·²è‡ªåŠ¨å°† model-emb è°ƒæ•´ä¸º {v_tok}")
                return
            except Exception as e:
                raise RuntimeError(
                    f"[vocab-mismatch] tokenizer({v_tok}) != model-emb({v_model})ï¼Œä¸”è‡ªåŠ¨å¯¹é½å¤±è´¥: {e}"
                )

    # ------------------------ å†…éƒ¨è¾…åŠ©æ–¹æ³• ------------------------
    def _get_system_message(self, task_type: Optional[str], realtime_mol: bool) -> str:
        """æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹© system message"""
        if (task_type == "molecule_generation" or task_type == "i2s") and realtime_mol:
            return "You are a helpful chemist. Generate a detailed description of a molecule based on the user's request. Include information about the molecule's structure, properties, and potential applications. Only output the description."
        elif task_type == "drug_optim" and realtime_mol:
            return "You are a medicinal chemistry assistant. Given an original molecule (SMILES) and its ADMET profile, design a single optimized molecule that best improves the stated ADMET liabilities while preserving potency.\n\nWrite naturally: first give a brief rationale (6-8 sentences) explaining the key modifications and why they help (e.g., solubility, permeability, metabolic stability, hERG, clearance). Then provide exactly one SMILES for your best design in a code block:\n\n```smiles\n<one valid SMILES here>\n```",
            # return "You are a medicinal chemistry assistant. Given an original molecule (SMILES) and its ADMET profile, design an optimization strategy.\n\nFirst, provide a 'Rationale' (6-8 sentences) explaining the medicinal chemistry reasoning, such as bioisosteric replacements or metabolic site blocking to fix ADMET issues while preserving potency.\n\nThen, provide exactly one 'SMILES Description' (1-2 sentences) that explicitly describes the structural change (e.g., 'Replace the terminal methyl group with a trifluoromethyl group'). This description must be wrapped in the specific tag below:\n\n<smiles_description>\n[Your concise modification instruction here]\n</smiles_description>"
        else:
            return "You are a careful chemist. Follow the requested output format exactly. Please avoid duplicate outputs"
    
    def _encode_prompts(
        self,
        prompts: List[str],
        system_msg: str,
        add_dialog_wrapper: bool,
    ) -> Dict[str, torch.Tensor]:
        """ç¼–ç  prompts ä¸º input_ids å’Œ attention_mask"""
        if add_dialog_wrapper:
            # é€æ¡ apply_chat_templateï¼Œå† padding
            encoded_list = []
            for p in prompts:
                enc = _encode_with_chat_template(
                    self.tokenizer, system_msg, p, 
                    enable_thinking=self.enable_thinking
                )
                if enc is None:
                    enc = _encode_fallback_plain(self.tokenizer, system_msg, p)
                encoded_list.append(enc)
            
            # ç»Ÿä¸€ paddingï¼ˆdecoder-only å»ºè®® left paddingï¼Œå·²åœ¨ load é‡Œè®¾ç½®ï¼‰
            input_ids = [e["input_ids"].squeeze(0) for e in encoded_list]
            attn = [e["attention_mask"].squeeze(0) for e in encoded_list]
            batch_enc = self.tokenizer.pad(
                {"input_ids": input_ids, "attention_mask": attn},
                padding=True,
                return_tensors="pt"
            )
        else:
            # ä¸åŒ…å¯¹è¯å°±ç›´æ¥ tokenize
            batch_enc = self.tokenizer(prompts, return_tensors="pt", padding=True, add_special_tokens=True)
        
        return batch_enc
    
    def _prepare_extra_embeddings(
        self,
        extra_embeddings: Union[torch.Tensor, List[torch.Tensor]],
        model_device: torch.device,
        num_prompts: int,
    ) -> Optional[torch.Tensor]:
        """
        å‡†å¤‡é¢å¤–çš„ embeddingï¼Œç”¨äºä½œä¸º inputs_embeds ä¼ å…¥ï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰ã€‚
        ç°åœ¨æ”¯æŒåŒæ—¶ä¼ å…¥ input_ids å’Œ inputs_embedsï¼Œå…¶ä¸­ inputs_embeds ä½œä¸ºé¢å¤– embeddingã€‚
        
        Returns:
            inputs_embeds: [B, N, D] æˆ– Noneï¼ˆå¦‚æœæ²¡æœ‰é¢å¤–çš„ embeddingï¼‰
        """
        if extra_embeddings is None:
            return None
        
        # å¤„ç† extra_embeddingsï¼šå¯èƒ½æ˜¯å•ä¸ª tensor æˆ–åˆ—è¡¨
        if isinstance(extra_embeddings, list):
            extra_emb_list = extra_embeddings
        else:
            extra_emb_list = [extra_embeddings]
        
        # ç¡®ä¿æ•°é‡åŒ¹é…ï¼ˆå¦‚æœåªæœ‰ä¸€ä¸ªï¼Œå¤åˆ¶ç»™æ‰€æœ‰æ ·æœ¬ï¼‰
        if len(extra_emb_list) == 1 and num_prompts > 1:
            extra_emb_list = extra_emb_list * num_prompts
        
        if len(extra_emb_list) != num_prompts:
            raise ValueError(f"extra_embeddings æ•°é‡ ({len(extra_emb_list)}) ä¸ prompts æ•°é‡ ({num_prompts}) ä¸åŒ¹é…")
        
        # å¤„ç†æ¯ä¸ª embeddingï¼Œç¡®ä¿å½¢çŠ¶æ­£ç¡®
        processed_embeds = []
        for emb in extra_emb_list:
            emb = emb.to(model_device)
            if emb.dim() == 1:
                emb = emb.unsqueeze(0)  # [D] -> [1, D]
            elif emb.dim() == 2:
                pass  # å·²ç»æ˜¯ [N, D]
            else:
                raise ValueError(f"extra_embeddings çš„å½¢çŠ¶ä¸æ­£ç¡®: {emb.shape}ï¼ŒæœŸæœ› [D] æˆ– [N, D]")
            processed_embeds.append(emb)
        
        # æ‰¾åˆ°æœ€å¤§é•¿åº¦å¹¶ paddingï¼ˆå¦‚æœéœ€è¦ï¼‰
        max_len = max(emb.shape[0] for emb in processed_embeds)
        if max_len > 1:
            # å¦‚æœé•¿åº¦ä¸ä¸€è‡´ï¼Œéœ€è¦ padding
            padded_embeds = []
            for emb in processed_embeds:
                pad_len = max_len - emb.shape[0]
                if pad_len > 0:
                    # ä½¿ç”¨é›¶å‘é‡è¿›è¡Œ padding
                    pad_emb = torch.zeros(pad_len, emb.shape[1], device=model_device, dtype=emb.dtype)
                    emb = torch.cat([emb, pad_emb], dim=0)
                padded_embeds.append(emb)
            return torch.stack(padded_embeds, dim=0)  # [B, N, D]
        else:
            # æ‰€æœ‰ embedding éƒ½æ˜¯å•ä¸ªå‘é‡ï¼Œç›´æ¥ stack
            return torch.stack(processed_embeds, dim=0)  # [B, 1, D]
    
    def _get_eos_pad_ids(self, eos_token_id: Optional[int]) -> Tuple[Any, int]:
        """è·å– eos å’Œ pad token ID"""
        eos_id = eos_token_id
        if eos_id is None:
            eos_id = self.tokenizer.eos_token_id
        if eos_id is None:
            eos_id = getattr(self.model.llm.config, "eos_token_id", None)
        if isinstance(eos_id, (list, tuple)):
            pass
        elif eos_id is None:
            eos_id = self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
            if eos_id is None or eos_id < 0:
                eos_id = 0
        
        pad_id = self.tokenizer.pad_token_id
        if pad_id is None or pad_id < 0:
            pad_id = eos_id if not isinstance(eos_id, (list, tuple)) else (eos_id[0] if eos_id else 0)
        
        return eos_id, pad_id
    
    def _call_model_generate(
        self,
        input_ids: Optional[torch.Tensor],
        input_embeds: Optional[torch.Tensor],  # é¢å¤–çš„ embeddingï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰
        attention_mask: torch.Tensor,
        realtime_mol: bool,
        verbose_logging: bool,
        max_text_length_for_detection: int,
        gen_kwargs: Dict[str, Any],
    ) -> torch.Tensor:
        """è°ƒç”¨æ¨¡å‹ç”Ÿæˆ
        ç°åœ¨æ”¯æŒåŒæ—¶ä¼ å…¥ input_ids å’Œ input_embedsï¼ˆé¢å¤–çš„ embeddingï¼‰
        - input_ids: ç”¨äº token æ£€æµ‹
        - input_embeds: ä½œä¸ºé¢å¤– embedding æ’å…¥ï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰
        """
        if realtime_mol:
            gen_kwargs["enable_thinking"] = self.enable_thinking
            # åŒæ—¶ä¼ å…¥ input_ids å’Œ inputs_embedsï¼ˆå¦‚æœæœ‰é¢å¤–çš„ embeddingï¼‰
            return self.model.generate(
                input_ids=input_ids,
                inputs_embeds=input_embeds,  # é¢å¤–çš„ embeddingï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰
                attention_mask=attention_mask,
                realtime_mol=True,
                verbose_logging=verbose_logging,
                max_text_length_for_detection=max_text_length_for_detection,
                **gen_kwargs
            )
        else:
            # é realtime_mol æ¨¡å¼ï¼šå¦‚æœåŒæ—¶æœ‰ input_ids å’Œ input_embedsï¼Œä¼˜å…ˆä½¿ç”¨ input_ids
            if input_ids is not None:
                return self.model.llm.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    **gen_kwargs
                )
            elif input_embeds is not None:
                return self.model.llm.generate(
                    inputs_embeds=input_embeds,
                    attention_mask=attention_mask,
                    **gen_kwargs
                )
            else:
                raise ValueError("å¿…é¡»æä¾› input_ids æˆ– input_embeds")
    
    def _decode_generated_ids(
        self,
        out_ids: torch.Tensor,
        prompt_lens: List[int],
        skip_special_tokens: bool,
    ) -> Tuple[List[str], List[str]]:
        """è§£ç ç”Ÿæˆçš„ token IDs ä¸ºæ–‡æœ¬"""
        results = []
        raw_outputs = []
        
        for i in range(out_ids.size(0)):
            start = prompt_lens[i]
            gen_ids = out_ids[i, start:]
            
            text = self.tokenizer.decode(
                gen_ids,
                skip_special_tokens=skip_special_tokens,
                clean_up_tokenization_spaces=True
            ).strip()
            
            results.append(text)
            raw_outputs.append(text)
        
        return results, raw_outputs
    
    def _postprocess_special_tasks(
        self,
        results: List[str],
        task_type: Optional[str],
        realtime_mol: bool,
        verbose_logging: bool,
        src_smiles: Optional[str],
    ) -> List[str]:
        """å¤„ç†ç‰¹æ®Šä»»åŠ¡ï¼ˆmolecule_generation ç­‰ï¼‰"""
        if not results:
            return results
        
        assistant_text = results[0]
        
        # å¦‚æœè¾“å‡ºæ°å¥½å°±æ˜¯å ä½ç¬¦ï¼ˆæ²¡æœ‰å…¶ä»–å†…å®¹ï¼‰ï¼Œè¯´æ˜æ¨¡å‹åªè¾“å‡ºäº†å ä½ç¬¦token
        if assistant_text.strip() in ("<think>", "<think>"):
            assistant_text = ""
            results[0] = assistant_text
        
        # å¦‚æœæ˜¯åˆ†å­ç”Ÿæˆä»»åŠ¡ï¼Œä½¿ç”¨diffusionç”Ÿæˆæœ€ç»ˆåˆ†å­
        if (task_type == "molecule_generation" or task_type == "molecule_editing") and realtime_mol:
            if verbose_logging:
                print(f"\n[Molecule Generation] ğŸ“ LLM ç”Ÿæˆçš„æè¿°:")
                print(f"{assistant_text}")
                print()
            
            assert self.ldmol is not None, "LDMolInferer is not initialized"
            if verbose_logging:
                print(f"[Molecule Generation] ğŸŸ£ å¼€å§‹ä½¿ç”¨ Diffusion ä»æè¿°ç”Ÿæˆ SMILES...")
            try:
                pattern = r"<smiles_description>(.*?)</smiles_description>"
                match = re.search(pattern, assistant_text, re.DOTALL)
                if match:
                    smiles_description = match.group(1).strip()
                else:
                    raise ValueError(f"No smiles description found in the assistant text: {assistant_text}")

                if task_type == "molecule_generation":
                    generated_smiles = self.ldmol.generate_molecule(
                        description=assistant_text,
                        qwen=self.model.llm,
                        qwen_tokenizer=self.tokenizer,
                    )
                elif task_type == "molecule_editing":
                    generated_smiles = self.ldmol.edit_molecule(
                        prompt=assistant_text,
                        src_smiles=src_smiles,
                    )
                    raise NotImplementedError("Molecule editing is not implemented yet")

                if generated_smiles:
                    if verbose_logging:
                        print(f"[Molecule Generation] âœ… Diffusion ç”Ÿæˆ SMILES: {generated_smiles}")
                    results[0] = generated_smiles
                else:
                    if verbose_logging:
                        print(f"[Molecule Generation] âŒ Diffusion æœªèƒ½ç”Ÿæˆ SMILESï¼Œè¿”å›æè¿°")
            except Exception as e:
                if verbose_logging:
                    print(f"[Molecule Generation] âŒ é”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
        else:
            if verbose_logging:
                print(f"[Molecule Generation] âš ï¸  LDMol components ä¸å¯ç”¨ï¼Œè¿”å›æè¿°")
        
        return results

    # ------------------------ å¯¹å¤– API ------------------------
    def load(self, cfg: Dict[str, Any]) -> None:
        """
        cfg ç¤ºä¾‹ï¼š
        {
          "ckpt_dir": "...",  # æ ¹ç›®å½•ï¼Œåº”åŒ…å« llm/ å’Œ extras/ å­ç›®å½•
          "device": "cuda:0",  # å•å¡æ¨¡å¼æ—¶ä½¿ç”¨ï¼Œå¤šå¡æ¨¡å¼å¯å¿½ç•¥
          "device_map": "auto" | None | dict,  # å¤šå¡æ¨¡å¼ï¼šNone=å•å¡, "auto"=è‡ªåŠ¨åˆ†é…, dict=æ‰‹åŠ¨æŒ‡å®š
          "dtype": "bf16" | "fp32",
          "token_classifier_path": "...",  # token classifier æƒé‡è·¯å¾„
        }
        """
        self.loaded_cfg = cfg
        ckpt_dir = cfg["ckpt_dir"]
        self.device = cfg.get("device", self.device)
        
        # æ˜¯å¦å¯ç”¨ thinking æ¨¡å¼ï¼ˆé»˜è®¤å…³é—­ï¼‰
        self.enable_thinking = cfg.get("enable_thinking", False)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¤šGPU
        device_map = cfg.get("device_map", None)
        self.use_multi_gpu = device_map is not None and device_map != "cpu"
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šdevice_mapï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¤šå¼ GPUå¯ç”¨
        if device_map is None and torch.cuda.device_count() > 1:
            # é»˜è®¤ä½¿ç”¨å•å¡ï¼Œä¿æŒå‘åå…¼å®¹
            logger.info(f"æ£€æµ‹åˆ° {torch.cuda.device_count()} å¼ GPUï¼Œä½†æœªå¯ç”¨å¤šå¡æ¨¡å¼ã€‚"
                        f"è¦å¯ç”¨å¤šå¡æ¨ç†ï¼Œè¯·è®¾ç½® device_map='auto'")

        # ä½¿ç”¨ model_init çš„é€»è¾‘æ¥å¤„ç† checkpoint åŠ è½½
        from modules.model_init import (
            init_tokenizer, init_llm, init_model, 
            load_model_weights_from_checkpoint_dir
        )
        from pathlib import Path
        
        # æ£€æŸ¥ checkpoint ç›®å½•ç»“æ„
        ckpt_path = Path(ckpt_dir)
        llm_dir = ckpt_path / "llm"
        extras_dir = ckpt_path / "extras"
        has_llm_dir = llm_dir.exists() and llm_dir.is_dir()
        has_extras_dir = extras_dir.exists() and extras_dir.is_dir()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‹†åˆ† checkpointï¼ˆå¦‚æœæœ‰ pytorch_model.bin æˆ– model.safetensors ä½†æ²¡æœ‰ llm ç›®å½•ï¼‰
        needs_split = False
        if not has_llm_dir:
            bin_path = ckpt_path / "pytorch_model.bin"
            safetensors_path = ckpt_path / "model.safetensors"
            if bin_path.exists() or safetensors_path.exists():
                needs_split = True
                logger.info(f"ğŸ“¦ æ£€æµ‹åˆ°æ··åˆ checkpointï¼Œéœ€è¦æ‹†åˆ†: {ckpt_dir}")
                try:
                    split_script_path = Path(__file__).parent / "scripts" / "ckpt" / "split_llm_extras.py"
                    if split_script_path.exists():
                        import importlib.util
                        spec = importlib.util.spec_from_file_location("split_llm_extras", split_script_path)
                        split_module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(split_module)
                        success = split_module.split_checkpoint(str(ckpt_path), str(ckpt_path))
                        if success:
                            logger.info(f"âœ… Checkpoint æ‹†åˆ†å®Œæˆ")
                            has_llm_dir = (ckpt_path / "llm").exists()
                            has_extras_dir = (ckpt_path / "extras").exists()
                        else:
                            logger.warning(f"âš ï¸ Checkpoint æ‹†åˆ†å¤±è´¥ï¼Œå°è¯•ç»§ç»­åŠ è½½...")
                except Exception as e:
                    logger.warning(f"âš ï¸ è‡ªåŠ¨æ‹†åˆ†å¤±è´¥: {e}")
        
        # åŠ è½½ tokenizer
        # ä¼˜å…ˆä» checkpoint æ ¹ç›®å½•åŠ è½½ï¼ˆtokenizer é€šå¸¸ä¿å­˜åœ¨æ ¹ç›®å½•ï¼‰
        tokenizer_dir = str(ckpt_path)
        tokenizer_loaded = False
        
        # æ£€æŸ¥æ ¹ç›®å½•æ˜¯å¦æœ‰ tokenizer æ–‡ä»¶
        tokenizer_files = ["tokenizer.json", "tokenizer.model", "tokenizer_config.json"]
        has_tokenizer_in_root = any((ckpt_path / f).exists() for f in tokenizer_files)
        
        if has_tokenizer_in_root:
            logger.info(f"ä» checkpoint æ ¹ç›®å½•åŠ è½½ tokenizer: {tokenizer_dir}")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, use_fast=True, trust_remote_code=True)
                tokenizer_loaded = True
            except Exception as e:
                logger.warning(f"ä»æ ¹ç›®å½•åŠ è½½ tokenizer å¤±è´¥: {e}")
        
        # å¦‚æœæ ¹ç›®å½•åŠ è½½å¤±è´¥ï¼Œå°è¯•ä» llm ç›®å½•åŠ è½½
        if not tokenizer_loaded and has_llm_dir:
            tokenizer_dir = str(llm_dir)
            logger.info(f"å°è¯•ä» checkpoint çš„ llm ç›®å½•åŠ è½½ tokenizer: {tokenizer_dir}")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, use_fast=True, trust_remote_code=True)
                tokenizer_loaded = True
            except Exception as e:
                logger.warning(f"ä» llm ç›®å½•åŠ è½½ tokenizer å¤±è´¥: {e}")
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ base_llm_path
        if not tokenizer_loaded:
            base_llm_path = cfg.get("base_llm_path")
            if base_llm_path:
                logger.info(f"å›é€€åˆ° base LLM è·¯å¾„åŠ è½½ tokenizer: {base_llm_path}")
                self.tokenizer = init_tokenizer(base_llm_path, mol_token="<mol>")
            else:
                raise RuntimeError(f"æ— æ³•åŠ è½½ tokenizerï¼Œè¯·æ£€æŸ¥ checkpoint ç›®å½•æˆ–æä¾› base_llm_path")
        
        self._ensure_special_tokens()

        # ç²¾åº¦
        dtype_flag = str(cfg.get("dtype", "bf16")).lower()
        torch_dtype = torch.bfloat16 if (torch.cuda.is_available() and "bf16" in dtype_flag) else torch.float32

        # åŠ è½½æ¨¡å‹
        if has_llm_dir:
            # æœ‰ llm ç›®å½•ï¼Œä¼˜å…ˆä½¿ç”¨ from_pretrained åŠ è½½
            try:
                # å‡†å¤‡ Layer2 é…ç½®
                layer2_config = cfg.get("layer2")
                use_layer2 = cfg.get("train", {}).get("use_layer2", False)
                
                if self.use_multi_gpu:
                    logger.info(f"ä½¿ç”¨å¤šGPUæ¨¡å¼ï¼Œdevice_map={device_map}")
                    self.model = MolAwareCausalLM.from_pretrained(
                        save_directory=str(ckpt_path),
                        tokenizer=self.tokenizer,
                        torch_dtype=torch_dtype,
                        device_map=device_map,
                        layer2_config=layer2_config,
                        use_layer2=use_layer2,
                    )
                else:
                    self.model = MolAwareCausalLM.from_pretrained(
                        save_directory=str(ckpt_path),
                        tokenizer=self.tokenizer,
                        torch_dtype=torch_dtype,
                        device_map=None,
                        layer2_config=layer2_config,
                        use_layer2=use_layer2,
                    ).to(self.device)
            except (ValueError, RuntimeError) as e:
                # å¦‚æœ from_pretrained å¤±è´¥ï¼ˆæ¯”å¦‚ torch ç‰ˆæœ¬é—®é¢˜ï¼‰ï¼Œä½¿ç”¨ model_init é€»è¾‘
                error_msg = str(e)
                if "torch.load" in error_msg or "v2.6" in error_msg or "CVE" in error_msg:
                    logger.warning(f"from_pretrained å¤±è´¥ï¼ˆtorchç‰ˆæœ¬é™åˆ¶ï¼‰: {e}")
                    logger.info("ä½¿ç”¨ model_init é€»è¾‘ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
                else:
                    logger.warning(f"from_pretrained å¤±è´¥: {e}ï¼Œä½¿ç”¨ model_init é€»è¾‘")
                
                # ä½¿ç”¨ model_init é€»è¾‘åŠ è½½
                # å¦‚æœ llm_dir ä¸å­˜åœ¨æˆ–æ²¡æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œå°è¯•ä»æ ¹ç›®å½•åŠ è½½
                actual_llm_dir = str(llm_dir)
                if not has_llm_dir or not os.path.exists(actual_llm_dir):
                    # llm ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ ¹ç›®å½•åŠ è½½
                    actual_llm_dir = str(ckpt_path)
                    logger.info(f"llm ç›®å½•ä¸å­˜åœ¨ï¼Œä»æ ¹ç›®å½•åŠ è½½: {actual_llm_dir}")
                
                # ç›´æ¥ä½¿ç”¨ init_llmï¼Œå®ƒä¼šå¤„ç† torch ç‰ˆæœ¬é™åˆ¶å’Œè‡ªåŠ¨è½¬æ¢
                logger.info(f"ä½¿ç”¨ init_llm ä» {actual_llm_dir} åŠ è½½ base LLMï¼ˆå¤„ç† torch ç‰ˆæœ¬é™åˆ¶ï¼‰")
                base_llm = init_llm(actual_llm_dir, self.tokenizer, "bf16" in dtype_flag, self.device)
                
                # æ„å»ºç®€åŒ–çš„ config
                simple_cfg = {
                    "tokens": {"mol_token": "<mol>"},
                    "train": {
                        "use_diffusion": False,
                        "use_layer2": cfg.get("train", {}).get("use_layer2", False),  # ä¿ç•™ Layer2 é…ç½®
                    },
                    "network": {},
                    "diffusion": {},
                }
                # ä¿ç•™ Layer2 é…ç½®
                if "layer2" in cfg:
                    simple_cfg["layer2"] = cfg["layer2"]
                self.model = init_model(simple_cfg, self.tokenizer, base_llm, self.device)
                # åŠ è½½ extrasï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if has_extras_dir:
                    load_model_weights_from_checkpoint_dir(self.model, str(ckpt_path), self.device)
        else:
            # æ²¡æœ‰ llm ç›®å½•ï¼Œå¯èƒ½æ˜¯çº¯ LLM checkpoint æˆ–éœ€è¦æ‹†åˆ†çš„æ··åˆ checkpoint
            base_llm_path = cfg.get("base_llm_path")
            
            # æ£€æŸ¥ checkpoint æ ¹ç›®å½•æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯çº¯ LLM checkpointï¼‰
            has_model_files = any([
                (ckpt_path / "pytorch_model.bin").exists(),
                (ckpt_path / "model.safetensors").exists(),
                (ckpt_path / "model.safetensors.index.json").exists(),
            ])
            
            if not base_llm_path:
                if has_model_files:
                    # çº¯ LLM checkpointï¼šä»æ ¹ç›®å½•åŠ è½½
                    logger.info(f"æ£€æµ‹åˆ°çº¯ LLM checkpointï¼ˆæ—  llm/ ç›®å½•ï¼‰ï¼Œä»æ ¹ç›®å½•åŠ è½½: {ckpt_dir}")
                    base_llm_path = str(ckpt_path)
                elif ckpt_path.exists():
                    # å°è¯•æ‹†åˆ†æ··åˆ checkpoint
                    logger.info(f"checkpoint æ²¡æœ‰ llm ç›®å½•ï¼Œå°è¯•ä»æ ¹ç›®å½•åŠ è½½å¹¶æ‹†åˆ†")
                    # ä½¿ç”¨ load_model_weights_from_checkpoint_dir çš„é€»è¾‘ï¼Œå®ƒä¼šè‡ªåŠ¨æ‹†åˆ†
                    # ä½†æˆ‘ä»¬éœ€è¦å…ˆæœ‰ä¸€ä¸ª base_llm
                    # å¦‚æœæ²¡æœ‰ base_llm_pathï¼Œæ— æ³•ç»§ç»­
                    raise RuntimeError(
                        f"checkpoint æ²¡æœ‰ llm ç›®å½•ï¼Œä¸”æœªæä¾› base_llm_pathã€‚"
                        f"è¯·æä¾› base_llm_path æˆ–ç¡®ä¿ checkpoint åŒ…å« llm/ å­ç›®å½•ã€‚"
                    )
                else:
                    raise RuntimeError(f"checkpoint ç›®å½•ä¸å­˜åœ¨: {ckpt_dir}")
            
            logger.info(f"ä½¿ç”¨ model_init é€»è¾‘ä» base_llm_path åŠ è½½: {base_llm_path}")
            base_llm = init_llm(base_llm_path, self.tokenizer, "bf16" in dtype_flag, self.device)
            
            # æ„å»ºç®€åŒ–çš„ config
            # å¯¹äºçº¯ LLM checkpointï¼Œç¦ç”¨ GNNï¼ˆå› ä¸ºæ²¡æœ‰ GVP å’Œ mol_adapterï¼‰
            simple_cfg = {
                "tokens": {"mol_token": "<mol>"},
                "train": {
                    "use_diffusion": False,
                    "use_offline_spans": False,  # ç¦ç”¨ GNN
                    "use_layer2": cfg.get("train", {}).get("use_layer2", False),  # ä¿ç•™ Layer2 é…ç½®
                },
                "network": {},
                "diffusion": {},
                "paths": {"checkpoint_dir": str(ckpt_path)} if ckpt_path.exists() else {},
            }
            # ä¿ç•™ Layer2 é…ç½®
            if "layer2" in cfg:
                simple_cfg["layer2"] = cfg["layer2"]
            self.model = init_model(simple_cfg, self.tokenizer, base_llm, self.device)
            
            # å¦‚æœ checkpoint å­˜åœ¨ä¸”æœ‰ extras ç›®å½•ï¼Œå°è¯•åŠ è½½æƒé‡ï¼ˆä¼šè‡ªåŠ¨å¤„ç†æ‹†åˆ†ï¼‰
            # ä½†å¯¹äºçº¯ LLM checkpointï¼Œä¸ä¼šæœ‰ extras ç›®å½•ï¼Œæ‰€ä»¥è¿™é‡Œä¸ä¼šåŠ è½½ GVP/mol_adapter
            if ckpt_path.exists() and has_extras_dir:
                load_model_weights_from_checkpoint_dir(self.model, str(ckpt_path), self.device)
            elif ckpt_path.exists() and not has_extras_dir:
                logger.info(f"çº¯ LLM checkpointï¼ˆæ—  extras ç›®å½•ï¼‰ï¼Œè·³è¿‡ GVP/mol_adapter åŠ è½½")

        # åŠ è½½ token_classifier_head
        # ä¼˜å…ˆä» extras ç›®å½•æŸ¥æ‰¾ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®æŒ‡å®šçš„è·¯å¾„
        token_classifier_path = None
        extras_token_classifier = ckpt_path / "extras" / "token_classifier.pt"
        if extras_token_classifier.exists() and extras_token_classifier.is_file():
            token_classifier_path = str(extras_token_classifier)
            logger.info(f"âœ… Found token_classifier in extras directory: {token_classifier_path}")
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é…ç½®ä¸­æŒ‡å®šçš„è·¯å¾„
            token_classifier_path = cfg.get("token_classifier_path")
            if token_classifier_path:
                logger.info(f"Using token_classifier_path from config: {token_classifier_path}")
        
        if token_classifier_path and os.path.isfile(token_classifier_path):
            try:
                hidden_size = self.model.llm.config.hidden_size
                model_dtype = next(self.model.llm.parameters()).dtype
                
                # ç¡®å®štoken classifieråº”è¯¥æ”¾åœ¨å“ªä¸ªè®¾å¤‡
                if self.use_multi_gpu:
                    # æ”¾åœ¨LLMç¬¬ä¸€ä¸ªå±‚çš„è®¾å¤‡ä¸Š
                    token_classifier_device = next(self.model.llm.parameters()).device
                else:
                    token_classifier_device = self.device
                
                # å…ˆæ£€æŸ¥ checkpoint ä¸­çš„ hidden_size æ˜¯å¦åŒ¹é…
                try:
                    ckpt = torch.load(token_classifier_path, map_location="cpu")
                    # å¤„ç†å¤šç§å¯èƒ½çš„ checkpoint ç»“æ„
                    if isinstance(ckpt, dict):
                        if "state_dict" in ckpt:
                            raw_sd = ckpt["state_dict"]
                        elif "head_state_dict" in ckpt:
                            # æ–°çš„æ ¼å¼ï¼šstate_dict å­˜å‚¨åœ¨ head_state_dict ä¸­
                            raw_sd = ckpt["head_state_dict"]
                            # åŒæ—¶æ£€æŸ¥ hidden_size æ˜¯å¦åŒ¹é…
                            if "hidden_size" in ckpt:
                                ckpt_hidden_size = ckpt["hidden_size"]
                                if ckpt_hidden_size != hidden_size:
                                    logger.warning(
                                        f"âš ï¸ Token classifier hidden_size mismatch: "
                                        f"checkpoint={ckpt_hidden_size}, model={hidden_size}. "
                                        f"Skipping token_classifier_head loading."
                                    )
                                    setattr(self.model, "token_classifier_head", None)
                                    raise ValueError("Hidden size mismatch")  # è§¦å‘å¤–å±‚ except å¤„ç†
                        elif "model_state_dict" in ckpt:
                            raw_sd = ckpt["model_state_dict"]
                        else:
                            # æ£€æŸ¥æ˜¯å¦é¡¶å±‚å°±æ˜¯ state_dictï¼ˆæ‰€æœ‰å€¼éƒ½æ˜¯ tensorï¼‰
                            if all(isinstance(v, torch.Tensor) for v in ckpt.values() if v is not None):
                                raw_sd = ckpt
                            else:
                                # å¦‚æœé¡¶å±‚æœ‰é tensor å€¼ï¼Œå°è¯•æŸ¥æ‰¾å¯èƒ½çš„ state_dict
                                raw_sd = None
                                for key in ["head_state_dict", "state_dict", "model_state_dict", "classifier"]:
                                    if key in ckpt and isinstance(ckpt[key], dict):
                                        raw_sd = ckpt[key]
                                        break
                                if raw_sd is None:
                                    raise ValueError(f"Could not find state_dict in checkpoint. Available keys: {list(ckpt.keys())}")
                    else:
                        raw_sd = ckpt
                    
                    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªçº¿æ€§å±‚çš„æƒé‡æ¥ç¡®å®šåŸå§‹ hidden_sizeï¼ˆå¦‚æœè¿˜æ²¡æœ‰ä»é¡¶å±‚æ£€æŸ¥è¿‡ï¼‰
                    ckpt_hidden_size = None
                    # å¦‚æœå·²ç»ä»é¡¶å±‚ ckpt ä¸­è¯»å–äº† hidden_sizeï¼Œå°±ä¸éœ€è¦å†ä» state_dict ä¸­æ¨æ–­
                    if isinstance(ckpt, dict) and "hidden_size" in ckpt:
                        ckpt_hidden_size = ckpt["hidden_size"]
                    else:
                        # ä» state_dict ä¸­æ¨æ–­ hidden_size
                        for key, value in raw_sd.items():
                            if isinstance(value, torch.Tensor) and "weight" in key and len(value.shape) == 2:
                                # é€šå¸¸æ˜¯ç¬¬ä¸€å±‚çš„è¾“å…¥ç»´åº¦
                                ckpt_hidden_size = value.shape[1]
                                break
                    
                    if ckpt_hidden_size is not None and ckpt_hidden_size != hidden_size:
                        logger.warning(
                            f"âš ï¸ Token classifier hidden_size mismatch: "
                            f"checkpoint={ckpt_hidden_size}, model={hidden_size}. "
                            f"Skipping token_classifier_head loading. "
                            f"Model will use text-matching fallback for entity detection."
                        )
                        setattr(self.model, "token_classifier_head", None)
                    else:
                        # ç»´åº¦åŒ¹é…ï¼Œç»§ç»­åŠ è½½
                        # ä» checkpoint ä¸­æ¨æ–­ä¸­é—´å±‚ç»´åº¦ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                        intermediate_dim = 128  # é»˜è®¤å€¼
                        if isinstance(ckpt, dict) and "head_state_dict" in ckpt:
                            # å°è¯•ä» state_dict ä¸­æ¨æ–­ä¸­é—´å±‚ç»´åº¦
                            for key, value in raw_sd.items():
                                if isinstance(value, torch.Tensor) and "0.weight" in key and len(value.shape) == 2:
                                    # ç¬¬ä¸€å±‚ Linear çš„è¾“å‡ºç»´åº¦å°±æ˜¯ä¸­é—´å±‚ç»´åº¦
                                    intermediate_dim = value.shape[0]
                                    logger.info(f"ğŸ“ Inferred intermediate_dim from checkpoint: {intermediate_dim}")
                                    break
                        
                        # å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†ä¸­é—´å±‚ç»´åº¦ï¼Œä½¿ç”¨é…ç½®çš„å€¼
                        if "token_classifier_intermediate_dim" in cfg:
                            intermediate_dim = cfg["token_classifier_intermediate_dim"]
                            logger.info(f"ğŸ“ Using intermediate_dim from config: {intermediate_dim}")
                        
                        token_head = nn.Sequential(
                            nn.Linear(hidden_size, intermediate_dim),
                            nn.ReLU(),
                            nn.Dropout(0.1),
                            nn.Linear(intermediate_dim, 2)
                        ).to(device=token_classifier_device, dtype=model_dtype)
                        
                        # ä½¿ç”¨ä¸ init_offline_token_classifier ç›¸åŒçš„é€»è¾‘æ¥æ¸…ç† state_dict
                        from collections import OrderedDict
                        
                        # æ¸…ç† state_dictï¼šç§»é™¤å¯èƒ½çš„ module. å’Œ net. å‰ç¼€
                        clean_sd = OrderedDict()
                        for k, v in raw_sd.items():
                            name = k
                            # ç§»é™¤ module. å‰ç¼€ï¼ˆDDP è®­ç»ƒäº§ç”Ÿçš„ï¼‰
                            if name.startswith("module."):
                                name = name[7:]
                            # ç§»é™¤ net. å‰ç¼€ï¼ˆæŸäº› checkpoint æ ¼å¼ï¼‰
                            if name.startswith("net."):
                                name = name[4:]
                            clean_sd[name] = v
                        
                        # æ„å»ºæœ€ç»ˆçš„ state_dictï¼Œå°è¯•å¤šç§å¯èƒ½çš„ key æ ¼å¼
                        # åªå¤„ç† tensor å€¼ï¼Œè·³è¿‡å­—ç¬¦ä¸²ç­‰å…¶ä»–ç±»å‹
                        final_sd = OrderedDict()
                        for k, v in clean_sd.items():
                            # åªå¤„ç† tensor å€¼
                            if not isinstance(v, torch.Tensor):
                                continue
                            
                            # å°è¯•å¤šç§å¯èƒ½çš„ key æ ¼å¼
                            if k.startswith("classifier."):
                                final_sd[k.replace("classifier.", "")] = v
                            elif k.startswith("token_classifier."):
                                final_sd[k.replace("token_classifier.", "")] = v
                            elif not "." in k or k.count(".") <= 1:
                                # å¦‚æœ key çœ‹èµ·æ¥åƒæ˜¯åˆ†ç±»å™¨çš„å‚æ•°ï¼ˆæ²¡æœ‰å¤ªå¤šå±‚çº§ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                                final_sd[k] = v
                            elif "0.weight" in k or "0.bias" in k or "3.weight" in k or "3.bias" in k:
                                # åŒ¹é… Sequential æ¨¡å‹çš„ keyï¼ˆ0 æ˜¯ç¬¬ä¸€å±‚ Linearï¼Œ3 æ˜¯ç¬¬äºŒå±‚ Linearï¼‰
                                final_sd[k] = v
                        
                        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…
                        if not final_sd:
                            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« weight æˆ– bias çš„ keyï¼Œå¹¶å°è¯•åŒ¹é… Sequential çš„ç´¢å¼•æ ¼å¼
                            for k, v in clean_sd.items():
                                # åªå¤„ç† tensor å€¼
                                if not isinstance(v, torch.Tensor):
                                    continue
                                
                                if "weight" in k or "bias" in k:
                                    parts = k.split(".")
                                    # å°è¯•åŒ¹é… Sequential çš„ç´¢å¼•æ ¼å¼ï¼ˆ0, 1, 2, 3ï¼‰
                                    # ä¾‹å¦‚ï¼š0.weight, 0.bias, 3.weight, 3.bias
                                    if len(parts) >= 2 and parts[-2].isdigit() and parts[-1] in ["weight", "bias"]:
                                        idx = int(parts[-2])
                                        if idx in [0, 3]:  # åªåŒ¹é…ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚ Linear
                                            final_sd[k] = v
                                    elif len(parts) == 1:
                                        # ç›´æ¥æ˜¯ weight æˆ– biasï¼ˆæ— å±‚çº§ï¼‰
                                        final_sd[k] = v
                        
                        if not final_sd:
                            logger.warning(f"âš ï¸ No matching keys found in checkpoint. Available keys: {list(clean_sd.keys())[:20]}")
                            raise ValueError(f"No matching keys found in checkpoint. Available keys: {list(clean_sd.keys())[:20]}")
                        
                        logging.debug(f"ğŸ“‹ Matched {len(final_sd)} keys: {list(final_sd.keys())}")
                        
                        # è½¬æ¢æ•°æ®ç±»å‹å¹¶åŠ è½½ï¼ˆåªå¤„ç† tensorï¼Œè·³è¿‡é tensor å€¼ï¼‰
                        final_sd_clean = OrderedDict()
                        for k, v in final_sd.items():
                            if isinstance(v, torch.Tensor):
                                final_sd_clean[k] = v.to(dtype=model_dtype)
                            else:
                                logger.warning(f"âš ï¸ Skipping non-tensor value for key '{k}': type={type(v)}")
                        
                        if not final_sd_clean:
                            raise ValueError(f"No valid tensor values found in final_sd. Keys: {list(final_sd.keys())}")
                        
                        token_head.load_state_dict(final_sd_clean, strict=True)
                        token_head.eval()
                        
                        setattr(self.model, "token_classifier_head", token_head)
                        for p in self.model.token_classifier_head.parameters():
                            p.requires_grad = False
                        
                        logger.info(f"âœ… Loaded token_classifier_head from {token_classifier_path}")
                except Exception as inner_e:
                    raise inner_e
            except Exception as e:
                logger.warning(
                    f"âš ï¸ Failed to load token_classifier_head from {token_classifier_path}: {e}. "
                    f"Model will use text-matching fallback for entity detection. "
                    f"This is usually fine and will not affect generation quality."
                )
                # ç¡®ä¿ token_classifier_head æ˜¯ Noneï¼Œè¿™æ ·ä¼šä½¿ç”¨ fallback æ–¹æ³•
                setattr(self.model, "token_classifier_head", None)

        self.model.debug = bool(cfg.get("debug", False))
        for p in self.model.parameters():
            p.requires_grad = False
        logger.info(f'self.model #parameters: {sum(p.numel() for p in self.model.parameters())}, #trainable: {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}')

        # ä¸¥æ ¼æ ¡éªŒ tokenizer/embedding ä¸€è‡´æ€§
        self._sync_vocab_and_embeddings(strict=True)
        
        ##############################
        # NOTE: LDMOL Init (lazy / opt-in)
        # TODO: æœªæ¥å°†LDMol ckptåº”æ”¾åœ¨ ckpt_pathç›®å½•ä¸‹
        self.ldmol = None  # é»˜è®¤ä¸åŠ è½½ï¼Œé¿å… ChemBench è¿™ç§çº¯æ–‡æœ¬è¯„æµ‹ OOM

        enable_ldmol = bool(cfg.get("enable_ldmol", False))  # é»˜è®¤ False
        ldmol_device = cfg.get("ldmol_device", "cpu")        # é»˜è®¤æ”¾ CPUï¼Œæ›´å®‰å…¨

        if enable_ldmol and LDMOL_ENABLED:
            logger.info(f"ğŸ§ª Enable LDMolInferer: device={ldmol_device}")
            self.ldmol = LDMolInferer(device=ldmol_device)
        else:
            logger.info("ğŸ§ª LDMolInferer unavailable")
        ##############################

        if self.use_multi_gpu:
            logger.info(f"âœ… Model & tokenizer loaded from {ckpt_dir} on multiple GPUs (device_map={device_map}).")
        else:
            logger.info(f"âœ… Model & tokenizer loaded from {ckpt_dir} on {self.device}.")
        
        logger.info(f"â„¹ï¸  Thinking æ¨¡å¼: {'å¯ç”¨' if self.enable_thinking else 'ç¦ç”¨'} (enable_thinking={self.enable_thinking})")
        
        # æ£€æŸ¥ Layer2 æ˜¯å¦å¯ç”¨
        self.use_layer2 = hasattr(self.model, 'layer2_inferer') and self.model.layer2_inferer is not None
        if self.use_layer2:
            logger.info("âœ… Layer2 å·²å¯ç”¨ï¼Œæ”¯æŒååº”äº§ç‡é¢„æµ‹ pipeline")
        else:
            logger.info("â„¹ï¸  Layer2 æœªå¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†ç”Ÿæˆæ¨¡å¼")

    @torch.no_grad()
    def generate(
        self,
        prompt,
        *,
        add_dialog_wrapper: bool = True,
        realtime_mol: bool = True,
        max_new_tokens: int = 256,
        max_tokens: Optional[int] = None,
        do_sample: bool = False,
        temperature: float = 1.0,
        top_k: int = 0,
        top_p: float = 1.0,
        repetition_penalty: float = 1.15,  # æé«˜é»˜è®¤å€¼ä»¥å‡å°‘é‡å¤ï¼ˆä»1.05æé«˜åˆ°1.15ï¼‰
        no_repeat_ngram_size: int = 3,  # é˜²æ­¢é‡å¤n-gramï¼ˆé»˜è®¤3ï¼Œå³é˜²æ­¢3ä¸ªtokençš„é‡å¤ï¼‰
        skip_special_tokens: bool = True,  # é»˜è®¤è·³è¿‡ç‰¹æ®Štokenï¼ˆå®˜æ–¹é£æ ¼ï¼‰
        eos_token_id: Optional[int] = None,
        return_ids: bool = False,
        verbose_logging: bool = False,  # æ§åˆ¶è¯¦ç»†æ—¥å¿—è¾“å‡º
        max_text_length_for_detection: int = 4096,  # è¶…å‡ºæ­¤é•¿åº¦è·³è¿‡å®ä½“æ£€æµ‹ï¼ˆä½†ä¸åœæ­¢ç”Ÿæˆï¼‰ï¼Œæ”¯æŒfew-shotç­‰é•¿prompt
        use_diffusion_as_smiles_supplement: bool = False,  # å¦‚æœä¸æ˜¯SMILESå°±è°ƒç”¨diffusionå¾—åˆ°SMILES
        task_type: Optional[str] = None, 
        src_smiles: str = None,  # TODO: åˆ†å­ç¼–è¾‘ä»»åŠ¡çš„è¾“å…¥smiles
        use_layer2_pipeline: bool = False,  # æ˜¯å¦ä½¿ç”¨ Layer2 pipeline
        extract_reactant_fn: Optional[callable] = None,  # è‡ªå®šä¹‰æå–ååº”ç‰© SMILES çš„å‡½æ•°ï¼ˆç”¨äº Layer2ï¼‰
        return_intermediate: bool = False,  # æ˜¯å¦è¿”å›ä¸­é—´ç»“æœï¼ˆç”¨äº Layer2ï¼‰
        force_use_layer2: Optional[bool] = None,  # å¼ºåˆ¶ä½¿ç”¨ Layer2ï¼ˆç”¨äº Layer2ï¼‰
        extra_embeddings: Optional[torch.Tensor | List[torch.Tensor]] = None,  # é¢å¤–çš„ embedding è¿½åŠ åˆ°åºåˆ—æœ«å°¾ï¼ˆåƒ GVP ä¸€æ ·ï¼‰
    ):
        """
        ç”Ÿæˆæ–‡æœ¬ï¼Œæ”¯æŒå®æ—¶åˆ†å­æ ‡æ³¨å’Œ GVP è°ƒç”¨ã€‚
        
        æ¨ç†æ—¶çš„GNNæµç¨‹ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰ï¼š
        1. æ£€æµ‹åˆ° <mol>...</mol> æ ‡ç­¾ï¼Œæå–SMILES
        2. SMILES -> GVP encoder -> å›¾embedding
        3. å›¾embedding -> mol_adapter -> LLMç»´åº¦embedding
        4. è¿™ä¸ªembeddingè¿½åŠ åˆ°åºåˆ—ä¸­ï¼ˆä¸ä½œä¸ºtokenæ˜¾ç¤ºï¼Œä½†ä½œä¸ºhidden stateå­˜åœ¨ï¼‰
        5. ç»§ç»­ç”Ÿæˆåç»­æ–‡æœ¬
        
        æ³¨æ„ï¼šGNNçš„embeddingä¼šä¸€ç›´å­˜åœ¨äºhidden statesä¸­ï¼Œå½±å“åç»­ç”Ÿæˆï¼Œ
        ä½†ä¸ä¼šç”Ÿæˆå¯¹åº”çš„tokenï¼Œè¿™æ˜¯è®¾è®¡ä¸Šçš„ç‰¹æ€§ã€‚
        
        Args:
            use_layer2_pipeline: å¦‚æœä¸º Trueï¼Œä½¿ç”¨ Layer2 pipelineï¼ˆä¸¤è½®ç”Ÿæˆ + Layer2 é¢„æµ‹ï¼‰
            å…¶ä»–å‚æ•°åŒ generate_with_layer2
        """
        assert self.model is not None and self.tokenizer is not None, "è¯·å…ˆè°ƒç”¨ load(config) å®Œæˆåˆå§‹åŒ–ã€‚"
        
        # å¦‚æœå¯ç”¨ Layer2 pipelineï¼Œè°ƒç”¨ generate_with_layer2
        if use_layer2_pipeline:
            return self.generate_with_layer2(
                prompt=prompt,
                add_dialog_wrapper=add_dialog_wrapper,
                realtime_mol=realtime_mol,
                max_new_tokens=max_new_tokens,
                max_tokens=max_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                no_repeat_ngram_size=no_repeat_ngram_size,
                task_type=task_type,
                extract_reactant_fn=extract_reactant_fn,
                return_intermediate=return_intermediate,
                force_use_layer2=force_use_layer2,
            )

        # 1) ç»Ÿä¸€æˆ batchï¼ˆå®˜æ–¹é£æ ¼ï¼šbatch/single ç”¨åŒä¸€å¥—é€»è¾‘ï¼‰
        is_batched = isinstance(prompt, (list, tuple))
        prompts: List[str] = list(prompt) if is_batched else [prompt]
        
        # 2) é€‰æ‹© system message
        system_msg = self._get_system_message(task_type, realtime_mol)
        logger.info(f"task_type: {task_type}, use system_msg: {system_msg}")
        
        # 3) ç¼–ç  prompts
        batch_enc = self._encode_prompts(prompts, system_msg, add_dialog_wrapper)
        
        # 4) æ”¾åˆ°æ­£ç¡® deviceï¼ˆå…¼å®¹ device_map="auto"ï¼‰
        model_device = _get_model_device(self.model.llm)
        input_ids = batch_enc["input_ids"].to(model_device)
        attention_mask = batch_enc.get("attention_mask", torch.ones_like(input_ids)).to(model_device)
        
        # 5) å¤„ç†é¢å¤–çš„ embeddingï¼ˆå¦‚æœæœ‰ï¼‰
        # ç°åœ¨æ”¯æŒåŒæ—¶ä¼ å…¥ input_ids å’Œ inputs_embedsï¼ˆé¢å¤–çš„ embeddingï¼‰
        # input_ids ç”¨äº token æ£€æµ‹ï¼Œinputs_embeds ä½œä¸ºé¢å¤– embedding æ’å…¥ï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰
        extra_inputs_embeds = None
        
        if extra_embeddings is not None:
            extra_inputs_embeds = self._prepare_extra_embeddings(
                extra_embeddings, model_device, len(prompts)
            )
        
        # 6) è·å– eos/pad IDs
        eos_id, pad_id = self._get_eos_pad_ids(eos_token_id)
        
        # 7) è®¡ç®— prompt_len
        prompt_lens = [input_ids.shape[1] for _ in range(len(prompts))]
        
        if verbose_logging:
            print(f"[DEBUG] batch={len(prompts)}, input_ids={tuple(input_ids.shape)}, device={model_device}")
            print(f"[DEBUG] eos_id={eos_id}, pad_id={pad_id}")
            if extra_inputs_embeds is not None:
                print(f"[DEBUG] Using extra inputs_embeds with shape {extra_inputs_embeds.shape}")
        
        # 8) æ„å»ºç”Ÿæˆå‚æ•°
        gen_kwargs = dict(
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            no_repeat_ngram_size=no_repeat_ngram_size,
            eos_token_id=eos_id,
            pad_token_id=pad_id,
        )
        
        if max_tokens is not None:
            gen_kwargs["max_tokens"] = max_tokens
        
        # 9) è°ƒç”¨æ¨¡å‹ç”Ÿæˆ
        # å¦‚æœ realtime_mol=True ä½† batch size >= 1ï¼Œé€ä¸ªå¤„ç†
        if realtime_mol and len(prompts) >= 1:
            logger.info(f"â„¹ï¸  realtime_mol ä»…æ”¯æŒ batch=1ï¼Œå½“å‰ batch={len(prompts)}ï¼Œå°†é€ä¸ªå¤„ç†")
            # é€ä¸ªå¤„ç†æ¯ä¸ª prompt
            results = []
            raw_outputs = []
            for i, prompt in enumerate(prompts):
                # é‡æ–°ç¼–ç å•ä¸ª prompt
                single_prompt_enc = self._encode_prompts([prompt], system_msg, add_dialog_wrapper)
                single_input_ids = single_prompt_enc["input_ids"].to(model_device)
                single_attention_mask = single_prompt_enc.get("attention_mask", torch.ones_like(single_input_ids)).to(model_device)
                
                # å¤„ç†å•ä¸ª prompt çš„ extra_embeddingsï¼ˆå¦‚æœæœ‰ï¼‰
                single_extra_inputs_embeds = None
                
                if extra_embeddings is not None:
                    # å¦‚æœ extra_embeddings æ˜¯åˆ—è¡¨ï¼Œå–å¯¹åº”çš„å…ƒç´ ï¼›å¦åˆ™ä½¿ç”¨åŒä¸€ä¸ª
                    if isinstance(extra_embeddings, list):
                        single_extra_emb = extra_embeddings[i] if i < len(extra_embeddings) else extra_embeddings[0]
                    else:
                        single_extra_emb = extra_embeddings
                    
                    single_extra_inputs_embeds = self._prepare_extra_embeddings(
                        single_extra_emb, model_device, 1
                    )
                
                single_prompt_len = single_input_ids.shape[1]
                
                # è°ƒç”¨æ¨¡å‹ç”Ÿæˆï¼ˆåŒæ—¶ä¼ å…¥ input_ids å’Œ inputs_embedsï¼‰
                single_out_ids = self._call_model_generate(
                    input_ids=single_input_ids,
                    input_embeds=single_extra_inputs_embeds,  # é¢å¤–çš„ embeddingï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰
                    attention_mask=single_attention_mask,
                    realtime_mol=realtime_mol,  # ä¿æŒ realtime_mol=True
                    verbose_logging=verbose_logging,
                    max_text_length_for_detection=max_text_length_for_detection,
                    gen_kwargs=gen_kwargs,
                )
                
                # è§£ç å•ä¸ªç»“æœ
                single_results, single_raw_outputs = self._decode_generated_ids(
                    single_out_ids, [single_prompt_len], skip_special_tokens
                )
                results.append(single_results[0] if single_results else "")
                raw_outputs.append(single_raw_outputs[0] if single_raw_outputs else "")
        else:
            # batch_size=1 æˆ– realtime_mol=Falseï¼Œæ­£å¸¸å¤„ç†
            # åŒæ—¶ä¼ å…¥ input_ids å’Œ inputs_embedsï¼ˆå¦‚æœæœ‰é¢å¤–çš„ embeddingï¼‰
            out_ids = self._call_model_generate(
                input_ids=input_ids,
                input_embeds=extra_inputs_embeds,  # é¢å¤–çš„ embeddingï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰
                attention_mask=attention_mask,
                realtime_mol=realtime_mol,
                verbose_logging=verbose_logging,
                max_text_length_for_detection=max_text_length_for_detection,
                gen_kwargs=gen_kwargs,
            )
            
            # 10) è§£ç ç»“æœ
            results, raw_outputs = self._decode_generated_ids(
                out_ids, prompt_lens, skip_special_tokens
            )
        
        # 11) ä¿å­˜ last_raw_outputs
        self._last_raw_outputs = raw_outputs
        
        # 12) å¤„ç†ç‰¹æ®Šä»»åŠ¡
        if not is_batched:
            results = self._postprocess_special_tasks(
                results, task_type, realtime_mol, verbose_logging, src_smiles
            )
        
        # è¿”å›ç»“æœ
        return results if is_batched else results[0]
    
    @torch.no_grad()
    def generate_with_layer2(
        self,
        prompt: str,
        *,
        add_dialog_wrapper: bool = True,
        realtime_mol: bool = True,  # ä¸¤è½®ç”Ÿæˆéƒ½ä½¿ç”¨æ­¤å‚æ•°
        max_new_tokens: int = 256,
        max_tokens: Optional[int] = None,
        do_sample: bool = False,
        temperature: float = 1.0,
        top_k: int = 0,
        top_p: float = 1.0,
        repetition_penalty: float = 1.15,
        no_repeat_ngram_size: int = 3,
        task_type: Optional[str] = None,
        extract_reactant_fn: Optional[callable] = None,  # è‡ªå®šä¹‰æå–ååº”ç‰© SMILES çš„å‡½æ•°
        return_intermediate: bool = False,  # æ˜¯å¦è¿”å›ä¸­é—´ç»“æœ
        force_use_layer2: Optional[bool] = None,  # å¼ºåˆ¶ä½¿ç”¨ Layer2ï¼ˆNone æ—¶æ ¹æ® task_type è‡ªåŠ¨åˆ¤æ–­ï¼‰
    ) -> str | Dict[str, Any]:
        """
        Layer2 Pipeline: query -> LLM -> Layer2 -> LLM
        
        Pipeline æµç¨‹:
        1. ç¬¬ä¸€è½® LLM ç”Ÿæˆï¼Œæå–ååº”ç‰© SMILES å’Œ amount_info
        2. GVP è·å–å¯¹åº” embedding
        3. Layer2 é¢„æµ‹ [yield_bin, embedding]ï¼ˆä»…å½“ä»»åŠ¡éœ€è¦æ—¶ï¼‰
        4. å°† embedding è¿½åŠ åˆ° prompt åï¼ˆåƒ GVP ä¸€æ ·ï¼‰ï¼Œæ„å»ºå¢å¼º promptï¼ˆåŒ…å« yield_bin ä¿¡æ¯ï¼‰
        5. ç¬¬äºŒè½® LLM ç”Ÿæˆï¼Œè¾“å‡ºæœ€ç»ˆç»“æœ
        
        ä»»åŠ¡ç±»å‹æ”¯æŒ:
        - "yield_prediction": äº§ç‡é¢„æµ‹ - ä½¿ç”¨ Layer2
        - "product_yield_prediction": äº§ç‰©+äº§ç‡é¢„æµ‹ - ä½¿ç”¨ Layer2
        - "product_prediction": äº§ç‰©é¢„æµ‹ - ä¸ä½¿ç”¨ Layer2
        - "reaction_prediction": ååº”é¢„æµ‹ - ä¸ä½¿ç”¨ Layer2
        - å…¶ä»–æˆ– None: æ ¹æ® force_use_layer2 å‚æ•°å†³å®š
        
        Args:
            prompt: è¾“å…¥æŸ¥è¯¢
            extract_reactant_fn: è‡ªå®šä¹‰å‡½æ•°ï¼Œä»ç¬¬ä¸€è½®è¾“å‡ºä¸­æå–ååº”ç‰©ä¿¡æ¯
                                å‡½æ•°ç­¾å: (text: str) -> Dict[str, Any]
                                è¿”å›æ ¼å¼: {
                                    "reactant_smiles": str | List[str],
                                    "amount_info": Optional[Dict | List[Dict]]
                                }
            return_intermediate: æ˜¯å¦è¿”å›ä¸­é—´ç»“æœ
            force_use_layer2: å¼ºåˆ¶ä½¿ç”¨ Layer2ï¼ˆNone æ—¶æ ¹æ® task_type è‡ªåŠ¨åˆ¤æ–­ï¼‰
            ... (å…¶ä»–å‚æ•°åŒ generate æ–¹æ³•)
        
        Returns:
            å¦‚æœ return_intermediate=False: è¿”å›æœ€ç»ˆç”Ÿæˆçš„æ–‡æœ¬
            å¦‚æœ return_intermediate=True: è¿”å›å­—å…¸ {
                "first_response": str,
                "layer2_info": {
                    "yield_bin": int,
                    "yield_reg": float,
                    "embedding": torch.Tensor,
                } | None,  # å¦‚æœæœªä½¿ç”¨ Layer2 åˆ™ä¸º None
                "final_response": str
            }
        """
        assert self.model is not None and self.tokenizer is not None, "è¯·å…ˆè°ƒç”¨ load(config) å®Œæˆåˆå§‹åŒ–ã€‚"
        
        # å¦‚æœå¼ºåˆ¶ä¸ä½¿ç”¨ Layer2ï¼Œå›é€€åˆ°æ ‡å‡†ç”Ÿæˆ
        if force_use_layer2 is False:
            logger.info(f"å¼ºåˆ¶ä¸ä½¿ç”¨ Layer2ï¼Œä½¿ç”¨æ ‡å‡†ç”Ÿæˆæ¨¡å¼")
            return self.generate(
                prompt,
                add_dialog_wrapper=add_dialog_wrapper,
                realtime_mol=realtime_mol,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                no_repeat_ngram_size=no_repeat_ngram_size,
                task_type=task_type,
            )
        
        # å¦‚æœ Layer2 æœªå¯ç”¨ï¼Œå›é€€åˆ°æ ‡å‡†ç”Ÿæˆ
        if not self.use_layer2:
            logger.warning("Layer2 æœªå¯ç”¨ï¼Œå›é€€åˆ°æ ‡å‡†ç”Ÿæˆæ¨¡å¼")
            return self.generate(
                prompt,
                add_dialog_wrapper=add_dialog_wrapper,
                realtime_mol=realtime_mol,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                no_repeat_ngram_size=no_repeat_ngram_size,
                task_type=task_type,
            )
        
        logger.info(f"ğŸ”„ å¼€å§‹ Layer2 Pipeline (ä»»åŠ¡ç±»å‹: {task_type})")
        
        # ===== ç¬¬ä¸€é˜¶æ®µï¼šç¬¬ä¸€è½® LLM ç”Ÿæˆï¼Œä¸“é—¨è¾“å‡º JSON æ ¼å¼çš„ååº”ç‰©ä¿¡æ¯ =====
        logger.info("ğŸ“ ç¬¬ä¸€é˜¶æ®µï¼šLLM ç”Ÿæˆï¼ˆJSON æ ¼å¼æå–ååº”ç‰©ä¿¡æ¯ï¼‰")
        
        # æ„å»ºä¸“é—¨ç”¨äºæå–ååº”ç‰©ä¿¡æ¯çš„ promptï¼ˆè¦æ±‚è¾“å‡º JSONï¼‰
        extraction_prompt = self._build_layer2_extraction_prompt(prompt)
        
        first_response = self.generate(
            extraction_prompt,
            add_dialog_wrapper=add_dialog_wrapper,
            max_new_tokens=max_new_tokens // 2,  # ç¬¬ä¸€è½®ç”Ÿæˆè¾ƒçŸ­
            do_sample=do_sample,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            no_repeat_ngram_size=no_repeat_ngram_size,
            task_type=None,  # ç¬¬ä¸€è½®ä¸ä½¿ç”¨ç‰¹æ®Šä»»åŠ¡ç±»å‹ï¼Œä¸“æ³¨äº JSON æå–
            realtime_mol=False,  # ç¬¬ä¸€è½®ä¸éœ€è¦ realtime_molï¼Œåªæå–ä¿¡æ¯
        )
        
        if isinstance(first_response, list):
            first_response = first_response[0]
        
        logger.info(f"ç¬¬ä¸€è½®è¾“å‡º (JSON): {first_response[:200]}...")
        
        # æå–åˆ†å­ä¿¡æ¯å’Œè§’è‰²
        if extract_reactant_fn is not None:
            # ä½¿ç”¨è‡ªå®šä¹‰æå–å‡½æ•°
            extracted_info = extract_reactant_fn(first_response)
            if isinstance(extracted_info, dict):
                # å…¼å®¹æ–°æ ¼å¼
                if "molecules" in extracted_info:
                    molecules_info = extracted_info["molecules"]
                else:
                    # å…¼å®¹æ—§æ ¼å¼
                    reactant_smiles = extracted_info.get("reactant_smiles", extracted_info.get("smiles"))
                    if isinstance(reactant_smiles, str):
                        reactant_smiles = [reactant_smiles]
                    molecules_info = [
                        {
                            "smiles": smi,
                            "role": "REACTANT",
                            "amount_info": extracted_info.get("amount_info")
                        }
                        for smi in reactant_smiles
                    ]
            else:
                # å…¼å®¹æ—§æ¥å£ï¼šç›´æ¥è¿”å› SMILES å­—ç¬¦ä¸²
                if isinstance(extracted_info, str):
                    extracted_info = [extracted_info]
                molecules_info = [
                    {"smiles": smi, "role": "REACTANT"}
                    for smi in extracted_info
                ]
        else:
            # ä¼˜å…ˆä» JSON ä¸­è§£æ
            parsed_json = self._parse_json_response(first_response)
            if parsed_json is not None:
                molecules_info = parsed_json.get("molecules", [])
            else:
                # JSON è§£æå¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤æå–é€»è¾‘
                logger.warning("âš ï¸  JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æå–é€»è¾‘")
                reactant_smiles = self._extract_reactant_smiles(first_response)
                if isinstance(reactant_smiles, str):
                    reactant_smiles = [reactant_smiles]
                molecules_info = [
                    {"smiles": smi, "role": "REACTANT"}
                    for smi in reactant_smiles
                ]
        
        if not molecules_info:
            logger.warning("âš ï¸  æœªèƒ½ä»ç¬¬ä¸€è½®è¾“å‡ºä¸­æå–åˆ†å­ä¿¡æ¯ï¼Œå›é€€åˆ°æ ‡å‡†ç”Ÿæˆ")
            if return_intermediate:
                return {
                    "first_response": first_response,
                    "layer2_info": None,
                    "final_response": first_response,
                }
            return first_response
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹å’Œè§’è‰²ä¿¡æ¯ï¼Œå†³å®šéœ€è¦é¢„æµ‹çš„å†…å®¹
        # å¦‚æœæ˜¯é¢„æµ‹ååº”ç‰©ï¼Œåº”è¯¥ mask ååº”ç‰©éƒ¨åˆ†ï¼›å¦‚æœæ˜¯é¢„æµ‹äº§ç‰©ï¼Œåº”è¯¥ mask äº§ç‰©éƒ¨åˆ†
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦æ ¹æ® task_type æ¥åˆ¤æ–­
        target_role = None
        if task_type:
            task_type_lower = task_type.lower()
            if "reactant" in task_type_lower or "reaction" in task_type_lower:
                # é¢„æµ‹ååº”ç‰©ï¼Œéœ€è¦ mask ååº”ç‰©
                target_role = "REACTANT"
            elif "product" in task_type_lower:
                # é¢„æµ‹äº§ç‰©ï¼Œéœ€è¦ mask äº§ç‰©
                target_role = "PRODUCT"
            else:
                # é»˜è®¤é¢„æµ‹ååº”ç‰©
                target_role = "REACTANT"
        else:
            # é»˜è®¤é¢„æµ‹ååº”ç‰©
            target_role = "REACTANT"
        
        logger.info(f"âœ… æå–åˆ° {len(molecules_info)} ä¸ªåˆ†å­ï¼Œç›®æ ‡è§’è‰²: {target_role}")
        for i, mol in enumerate(molecules_info):
            logger.info(f"  åˆ†å­ {i+1}: {mol.get('smiles', 'N/A')} (è§’è‰²: {mol.get('role', 'N/A')})")
        
        # ===== ç¬¬äºŒé˜¶æ®µï¼šGVP + Layer2 é¢„æµ‹ =====
        logger.info("ğŸ”¬ ç¬¬äºŒé˜¶æ®µï¼šGVP + Layer2 é¢„æµ‹")
        
        try:
            # æ ¹æ®è§’è‰²ç­›é€‰éœ€è¦é¢„æµ‹çš„åˆ†å­
            # å¦‚æœç›®æ ‡æ˜¯é¢„æµ‹ååº”ç‰©ï¼Œæˆ‘ä»¬éœ€è¦æ‰€æœ‰å·²çŸ¥çš„åˆ†å­ï¼ˆåŒ…æ‹¬äº§ç‰©ï¼‰æ¥é¢„æµ‹ååº”ç‰©
            # å¦‚æœç›®æ ‡æ˜¯é¢„æµ‹äº§ç‰©ï¼Œæˆ‘ä»¬éœ€è¦æ‰€æœ‰å·²çŸ¥çš„åˆ†å­ï¼ˆåŒ…æ‹¬ååº”ç‰©ï¼‰æ¥é¢„æµ‹äº§ç‰©
            # è¿™é‡Œæˆ‘ä»¬å…ˆä½¿ç”¨æ‰€æœ‰åˆ†å­ï¼ŒLayer2 ä¼šæ ¹æ®è§’è‰²ä¿¡æ¯æ¥å¤„ç†
            
            # æå–æ‰€æœ‰åˆ†å­çš„ SMILES å’Œç›¸å…³ä¿¡æ¯
            all_smiles = [mol["smiles"] for mol in molecules_info]
            all_roles = [mol.get("role", "REACTANT") for mol in molecules_info]
            all_amount_info = [mol.get("amount_info") for mol in molecules_info]
            
            # è·å–æ‰€æœ‰åˆ†å­çš„ GVP embeddings
            gvp_embeddings = []
            for smi in all_smiles:
                gvp_emb = self.model.gvp_encoder.forward_from_smiles(smi)
                if gvp_emb is None:
                    raise ValueError(f"GVP encoder è¿”å› None for SMILES: {smi}")
                gvp_embeddings.append(gvp_emb.squeeze(0))  # [D]
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªåˆ†å­ï¼Œä¿æŒå…¼å®¹æ€§
            if len(gvp_embeddings) == 1:
                gvp_embedding = gvp_embeddings[0]
                amount_info = all_amount_info[0]
            else:
                gvp_embedding = gvp_embeddings
                amount_info = all_amount_info if any(ai is not None for ai in all_amount_info) else None
            
            # Layer2 é¢„æµ‹
            # æ ¹æ®ä»»åŠ¡ç±»å‹ï¼Œå†³å®šå“ªäº›åˆ†å­éœ€è¦è¢« maskï¼ˆéœ€è¦é¢„æµ‹çš„ï¼‰
            # å¦‚æœä»»åŠ¡æ˜¯é¢„æµ‹ååº”ç‰©ï¼Œåˆ™ mask ååº”ç‰©ï¼›å¦‚æœä»»åŠ¡æ˜¯é¢„æµ‹äº§ç‰©ï¼Œåˆ™ mask äº§ç‰©
            # è¿™é‡Œæˆ‘ä»¬åªä¼ å…¥éœ€è¦é¢„æµ‹çš„åˆ†å­ï¼ˆè¢« mask çš„ï¼‰ï¼Œå…¶ä»–ä½œä¸ºä¸Šä¸‹æ–‡
            
            # ç­›é€‰éœ€è¦é¢„æµ‹çš„åˆ†å­ï¼ˆæ ¹æ® target_roleï¼‰
            target_molecules = [mol for mol in molecules_info if mol.get("role") == target_role]
            context_molecules = [mol for mol in molecules_info if mol.get("role") != target_role]
            
            if not target_molecules:
                logger.warning(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°è§’è‰²ä¸º {target_role} çš„åˆ†å­ï¼Œä½¿ç”¨æ‰€æœ‰åˆ†å­ä½œä¸ºååº”ç‰©")
                target_molecules = molecules_info
                target_smiles = all_smiles
                target_gvp_embeddings = gvp_embeddings
                target_amount_info = amount_info
            else:
                # åªä½¿ç”¨ç›®æ ‡è§’è‰²çš„åˆ†å­è¿›è¡Œé¢„æµ‹
                target_smiles = [mol["smiles"] for mol in target_molecules]
                target_indices = [i for i, mol in enumerate(molecules_info) if mol.get("role") == target_role]
                target_gvp_embeddings = [gvp_embeddings[i] for i in target_indices]
                target_amount_info = [all_amount_info[i] for i in target_indices]
                if len(target_amount_info) == 1:
                    target_amount_info = target_amount_info[0]
                elif not any(ai is not None for ai in target_amount_info):
                    target_amount_info = None
            
            logger.info(f"ğŸ“Š é¢„æµ‹ç›®æ ‡: {len(target_molecules)} ä¸ª {target_role} åˆ†å­")
            if context_molecules:
                logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡: {len(context_molecules)} ä¸ªå…¶ä»–è§’è‰²åˆ†å­")
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªç›®æ ‡åˆ†å­ï¼Œä¿æŒå…¼å®¹æ€§
            if len(target_gvp_embeddings) == 1:
                target_gvp_embedding = target_gvp_embeddings[0]
            else:
                target_gvp_embedding = target_gvp_embeddings
            
            # Layer2 é¢„æµ‹ï¼ˆä¼ å…¥éœ€è¦é¢„æµ‹çš„åˆ†å­ï¼‰
            layer2_output = self.model.layer2_inferer.predict(
                reactant_smiles=target_smiles,  # ä¼ å…¥éœ€è¦é¢„æµ‹çš„ SMILES
                gvp_embedding=target_gvp_embedding,
                amount_info=target_amount_info,
            )
            
            yield_bin = layer2_output['yield_bin']
            yield_reg = layer2_output['yield_reg']
            task_embedding_raw = layer2_output['embedding']  # åŸå§‹ç»´åº¦ï¼ˆ256ï¼‰
            
            # é€šè¿‡ mol_adapter å°† embedding æ˜ å°„åˆ° LLM ç»´åº¦ï¼ˆä¸ GVP è¿”å›çš„æ ¼å¼ä¸€è‡´ï¼‰
            if hasattr(self.model, 'mol_adapter') and self.model.mol_adapter is not None:
                # ç¡®ä¿ dtype ä¸€è‡´
                mol_adapter_dtype = next(self.model.mol_adapter.parameters()).dtype
                task_embedding_raw = task_embedding_raw.to(dtype=mol_adapter_dtype)
                task_embedding = self.model.mol_adapter(task_embedding_raw)  # [LLM_hidden_size]
            else:
                # å¦‚æœæ²¡æœ‰ mol_adapterï¼Œä½¿ç”¨åŸå§‹ embedding
                task_embedding = task_embedding_raw
            
            # è®¡ç®— yield_bin å¯¹åº”çš„äº§ç‡èŒƒå›´
            yield_bin_percent_min = yield_bin * 10
            yield_bin_percent_max = (yield_bin + 1) * 10
            yield_bin_range = f"{yield_bin_percent_min}%-{yield_bin_percent_max}%"
            yield_reg_percent = yield_reg * 100
            
            logger.info(f"âœ… Layer2 é¢„æµ‹å®Œæˆ: yield_bin={yield_bin} (åŒºé—´: {yield_bin_range}), yield_reg={yield_reg:.3f} ({yield_reg_percent:.1f}%)")
            
        except Exception as e:
            logger.error(f"âŒ Layer2 é¢„æµ‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°æ ‡å‡†ç”Ÿæˆ")
            import traceback
            traceback.print_exc()
            return first_response
        
        # ===== ç¬¬ä¸‰é˜¶æ®µï¼šæ„å»ºå¢å¼º promptï¼Œç¬¬äºŒè½®ç”Ÿæˆ =====
        logger.info("ğŸ“ ç¬¬ä¸‰é˜¶æ®µï¼šæ„å»ºå¢å¼º promptï¼Œç¬¬äºŒè½®ç”Ÿæˆ")
        
        # æ„å»ºå¢å¼º promptï¼ŒåŒ…å«ï¼š
        # 1. åŸå§‹è¾“å…¥ï¼ˆpromptï¼‰
        # 2. Layer2 é¢„æµ‹ä¿¡æ¯ï¼ˆyield_bin, yield_regï¼Œå¦‚æœä»»åŠ¡éœ€è¦ï¼‰
        # æ³¨æ„ï¼šembedding å°†é€šè¿‡ç‰¹æ®Šæ ‡è®°è¿½åŠ åˆ°åºåˆ—æœ«å°¾ï¼ˆåƒ GVP ä¸€æ ·ï¼‰ï¼Œè€Œä¸æ˜¯æ–‡æœ¬åŒ–
        # æ³¨æ„ï¼šç¬¬ä¸€è½®çš„ JSON è¾“å‡ºåªæ˜¯ä¸­é—´ç»“æœï¼Œä¸éœ€è¦åœ¨ prompt ä¸­å±•ç¤º
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹å†³å®šæ˜¯å¦åŒ…å« yield ä¿¡æ¯
        include_yield_info = False
        if task_type:
            task_type_lower = task_type.lower()
            if task_type_lower in ["yield_prediction", "product_yield_prediction"]:
                include_yield_info = True
        
        if include_yield_info:
            # åŒ…å« yield ä¿¡æ¯çš„ prompt
            enhanced_prompt = f"""{prompt}

[Layer2 Prediction]
- Yield Bin: {yield_bin}/9 (predicted yield range: {yield_bin*10}%-{(yield_bin+1)*10}%)
- Yield Value: {yield_reg:.1%}

Based on the original input and Layer2 prediction, please provide your final answer:"""
        else:
            # ä¸åŒ…å« yield ä¿¡æ¯ï¼Œä½† Layer2 embedding ä¼šé€šè¿‡ extra_embeddings ä¼ é€’
            enhanced_prompt = f"""{prompt}

[Layer2 Information]
Layer2 has analyzed the reaction and provided additional context (embedded in the input sequence).

Based on the original input and Layer2 analysis, please provide your final answer:"""
        
        # ç¬¬äºŒè½®ç”Ÿæˆ
        # å°† Layer2 çš„ embedding è¿½åŠ åˆ°åºåˆ—æœ«å°¾ï¼ˆåƒ GVP ä¸€æ ·ï¼‰
        final_response = self.generate(
            enhanced_prompt,
            add_dialog_wrapper=add_dialog_wrapper,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            no_repeat_ngram_size=no_repeat_ngram_size,
            task_type=task_type,
            realtime_mol=realtime_mol,  # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
            extra_embeddings=task_embedding,  # è¿½åŠ  Layer2 embedding åˆ°åºåˆ—æœ«å°¾
        )
        
        if isinstance(final_response, list):
            final_response = final_response[0]
        
        logger.info("âœ… Layer2 Pipeline å®Œæˆ")
        
        # è¿”å›ç»“æœ
        if return_intermediate:
            return {
                "first_response": first_response,
                "layer2_info": {
                    "yield_bin": yield_bin,
                    "yield_reg": yield_reg,
                    "embedding": task_embedding,  # å·²ç»æ˜¯ LLM ç»´åº¦
                },
                "final_response": final_response,
            }
        return final_response
    
    def _build_layer2_extraction_prompt(self, original_prompt: str) -> str:
        """
        æ„å»ºç¬¬ä¸€è½®ç”Ÿæˆçš„ promptï¼Œè¦æ±‚æ¨¡å‹è¾“å‡º JSON æ ¼å¼çš„ååº”ç‰©/äº§ç‰©ä¿¡æ¯ã€‚
        
        Returns:
            ç”¨äºç¬¬ä¸€è½®ç”Ÿæˆçš„ prompt
        """
        # æ£€æŸ¥ prompt ä¸­æ˜¯å¦å·²ç»åŒ…å«ååº” SMILES
        has_rxn = "Reaction SMILES:" in original_prompt or "Reactants SMILES:" in original_prompt or "Target product SMILES:" in original_prompt
        
        if has_rxn:
            # å¦‚æœ prompt ä¸­å·²ç»åŒ…å«ååº”ä¿¡æ¯ï¼Œç›´æ¥è¦æ±‚æå–
            return f"""{original_prompt}

Extract the molecule information from the above question in JSON format. Output only a valid JSON object with the following structure:
{{
    "molecules": [
        {{
            "smiles": "SMILES string",
            "role": "REACTANT" | "PRODUCT" | "REAGENT" | "SOLVENT" | "CATALYST" | "BYPRODUCT" | "OTHER",
            "amount_info": {{
                "moles": float (optional),
                "mass": float (optional),
                "volume": float (optional)
            }} (optional)
        }},
        ...
    ]
}}

Important: Extract ALL molecules mentioned in the question, including reactants, products, reagents, and solvents.
Output only the JSON, no additional text:"""
        else:
            # å¦‚æœ prompt ä¸­æ²¡æœ‰æ˜ç¡®çš„ååº”ä¿¡æ¯ï¼Œè¦æ±‚æ›´è¯¦ç»†çš„æå–
            return f"""{original_prompt}

Extract the molecule information from the question in JSON format. Output only a valid JSON object with the following structure:
{{
    "molecules": [
        {{
            "smiles": "SMILES string",
            "role": "REACTANT" | "PRODUCT" | "REAGENT" | "SOLVENT" | "CATALYST" | "BYPRODUCT" | "OTHER",
            "amount_info": {{
                "moles": float (optional),
                "mass": float (optional),
                "volume": float (optional)
            }} (optional)
        }},
        ...
    ]
}}

Example:
{{
    "molecules": [
        {{
            "smiles": "CCO",
            "role": "REACTANT",
            "amount_info": {{
                "moles": 1.0,
                "mass": 46.07
            }}
        }},
        {{
            "smiles": "CC(=O)O",
            "role": "REACTANT"
        }},
        {{
            "smiles": "CC(=O)OCC",
            "role": "PRODUCT"
        }}
    ]
}}

Important: Extract ALL molecules mentioned in the question. If the question contains a reaction SMILES (with >> or >), parse it to extract all reactants and products.
Output only the JSON, no additional text:"""
    
    def _parse_json_response(self, text: str) -> Optional[Dict[str, Any]]:
        """
        ä»æ–‡æœ¬ä¸­è§£æ JSON æ ¼å¼çš„åˆ†å­ä¿¡æ¯ï¼ˆåŒ…å«è§’è‰²ï¼‰ã€‚
        ä½¿ç”¨ json_repair æ¥ä¿®å¤å¯èƒ½çš„ JSON æ ¼å¼é”™è¯¯ã€‚
        
        Returns:
            è§£æåçš„å­—å…¸ï¼ŒåŒ…å« molecules åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« smiles, role, amount_info
            å¦‚æœè§£æå¤±è´¥è¿”å› None
        """
        import json
        import re
        
        # å°è¯•å¯¼å…¥ json_repairï¼ˆä½¿ç”¨ç±»çº§åˆ«çš„ç¼“å­˜é¿å…é‡å¤å¯¼å…¥æ£€æŸ¥ï¼‰
        if not hasattr(MolAwareGenerator2, '_json_repair_available'):
            try:
                import json_repair
                MolAwareGenerator2._json_repair_available = True
                MolAwareGenerator2._json_repair_module = json_repair
            except ImportError:
                MolAwareGenerator2._json_repair_available = False
                MolAwareGenerator2._json_repair_module = None
                # åªåœ¨ç¬¬ä¸€æ¬¡è­¦å‘Š
                if not hasattr(MolAwareGenerator2, '_json_repair_warned'):
                    logger.warning("âš ï¸  json_repair æœªå®‰è£…ï¼Œå°†å°è¯•ç›´æ¥è§£æ JSONã€‚å»ºè®®å®‰è£…: pip install json-repair")
                    MolAwareGenerator2._json_repair_warned = True
        
        HAS_JSON_REPAIR = MolAwareGenerator2._json_repair_available
        json_repair = MolAwareGenerator2._json_repair_module if HAS_JSON_REPAIR else None
        
        # å°è¯•æå– JSON éƒ¨åˆ†ï¼ˆå¯èƒ½åœ¨ä»£ç å—ä¸­ï¼‰
        # æ–¹æ³•0: å¦‚æœåŒ…å« _type å’Œ _contentï¼Œæå– _content
        if "_type" in text and "_content" in text:
            try:
                # å…ˆè§£æå¤–å±‚ JSONï¼Œæå– _content
                outer_parsed = json.loads(text)
                if isinstance(outer_parsed, dict) and "_content" in outer_parsed:
                    content = outer_parsed["_content"]
                    # _content å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼ˆéœ€è¦å†æ¬¡è§£æï¼‰æˆ–å·²ç»æ˜¯å­—å…¸
                    if isinstance(content, str):
                        text = content  # ç»§ç»­å¤„ç†
                        logger.info("ğŸ“ ä» _content å­—æ®µæå– JSON å­—ç¬¦ä¸²")
                    elif isinstance(content, dict):
                        # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥è¿”å›
                        parsed = content
                        if "molecules" in parsed:
                            return parsed
                        # å¦åˆ™ç»§ç»­å¤„ç†
                        text = json.dumps(content)
                        logger.info("ğŸ“ ä» _content å­—æ®µæå– JSON å¯¹è±¡")
                else:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æ­£åˆ™æå–
                    content_match = re.search(r'"_content"\s*:\s*"([^"]+)"', text, re.DOTALL)
                    if content_match:
                        # å¤„ç†è½¬ä¹‰å­—ç¬¦
                        import codecs
                        text = codecs.decode(content_match.group(1), 'unicode_escape')
                        logger.info("ğŸ“ ä» _content å­—æ®µï¼ˆè½¬ä¹‰ï¼‰æå– JSON å†…å®¹")
            except Exception as e:
                logger.debug(f"è§£æ _content å¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹æ–‡æœ¬")
        
        # æ–¹æ³•1: æŸ¥æ‰¾ ```json ... ``` ä»£ç å—
        json_block_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
        match = re.search(json_block_pattern, text, re.DOTALL)
        if match:
            json_str = match.group(1)
        else:
            # æ–¹æ³•2: æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { ... } å—
            brace_match = re.search(r'\{.*\}', text, re.DOTALL)
            if brace_match:
                json_str = brace_match.group(0)
            else:
                # æ–¹æ³•3: ç›´æ¥ä½¿ç”¨æ•´ä¸ªæ–‡æœ¬
                json_str = text.strip()
        
        # æ¸…ç†å¯èƒ½çš„å°¾éšé€—å·ç­‰
        json_str = json_str.strip()
        if json_str.endswith(','):
            json_str = json_str[:-1]
        
        # å°è¯•ç›´æ¥è§£æ
        try:
            parsed = json.loads(json_str)
        except json.JSONDecodeError as e:
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ json_repair
            if HAS_JSON_REPAIR:
                try:
                    logger.info("ğŸ”§ ä½¿ç”¨ json_repair ä¿®å¤ JSON...")
                    repaired_json_str = json_repair.repair_json(json_str)
                    parsed = json.loads(repaired_json_str)
                    logger.info("âœ… JSON ä¿®å¤æˆåŠŸ")
                except Exception as repair_e:
                    logger.warning(f"âš ï¸  json_repair ä¿®å¤å¤±è´¥: {repair_e}")
                    logger.debug(f"å°è¯•è§£æçš„æ–‡æœ¬: {json_str[:200]}...")
                    return None
            else:
                logger.warning(f"âš ï¸  JSON è§£æå¤±è´¥: {e}")
                logger.debug(f"å°è¯•è§£æçš„æ–‡æœ¬: {json_str[:200]}...")
                return None
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if "molecules" not in parsed:
            # å…¼å®¹æ—§æ ¼å¼ï¼šå¦‚æœåªæœ‰ reactant_smilesï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
            if "reactant_smiles" in parsed:
                logger.info("ğŸ”„ æ£€æµ‹åˆ°æ—§æ ¼å¼ JSONï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼...")
                reactant_smiles = parsed["reactant_smiles"]
                if isinstance(reactant_smiles, str):
                    reactant_smiles = [reactant_smiles]
                
                molecules = []
                for smi in reactant_smiles:
                    molecules.append({
                        "smiles": smi,
                        "role": "REACTANT",
                        "amount_info": parsed.get("amount_info")
                    })
                parsed = {"molecules": molecules}
            else:
                logger.warning("âš ï¸  JSON ä¸­ç¼ºå°‘ molecules æˆ– reactant_smiles å­—æ®µ")
                return None
        
        # éªŒè¯ molecules æ ¼å¼
        if not isinstance(parsed["molecules"], list):
            logger.warning("âš ï¸  molecules å­—æ®µå¿…é¡»æ˜¯åˆ—è¡¨")
            return None
        
        # éªŒè¯æ¯ä¸ªåˆ†å­å¯¹è±¡
        for i, mol in enumerate(parsed["molecules"]):
            if not isinstance(mol, dict):
                logger.warning(f"âš ï¸  molecules[{i}] å¿…é¡»æ˜¯å­—å…¸")
                return None
            if "smiles" not in mol:
                logger.warning(f"âš ï¸  molecules[{i}] ç¼ºå°‘ smiles å­—æ®µ")
                return None
            if "role" not in mol:
                # é»˜è®¤è§’è‰²ä¸º REACTANT
                mol["role"] = "REACTANT"
                logger.info(f"â„¹ï¸  molecules[{i}] ç¼ºå°‘ role å­—æ®µï¼Œé»˜è®¤ä¸º REACTANT")
        
        return parsed
    
    def _extract_reactant_smiles(self, text: str) -> Optional[str | List[str]]:
        """
        ä»æ–‡æœ¬ä¸­æå–ååº”ç‰© SMILESã€‚
        ä¼˜å…ˆä» <mol>...</mol> æ ‡ç­¾ä¸­æå–ï¼Œå¦åˆ™ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ã€‚
        æ”¯æŒæå–å¤šä¸ªååº”ç‰©ã€‚
        """
        import re
        
        # æ–¹æ³•1: ä» <mol>...</mol> æ ‡ç­¾ä¸­æå–ï¼ˆå¯èƒ½å¤šä¸ªï¼‰
        mol_pattern = r'<mol>(.*?)</mol>'
        matches = re.findall(mol_pattern, text, re.DOTALL)
        if matches:
            candidates = []
            for match in matches:
                candidate = match.strip()
                # ç®€å•éªŒè¯ï¼šå¦‚æœçœ‹èµ·æ¥åƒ SMILESï¼ˆåŒ…å«å¸¸è§åŸå­ç¬¦å·ï¼‰
                if any(c in candidate for c in ['C', 'N', 'O', 'S', '(', ')', '=', '[', ']']):
                    candidates.append(candidate)
            if candidates:
                # å¦‚æœåªæœ‰ä¸€ä¸ªï¼Œè¿”å›å­—ç¬¦ä¸²ï¼›å¤šä¸ªåˆ™è¿”å›åˆ—è¡¨
                return candidates[0] if len(candidates) == 1 else candidates
        
        # æ–¹æ³•2: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¯èƒ½çš„ SMILES
        # SMILES æ¨¡å¼ï¼šåŒ…å«åŸå­ç¬¦å·ã€æ‹¬å·ã€æ•°å­—ç­‰
        simple_pattern = r'[CNOScnos()=\[\]#@0-9]+'
        matches = re.findall(simple_pattern, text)
        if matches:
            # å–æœ€é•¿çš„åŒ¹é…
            candidate = max(matches, key=len)
            if len(candidate) >= 5:  # è‡³å°‘5ä¸ªå­—ç¬¦æ‰å¯èƒ½æ˜¯æœ‰æ•ˆçš„ SMILES
                return candidate
        
        return None
    
    def _extract_amount_info(self, text: str) -> Optional[Dict[str, float] | List[Dict[str, float]]]:
        """
        ä»æ–‡æœ¬ä¸­æå– amount_infoï¼ˆé‡ä¿¡æ¯ï¼‰ã€‚
        å°è¯•ä»æ–‡æœ¬ä¸­è§£æ molesã€massã€volume ç­‰ä¿¡æ¯ã€‚
        
        è¿”å›æ ¼å¼:
        - å•ä¸ª: {"moles": float, "mass": float, "volume": float}
        - å¤šä¸ª: [{"moles": float, ...}, ...]
        """
        import re
        
        # ç®€å•çš„æå–é€»è¾‘ï¼šæŸ¥æ‰¾æ•°å­—å’Œå•ä½
        # ä¾‹å¦‚: "1.0 mol", "2.5 g", "100 mL"
        amount_info = {}
        
        # æå– moles
        moles_pattern = r'(\d+\.?\d*)\s*(?:mol|mole|moles)'
        moles_match = re.search(moles_pattern, text, re.IGNORECASE)
        if moles_match:
            amount_info["moles"] = float(moles_match.group(1))
        
        # æå– mass
        mass_pattern = r'(\d+\.?\d*)\s*(?:g|gram|grams|mg|milligram)'
        mass_match = re.search(mass_pattern, text, re.IGNORECASE)
        if mass_match:
            amount_info["mass"] = float(mass_match.group(1))
        
        # æå– volume
        volume_pattern = r'(\d+\.?\d*)\s*(?:ml|mL|milliliter|liters?|L)'
        volume_match = re.search(volume_pattern, text, re.IGNORECASE)
        if volume_match:
            amount_info["volume"] = float(volume_match.group(1))
        
        return amount_info if amount_info else None

   
if __name__ == "__main__":
    # å•å¡æ¨¡å¼é…ç½®
    CONFIG_SINGLE = {
        "ckpt_dir": "/data1/chenyuxuan/MSMLM/model/llama3.2-chem-sft-gnn/1125_llm/epoch2/refine_ner",
        "device": "cuda:0",
        "device_map": None,  # å•å¡æ¨¡å¼
        "dtype": "bf16",
        "debug": True,
        "token_classifier_path": "/data1/lvchangwei/LLM/Lora/llama_mlp_token_classifier.pt",
        
    
    }
    
    # å¤šå¡æ¨¡å¼é…ç½®ï¼ˆè‡ªåŠ¨åˆ†é…åˆ°æ‰€æœ‰å¯ç”¨GPUï¼‰
    CONFIG_MULTI = {
        "ckpt_dir": "/data1/chenyuxuan/MSMLM/model/llama3.2-chem-sft-gnn/1125_llm/epoch2/refine_ner",
        "device_map": "auto",  # å¤šå¡æ¨¡å¼ï¼šè‡ªåŠ¨åˆ†é…æ¨¡å‹åˆ°æ‰€æœ‰å¯ç”¨GPU
        "dtype": "bf16",
        "debug": True,
        "token_classifier_path": "/data1/lvchangwei/LLM/Lora/llama_mlp_token_classifier.pt",
    }
    
    # ä½¿ç”¨å•å¡é…ç½®ï¼ˆå¦‚éœ€å¤šå¡ï¼Œæ”¹ä¸º CONFIG_MULTIï¼‰
    CONFIG = CONFIG_SINGLE

    gen = MolAwareGenerator2()
    gen.load(CONFIG)

    prompt = "Describe this molecule: CCCCCCC(O)C/C=C\\CCCCCCCC(=O)[O-]\nPlease only output the answer."
    # prompt = "How's the weather in Beijing?"
    text = gen.generate(
        prompt,
        add_dialog_wrapper=True,
        realtime_mol=False,
        max_new_tokens=1024,
        max_tokens=16384,
        do_sample=True,
        temperature=0.2,
        repetition_penalty=1.05,
        skip_special_tokens=True,
    )
    print("\n=== Generated Text ===")
    print(text)
    print("\n=== Raw Output (first 500 chars) ===")
    if hasattr(gen, '_last_raw_outputs') and gen._last_raw_outputs:
        print(gen._last_raw_outputs[0][:500] if gen._last_raw_outputs[0] else "N/A")
    else:
        print("N/A")

