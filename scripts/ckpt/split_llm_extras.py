import torch
import os
import json
from pathlib import Path
from transformers import AutoConfig, AutoModelForCausalLM
import sys


def split_checkpoint(checkpoint_path: str, output_dir: str = None):
    """
    æ‹†åˆ† checkpoint ä¸­çš„æ··åˆæƒé‡ä¸º LLM å’Œ Extras
    
    Args:
        checkpoint_path: checkpoint ç›®å½•è·¯å¾„ï¼ˆåŒ…å« pytorch_model.binï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º checkpoint_path
    """
    if output_dir is None:
        output_dir = checkpoint_path
    
    checkpoint_path = Path(checkpoint_path)
    output_dir = Path(output_dir)
    
    # æ£€æŸ¥æƒé‡æ–‡ä»¶ï¼šæ”¯æŒå¤šç§æ ¼å¼
    bin_path = checkpoint_path / "pytorch_model.bin"
    safetensors_path = checkpoint_path / "model.safetensors"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†ç‰‡çš„ safetensors æ–‡ä»¶ï¼ˆmodel-00001-of-00005.safetensors ç­‰ï¼‰
    safetensors_index_path = checkpoint_path / "model.safetensors.index.json"
    sharded_safetensors = False
    if safetensors_index_path.exists():
        try:
            with open(safetensors_index_path, 'r') as f:
                index_data = json.load(f)
                if "weight_map" in index_data:
                    sharded_safetensors = True
                    print(f"ðŸ”„ æ£€æµ‹åˆ°åˆ†ç‰‡ safetensors æ–‡ä»¶")
        except:
            pass
    
    # æ£€æŸ¥ global_step ç›®å½•ï¼ˆDeepSpeed ZeRO checkpointï¼‰
    global_step_dirs = sorted([d for d in checkpoint_path.iterdir() if d.is_dir() and d.name.startswith("global_step")])
    
    if not bin_path.exists() and not safetensors_path.exists() and not sharded_safetensors and not global_step_dirs:
        print(f"âŒ æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶: {checkpoint_path}")
        print(f"   æœŸæœ›æ‰¾åˆ°: pytorch_model.bin, model.safetensors, åˆ†ç‰‡safetensors, æˆ– global_step* ç›®å½•")
        return False
    
    # ä¼˜å…ˆä½¿ç”¨ safetensorsï¼ˆå•ä¸ªæˆ–åˆ†ç‰‡ï¼‰
    if sharded_safetensors:
        print(f"ðŸ”„ æ­£åœ¨åŠ è½½åˆ†ç‰‡ safetensors æƒé‡...")
        from safetensors.torch import load_file
        full_state_dict = {}
        with open(safetensors_index_path, 'r') as f:
            index_data = json.load(f)
            weight_map = index_data.get("weight_map", {})
            # æ”¶é›†æ‰€æœ‰éœ€è¦åŠ è½½çš„æ–‡ä»¶
            shard_files = set(weight_map.values())
            for shard_file in sorted(shard_files):
                shard_path = checkpoint_path / shard_file
                if shard_path.exists():
                    print(f"   åŠ è½½åˆ†ç‰‡: {shard_file} ...")
                    shard_dict = load_file(str(shard_path))
                    full_state_dict.update(shard_dict)
                else:
                    print(f"   âš ï¸ è­¦å‘Š: åˆ†ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {shard_file}")
    elif safetensors_path.exists():
        print(f"ðŸ”„ æ­£åœ¨åŠ è½½æ··åˆæƒé‡ (safetensors): {safetensors_path} ...")
        from safetensors.torch import load_file
        full_state_dict = load_file(str(safetensors_path))
    elif global_step_dirs:
        # å°è¯•ä»Žæœ€æ–°çš„ global_step ç›®å½•æ¢å¤
        latest_global_step = global_step_dirs[-1]
        print(f"ðŸ”„ æ£€æµ‹åˆ° DeepSpeed ZeRO checkpointï¼Œå°è¯•ä»Ž {latest_global_step.name} æ¢å¤...")
        
        # æŸ¥æ‰¾æ¨¡åž‹çŠ¶æ€æ–‡ä»¶
        model_state_files = list(latest_global_step.glob("*model_states.pt"))
        if not model_state_files:
            print(f"âŒ åœ¨ {latest_global_step} ä¸­æ‰¾ä¸åˆ° model_states.pt æ–‡ä»¶")
            return False
        
        # åŠ è½½æ‰€æœ‰åˆ†ç‰‡çš„æ¨¡åž‹çŠ¶æ€
        full_state_dict = {}
        for model_state_file in sorted(model_state_files):
            print(f"   åŠ è½½æ¨¡åž‹çŠ¶æ€: {model_state_file.name} ...")
            state = torch.load(model_state_file, map_location="cpu")
            # DeepSpeed ZeRO æ ¼å¼ï¼šstate å¯èƒ½åŒ…å« 'module' é”®
            if isinstance(state, dict):
                if 'module' in state:
                    state = state['module']
                elif 'model' in state:
                    state = state['model']
                # åˆå¹¶åˆ° full_state_dict
                if isinstance(state, dict):
                    full_state_dict.update(state)
                else:
                    print(f"   âš ï¸ è­¦å‘Š: {model_state_file.name} æ ¼å¼æœªçŸ¥ï¼Œè·³è¿‡")
        
        if not full_state_dict:
            print(f"âŒ æ— æ³•ä»Ž {latest_global_step} åŠ è½½æ¨¡åž‹æƒé‡")
            return False
        print(f"âœ… æˆåŠŸä»Ž {latest_global_step.name} åŠ è½½ {len(full_state_dict)} ä¸ªæƒé‡")
    else:
        print(f"ðŸ”„ æ­£åœ¨åŠ è½½æ··åˆæƒé‡ (bin): {bin_path} ...")
        full_state_dict = torch.load(bin_path, map_location="cpu")
    
    llm_sd = {}
    extras_sd = {}
    
    print("ðŸ”„ æ­£åœ¨æ‹†åˆ†æƒé‡...")
    keys = list(full_state_dict.keys())
    
    # å®šä¹‰ extras çš„å…³é”®è¯ï¼ˆç”¨äºŽè¯†åˆ«éž LLM æƒé‡ï¼‰
    extras_keywords = ["gvp_encoder", "mol_adapter", "diffusion_adapter", "diffusion"]
    
    for k in keys:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ extras æƒé‡
        is_extras = any(x in k for x in extras_keywords)
        
        if is_extras:
            extras_sd[k] = full_state_dict[k]
        elif k.startswith("llm."):
            # æœ‰ llm. å‰ç¼€ï¼ŒåŽ»æŽ‰å‰ç¼€åŽåŠ å…¥ llm_sd
            new_k = k[4:] 
            llm_sd[new_k] = full_state_dict[k]
        else:
            # æ²¡æœ‰å‰ç¼€ï¼Œå¯èƒ½æ˜¯ç›´æŽ¥çš„ LLM æƒé‡ï¼ˆæœªè¢«åŒ…è£…çš„æƒ…å†µï¼‰
            # æ£€æŸ¥æ˜¯å¦åŒ…å« LLM å¸¸è§çš„å±‚å
            llm_keywords = ["embed", "layers", "norm", "lm_head", "model.", "transformer."]
            if any(x in k for x in llm_keywords):
                llm_sd[k] = full_state_dict[k]
            else:
                # ä¸ç¡®å®šçš„ keyï¼Œé»˜è®¤å½“ä½œ LLM æƒé‡ï¼ˆæ›´å®‰å…¨ï¼‰
                llm_sd[k] = full_state_dict[k]
    
    print(f"ðŸ“Š æ‹†åˆ†ç»“æžœ: LLMæƒé‡={len(llm_sd)} keys, Extrasæƒé‡={len(extras_sd)} keys")
            
    # ================= ä¿å­˜ LLM (å¸¦è‡ªåŠ¨ä¿®å¤ Config åŠŸèƒ½) =================
    llm_save_dir = output_dir / "llm"
    llm_save_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ðŸ’¾ æ­£åœ¨ä¿å­˜ LLM åˆ° {llm_save_dir} ...")
    try:
        # 1. å°è¯•åŠ è½½ Configï¼ˆä¼˜å…ˆçº§ï¼šllm/å­ç›®å½• > checkpointæ ¹ç›®å½• > çˆ¶ç›®å½•ï¼‰
        config = None
        llm_config_path = checkpoint_path / "llm" / "config.json"
        root_config_path = checkpoint_path / "config.json"
        parent_config_path = checkpoint_path.parent / "config.json"
        
        if llm_config_path.exists():
            print(f"ðŸ“‚ ä»Ž llm/ å­ç›®å½•åŠ è½½ config: {llm_config_path}")
            config = AutoConfig.from_pretrained(str(llm_config_path))
        elif root_config_path.exists():
            print(f"ðŸ“‚ ä»Ž checkpoint æ ¹ç›®å½•åŠ è½½ config: {root_config_path}")
            config = AutoConfig.from_pretrained(str(checkpoint_path))
        elif parent_config_path.exists():
            print(f"ðŸ“‚ ä»Žçˆ¶ç›®å½•åŠ è½½ config: {parent_config_path}")
            config = AutoConfig.from_pretrained(str(checkpoint_path.parent))
        else:
            print("âš ï¸ æœªæ‰¾åˆ° config.jsonï¼Œå°è¯•ä»Ž checkpoint ç›®å½•è‡ªåŠ¨æ£€æµ‹...")
            try:
                config = AutoConfig.from_pretrained(str(checkpoint_path))
            except Exception as e:
                print(f"âš ï¸ æ— æ³•ä»Ž checkpoint åŠ è½½ config: {e}")
                # å°è¯•ä»Ž llm å­ç›®å½•åŠ è½½ï¼ˆå³ä½¿è·¯å¾„ä¸å­˜åœ¨ï¼ŒAutoConfig å¯èƒ½ä¼šè‡ªåŠ¨æŸ¥æ‰¾ï¼‰
                try:
                    llm_dir = checkpoint_path / "llm"
                    if llm_dir.exists():
                        config = AutoConfig.from_pretrained(str(llm_dir))
                        print(f"âœ… ä»Ž llm/ å­ç›®å½•æˆåŠŸåŠ è½½ config")
                except:
                    pass
                
                if config is None:
                    print("âš ï¸ æ— æ³•åŠ è½½ configï¼Œå°†ä½¿ç”¨æƒé‡ä¸­çš„å®žé™… vocab_sizeï¼ˆä»£ç ä¼šè‡ªåŠ¨ä¿®æ­£ï¼‰")
                    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ configï¼Œç¨åŽä¼šè¢«æƒé‡ä¸­çš„å®žé™…å€¼è¦†ç›–
                    # æ³¨æ„ï¼šAutoConfig å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œä¸éœ€è¦å†æ¬¡å¯¼å…¥
                    # å°è¯•ä»Žæ¨¡åž‹åç§°æŽ¨æ–­ï¼ˆå¦‚æžœè·¯å¾„åŒ…å«æ¨¡åž‹åï¼‰
                    model_name = str(checkpoint_path)
                    if "llama" in model_name.lower():
                        config = AutoConfig.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")
                    elif "mistral" in model_name.lower():
                        config = AutoConfig.from_pretrained("mistralai/Mistral-7B-v0.1")
                    else:
                        # é»˜è®¤ä½¿ç”¨ LLaMAï¼ˆæœ€å¸¸è§ï¼‰
                        config = AutoConfig.from_pretrained("meta-llama/Llama-3.2-3B-Instruct")
                    print(f"âš ï¸ ä½¿ç”¨é»˜è®¤ configï¼ˆç¨åŽä¼šè¢«æƒé‡ä¸­çš„å®žé™…å€¼ä¿®æ­£ï¼‰")

        # 2. ã€æ ¸å¿ƒä¿®å¤ã€‘æ£€æµ‹æƒé‡é‡Œçš„è¯è¡¨å¤§å°ï¼Œå¹¶ä¿®æ­£ Config
        # æ™ºèƒ½æŸ¥æ‰¾åµŒå…¥å±‚ï¼šæ”¯æŒå¤šç§å¯èƒ½çš„ key æ ¼å¼
        embed_weight = None
        possible_keys = [
            "model.embed_tokens.weight",  # LLaMA/Mistral æ ‡å‡†æ ¼å¼
            "embed_tokens.weight",  # æ—  model å‰ç¼€
            "transformer.wte.weight",  # GPT-2 æ ¼å¼
            "model.embedding.weight",  # å…¶ä»–å¯èƒ½æ ¼å¼
        ]
        
        # å…ˆå°è¯•å·²çŸ¥çš„ key
        for key in possible_keys:
            if key in llm_sd:
                embed_weight = llm_sd[key]
                print(f"ðŸ” æ‰¾åˆ°åµŒå…¥å±‚: {key}, shape={embed_weight.shape}")
                break
        
        # å¦‚æžœæ²¡æ‰¾åˆ°ï¼Œæœç´¢æ‰€æœ‰åŒ…å« "embed" çš„ key
        if embed_weight is None:
            for k, v in llm_sd.items():
                if "embed" in k.lower() and "weight" in k.lower() and len(v.shape) == 2:
                    embed_weight = v
                    print(f"ðŸ” è‡ªåŠ¨æ£€æµ‹åˆ°åµŒå…¥å±‚: {k}, shape={embed_weight.shape}")
                    break
        
        if embed_weight is not None:
            real_vocab_size = embed_weight.shape[0]
            if config.vocab_size != real_vocab_size:
                print(f"ðŸ”§ æ£€æµ‹åˆ°è¯è¡¨å¤§å°å˜æ›´: Config({config.vocab_size}) -> Weights({real_vocab_size})")
                print(f"ðŸ”§ è‡ªåŠ¨ä¿®æ­£ config.vocab_size = {real_vocab_size}")
                config.vocab_size = real_vocab_size
        else:
            print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°åµŒå…¥å±‚æƒé‡ï¼Œæ— æ³•è‡ªåŠ¨ä¿®æ­£ vocab_size")
            print(f"   å¯ç”¨çš„ key ç¤ºä¾‹: {list(llm_sd.keys())[:5]}...")
        
        # 3. ä½¿ç”¨ä¿®æ­£åŽçš„ Config åˆå§‹åŒ–æ¨¡åž‹å¹¶åŠ è½½æƒé‡
        model = AutoModelForCausalLM.from_config(config)
        
        # åŠ è½½æƒé‡ (æ­¤æ—¶å½¢çŠ¶åº”è¯¥åŒ¹é…äº†)
        model.load_state_dict(llm_sd, strict=True)
        
        # 4. ä¿å­˜ä¸º HF æ ¼å¼ (safetensors + config.json)
        model.save_pretrained(str(llm_save_dir), safe_serialization=True)
        
        # åŒæ—¶ä¿å­˜ tokenizer (å¦‚æžœ checkpoint ç›®å½•ä¸‹æœ‰ tokenizer æ–‡ä»¶)
        try:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(str(checkpoint_path))
            tokenizer.save_pretrained(str(llm_save_dir))
            print("âœ… Tokenizer ä¹Ÿå·²å¤åˆ¶å¹¶ä¿å­˜")
        except Exception as e:
            print(f"âš ï¸ æœªæ‰¾åˆ° Tokenizer æ–‡ä»¶: {e}")
        
        print("âœ… LLM ä¿å­˜æˆåŠŸ (safetensorsæ ¼å¼)")
        
    except Exception as e:
        import traceback
        print(f"âš ï¸ LLM save_pretrained å¤±è´¥: {e}")
        traceback.print_exc()
        print("å°è¯•ä»…ä¿å­˜ pytorch_model.bin ...")
        torch.save(llm_sd, str(llm_save_dir / "pytorch_model.bin"))
    
    # ================= ä¿å­˜ Extras =================
    extras_save_dir = output_dir / "extras"
    extras_save_dir.mkdir(parents=True, exist_ok=True)
    
    gvp_only = {k.replace("gvp_encoder.", ""): v for k, v in extras_sd.items() if "gvp_encoder" in k}
    mol_only = {k.replace("mol_adapter.", ""): v for k, v in extras_sd.items() if "mol_adapter" in k}
    diff_only = {k.replace("diffusion_adapter.", ""): v for k, v in extras_sd.items() if "diffusion_adapter" in k}

    if gvp_only: 
        torch.save(gvp_only, str(extras_save_dir / "gvp_encoder.pt"))
        print(f"âœ… ä¿å­˜ GVP encoder ({len(gvp_only)} params)")
    if mol_only: 
        torch.save(mol_only, str(extras_save_dir / "mol_adapter.pt"))
        print(f"âœ… ä¿å­˜ Mol adapter ({len(mol_only)} params)")
    if diff_only: 
        torch.save(diff_only, str(extras_save_dir / "diffusion_adapter.pt"))
        print(f"âœ… ä¿å­˜ Diffusion adapter ({len(diff_only)} params)")

    print(f"\nðŸŽ‰ æ‹†åˆ†å®Œæˆï¼è¾“å‡ºç›®å½•: {output_dir}")
    return True


if __name__ == "__main__":
    # é»˜è®¤é…ç½®ï¼ˆç”¨äºŽå‘½ä»¤è¡Œç›´æŽ¥è¿è¡Œï¼‰
    # epoch2_1 è®­ç»ƒå®ŒæˆåŽçš„æœ€åŽä¸€ä¸ª checkpoint
    checkpoint_path = "/data1/chenyuxuan/checkpoint/qwen3_8b_cpt_sft/epoch2/LLM_nofreeze/name_conversion/checkpoint-535"
    output_dir = ""
    if len(sys.argv) > 1:
        checkpoint_path = sys.argv[1]
    if len(sys.argv) > 2:
        output_dir = sys.argv[2]
    else:
        output_dir = checkpoint_path
    split_checkpoint(checkpoint_path, output_dir)

