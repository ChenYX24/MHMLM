#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°†æ¨¡å‹ä» .bin æ ¼å¼è½¬æ¢ä¸º safetensors æ ¼å¼
å¦‚æœæ¨¡å‹ç›®å½•åªæœ‰ .bin æ–‡ä»¶ï¼Œåˆ™è½¬æ¢ä¸º safetensors
"""

import argparse
import os
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


def check_has_safetensors(model_path: str) -> bool:
    """æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å·²æœ‰ safetensors æ–‡ä»¶"""
    model_dir = Path(model_path)
    if not model_dir.exists():
        return False
    
    safetensors_files = list(model_dir.glob("*.safetensors")) + list(model_dir.glob("model*.safetensors"))
    return len(safetensors_files) > 0


def check_has_bin_files(model_path: str) -> bool:
    """æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦æœ‰ .bin æ–‡ä»¶"""
    model_dir = Path(model_path)
    if not model_dir.exists():
        return False
    
    bin_files = list(model_dir.glob("*.bin")) + list(model_dir.glob("pytorch_model*.bin"))
    return len(bin_files) > 0


def convert_to_safetensors(model_path: str, device: str = "cpu", dtype: str = "bfloat16"):
    """
    å°†æ¨¡å‹è½¬æ¢ä¸º safetensors æ ¼å¼
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        device: åŠ è½½è®¾å¤‡ï¼ˆé»˜è®¤ cpuï¼Œé¿å…æ˜¾å­˜ä¸è¶³ï¼‰
        dtype: æ•°æ®ç±»å‹ï¼ˆbfloat16 æˆ– float16ï¼‰
    """
    print(f"ğŸ“‚ Checking model: {model_path}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ safetensors
    if check_has_safetensors(model_path):
        print("âœ… Model already has safetensors files. Skipping conversion.")
        return
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ .bin æ–‡ä»¶
    if not check_has_bin_files(model_path):
        print("âš ï¸  No .bin files found. Nothing to convert.")
        return
    
    print("ğŸ”„ Converting .bin files to safetensors format...")
    print(f"   Device: {device}")
    print(f"   Dtype: {dtype}")
    
    # é€‰æ‹© dtype
    if dtype == "bfloat16":
        torch_dtype = torch.bfloat16
    elif dtype == "float16":
        torch_dtype = torch.float16
    else:
        torch_dtype = torch.float32
    
    try:
        # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ CPU æˆ–æŒ‡å®šè®¾å¤‡ï¼Œé¿å…æ˜¾å­˜ä¸è¶³ï¼‰
        print("   Loading model...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
            device_map=device if device != "cpu" else None
        )
        
        # å¦‚æœæŒ‡å®šäº† GPUï¼Œéœ€è¦ç§»åˆ°å¯¹åº”è®¾å¤‡
        if device.startswith("cuda"):
            model = model.to(device)
        
        print("   Saving as safetensors...")
        # ä¿å­˜ä¸º safetensors æ ¼å¼
        model.save_pretrained(
            model_path,
            safe_serialization=True,
            max_shard_size="5GB"  # å¦‚æœæ¨¡å‹å¾ˆå¤§ï¼Œåˆ†ç‰‡ä¿å­˜
        )
        
        print("âœ… Conversion completed successfully!")
        print(f"   Safetensors files saved to: {model_path}")
        
        # å¯é€‰ï¼šåˆ é™¤æ—§çš„ .bin æ–‡ä»¶ï¼ˆè°¨æ…æ“ä½œï¼‰
        # print("   Cleaning up old .bin files...")
        # for bin_file in Path(model_path).glob("*.bin"):
        #     if "pytorch_model" in bin_file.name:
        #         bin_file.unlink()
        #         print(f"      Deleted: {bin_file.name}")
        
    except Exception as e:
        print(f"âŒ Conversion failed: {e}")
        import traceback
        traceback.print_exc()
        raise


def main():
    parser = argparse.ArgumentParser(description="Convert model from .bin to safetensors format")
    parser.add_argument('--model-path', type=str, required=True,
                        help='Path to the model directory')
    parser.add_argument('--device', type=str, default="cpu",
                        help='Device to load model (cpu, cuda:0, etc.). Default: cpu to save memory')
    parser.add_argument('--dtype', type=str, default="bfloat16",
                        choices=["bfloat16", "float16", "float32"],
                        help='Data type for model weights. Default: bfloat16')
    parser.add_argument('--check-only', action='store_true',
                        help='Only check if conversion is needed, do not convert')
    
    args = parser.parse_args()
    
    if args.check_only:
        has_safetensors = check_has_safetensors(args.model_path)
        has_bin = check_has_bin_files(args.model_path)
        
        print(f"Model: {args.model_path}")
        print(f"  Has safetensors: {has_safetensors}")
        print(f"  Has .bin files: {has_bin}")
        
        if has_safetensors:
            print("âœ… No conversion needed.")
        elif has_bin:
            print("âš ï¸  Conversion needed: model has .bin files but no safetensors.")
        else:
            print("âš ï¸  No model files found.")
    else:
        convert_to_safetensors(args.model_path, args.device, args.dtype)


if __name__ == "__main__":
    main()

