#!/bin/bash
# Layer2-LLM å®Œæ•´æµç¨‹ï¼šç”Ÿæˆæ•°æ® -> è®­ç»ƒ -> è¯„æµ‹
# ä½¿ç”¨æ–¹æ³•: bash scripts/layer2_llm/run_full_pipeline.sh

set -e  # åªå¯ç”¨é”™è¯¯é€€å‡ºï¼Œä¸å¯ç”¨æœªå®šä¹‰å˜é‡æ£€æŸ¥ï¼ˆé¿å… conda æ¿€æ´»é—®é¢˜ï¼‰

# ============================================
# é…ç½®å‚æ•°
# ============================================

# é¡¹ç›®æ ¹ç›®å½•
MHMLM_ROOT="${MHMLM_ROOT:-/data1/chenyuxuan/MHMLM}"
cd "$MHMLM_ROOT"

# ç¯å¢ƒé…ç½®
CONDA_ENV="${CONDA_ENV:-llam3.2}"
# CUDA_VISIBLE_DEVICES ç”¨äºè®­ç»ƒï¼ˆå¤š GPUï¼‰
# æ•°æ®ç”Ÿæˆé˜¶æ®µä¼šå•ç‹¬è®¾ç½®ï¼ˆå• GPUï¼‰
TRAIN_CUDA_VISIBLE_DEVICES="${CUDA_VISIBLE_DEVICES:-5,6,7}"
NUM_GPUS="${NUM_GPUS:-4}"

# ============================================
# é˜¶æ®µ1: ç”Ÿæˆè®­ç»ƒæ•°æ®
# ============================================

# è¾“å…¥æ•°æ®ï¼ˆä½¿ç”¨ ChemBenchï¼‰
USE_CHEMBENCH="${USE_CHEMBENCH:-1}"  # 1: ä½¿ç”¨ ChemBench, 0: ä½¿ç”¨æ–‡ä»¶
CHEMBENCH_TASK="${CHEMBENCH_TASK:-product}"  # product, retro, yield
CHEMBENCH_SPLIT="${CHEMBENCH_SPLIT:-dev}"  # dev, test (ChemBench æ²¡æœ‰ trainï¼Œä½¿ç”¨ dev ä½œä¸ºè®­ç»ƒæ•°æ®)
TRAIN_DATA_INPUT="${TRAIN_DATA_INPUT:-}"  # å¦‚æœæŒ‡å®šåˆ™ä½¿ç”¨æ–‡ä»¶ï¼Œå¦åˆ™ä½¿ç”¨ ChemBench
TRAIN_DATA_OUTPUT="${TRAIN_DATA_OUTPUT:-${MHMLM_ROOT}/scripts/layer2_llm/data/training_data_${CHEMBENCH_TASK}_${CHEMBENCH_SPLIT}.jsonl}"

# æ¨¡å‹é…ç½®ï¼ˆç”¨äºç”Ÿæˆæ•°æ®ï¼‰
GEN_CONFIG="${GEN_CONFIG:-${MHMLM_ROOT}/configs/qwen3_sft_epoch2_3.yaml}"
# é»˜è®¤ä½¿ç”¨ GPU 6ï¼ˆå¦‚æœç”¨æˆ·åˆ†é…äº† GPU 6 å’Œ 7ï¼Œæ•°æ®ç”Ÿæˆç”¨ GPU 6ï¼‰
GEN_DEVICE="${GEN_DEVICE:-cuda:6}"
GEN_TASK_TYPE="${GEN_TASK_TYPE:-reaction_prediction}"

# ============================================
# é˜¶æ®µ2: è®­ç»ƒ LLM
# ============================================

# è®­ç»ƒé…ç½®
TRAIN_CONFIG="${TRAIN_CONFIG:-${MHMLM_ROOT}/configs/qwen3_sft_epoch2_3.yaml}"
TRAIN_OUTPUT_DIR="${TRAIN_OUTPUT_DIR:-/data1/chenyuxuan/checkpoint/qwen3_8b_layer2_llm_$(date +%Y%m%d_%H%M%S)}"
TRAIN_MASTER_PORT="${TRAIN_MASTER_PORT:-29500}"

# ============================================
# é˜¶æ®µ3: è¯„æµ‹ ChemBench
# ============================================

# è¯„æµ‹é…ç½®
EVAL_OUTPUT_DIR="${EVAL_OUTPUT_DIR:-${MHMLM_ROOT}/eval_chembench_layer2_llm_$(date +%Y%m%d_%H%M%S)}"
# é»˜è®¤ä½¿ç”¨ GPU 7ï¼ˆå¦‚æœç”¨æˆ·åˆ†é…äº† GPU 6 å’Œ 7ï¼Œè¯„æµ‹ç”¨ GPU 7ï¼‰
EVAL_DEVICE="${EVAL_DEVICE:-cuda:7}"
EVAL_SPLIT="${EVAL_SPLIT:-test}"
TOKEN_CLASSIFIER_PATH="${TOKEN_CLASSIFIER_PATH:-/data1/chenyuxuan/checkpoint/gnn_classifier/qwen3_mlp_token_head.pt}"

# ============================================
# è„šæœ¬è·¯å¾„
# ============================================

GENERATE_SCRIPT="${MHMLM_ROOT}/scripts/layer2_llm/generate_training_data.py"
TRAIN_SCRIPT="${MHMLM_ROOT}/train_sft.py"
EVAL_SCRIPT="${MHMLM_ROOT}/scripts/eval/eval_layer2_chembench.py"

# ============================================
# å¼€å§‹æ‰§è¡Œ
# ============================================

echo "============================================"
echo "Layer2-LLM å®Œæ•´æµç¨‹"
echo "============================================"
echo "é¡¹ç›®æ ¹ç›®å½•:     $MHMLM_ROOT"
echo "è®­ç»ƒ GPU:      $TRAIN_CUDA_VISIBLE_DEVICES"
echo "è®­ç»ƒè¾“å‡º:      $TRAIN_OUTPUT_DIR"
echo "è¯„æµ‹è¾“å‡º:      $EVAL_OUTPUT_DIR"
echo "============================================"

# æ¿€æ´»ç¯å¢ƒ
source /data1/chenyuxuan/MHMLM/.venv/bin/activate

# ============================================
# é˜¶æ®µ1: ç”Ÿæˆè®­ç»ƒæ•°æ®
# ============================================

echo ""
echo "============================================"
echo "é˜¶æ®µ1: ç”Ÿæˆè®­ç»ƒæ•°æ®"
echo "============================================"
if [ -n "$TRAIN_DATA_INPUT" ]; then
    echo "è¾“å…¥:  $TRAIN_DATA_INPUT"
else
    echo "è¾“å…¥:  ChemBench ($CHEMBENCH_TASK/$CHEMBENCH_SPLIT)"
fi
echo "è¾“å‡º:  $TRAIN_DATA_OUTPUT"
echo "é…ç½®:  $GEN_CONFIG"
echo "è®¾å¤‡:  $GEN_DEVICE"
echo ""

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$(dirname "$TRAIN_DATA_OUTPUT")"

# ç”Ÿæˆè®­ç»ƒæ•°æ®
# å¤„ç†è®¾å¤‡æ˜ å°„ï¼šå¦‚æœ GEN_DEVICE æ˜¯ cuda:Xï¼Œè®¾ç½® CUDA_VISIBLE_DEVICES=Xï¼Œç„¶åä½¿ç”¨ cuda:0
# æ³¨æ„ï¼šè®¾ç½® CUDA_VISIBLE_DEVICES åï¼Œç‰©ç† GPU ä¼šè¢«é‡æ–°æ˜ å°„ä¸ºé€»è¾‘ GPU 0,1,2...
# æ‰€ä»¥å¦‚æœè®¾ç½® CUDA_VISIBLE_DEVICES=6ï¼Œé‚£ä¹ˆç‰©ç† GPU 6 ä¼šå˜æˆé€»è¾‘ GPU 0
GEN_DEVICE_MAPPED="$GEN_DEVICE"
if [[ "$GEN_DEVICE" == cuda:* ]]; then
    GPU_ID=$(echo "$GEN_DEVICE" | sed 's/cuda://')
    export CUDA_VISIBLE_DEVICES="$GPU_ID"
    GEN_DEVICE_MAPPED="cuda:0"
    echo "ğŸ“Œ æ•°æ®ç”Ÿæˆè®¾å¤‡æ˜ å°„: ç‰©ç† GPU $GPU_ID -> é€»è¾‘ GPU 0"
fi

GEN_ARGS=(
    --output "$TRAIN_DATA_OUTPUT"
    --config "$GEN_CONFIG"
    --task_type "$GEN_TASK_TYPE"
    --device "$GEN_DEVICE_MAPPED"
)

# å¦‚æœæŒ‡å®šäº†è¾“å…¥æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶ï¼›å¦åˆ™ä½¿ç”¨ ChemBench
if [ -n "$TRAIN_DATA_INPUT" ] && [ -f "$TRAIN_DATA_INPUT" ]; then
    echo "ğŸ“‚ ä½¿ç”¨è¾“å…¥æ–‡ä»¶: $TRAIN_DATA_INPUT"
    GEN_ARGS+=( --input "$TRAIN_DATA_INPUT" )
else
    echo "ğŸ“‚ ä½¿ç”¨ ChemBench æ•°æ®: task=$CHEMBENCH_TASK, split=$CHEMBENCH_SPLIT"
    GEN_ARGS+=( --use_chembench )
    GEN_ARGS+=( --chembench_task "$CHEMBENCH_TASK" )
    GEN_ARGS+=( --chembench_split "$CHEMBENCH_SPLIT" )
fi

CUDA_VISIBLE_DEVICES="$CUDA_VISIBLE_DEVICES" python "$GENERATE_SCRIPT" "${GEN_ARGS[@]}"

if [ ! -f "$TRAIN_DATA_OUTPUT" ]; then
    echo "âŒ è®­ç»ƒæ•°æ®ç”Ÿæˆå¤±è´¥ï¼"
    exit 1
fi

DATA_COUNT=$(wc -l < "$TRAIN_DATA_OUTPUT")
echo "âœ… è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ: $DATA_COUNT æ¡"

# ============================================
# é˜¶æ®µ2: è®­ç»ƒ LLM
# ============================================

echo ""
echo "============================================"
echo "é˜¶æ®µ2: è®­ç»ƒ LLM"
echo "============================================"
echo "è®­ç»ƒæ•°æ®:  $TRAIN_DATA_OUTPUT"
echo "é…ç½®æ–‡ä»¶:  $TRAIN_CONFIG"
echo "è¾“å‡ºç›®å½•:  $TRAIN_OUTPUT_DIR"
echo "GPUæ•°é‡:   $NUM_GPUS"
echo ""

# æ£€æŸ¥è®­ç»ƒæ•°æ®
if [ ! -f "$TRAIN_DATA_OUTPUT" ]; then
    echo "âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: $TRAIN_DATA_OUTPUT"
    exit 1
fi

# æ£€æŸ¥é…ç½®æ–‡ä»¶
if [ ! -f "$TRAIN_CONFIG" ]; then
    echo "âŒ è®­ç»ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $TRAIN_CONFIG"
    exit 1
fi

# åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼ˆæ›´æ–°æ•°æ®è·¯å¾„ï¼‰
TEMP_CONFIG="${TRAIN_CONFIG%.yaml}_layer2_temp.yaml"
cp "$TRAIN_CONFIG" "$TEMP_CONFIG"

# æ›´æ–°æ•°æ®è·¯å¾„ï¼ˆä½¿ç”¨ Python æˆ– sedï¼‰
python3 << EOF
import yaml
import sys

with open("$TEMP_CONFIG", 'r') as f:
    config = yaml.safe_load(f)

# æ›´æ–°æ•°æ®è·¯å¾„
if 'data' not in config:
    config['data'] = {}
config['data']['dataset_path'] = "$TRAIN_DATA_OUTPUT"

with open("$TEMP_CONFIG", 'w') as f:
    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

print(f"âœ… å·²æ›´æ–°é…ç½®æ–‡ä»¶: $TEMP_CONFIG")
print(f"   æ•°æ®è·¯å¾„: $TRAIN_DATA_OUTPUT")
EOF

# ä½¿ç”¨ä¸´æ—¶é…ç½®æ–‡ä»¶
ACTUAL_TRAIN_CONFIG="$TEMP_CONFIG"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$TRAIN_OUTPUT_DIR"

# è®­ç»ƒ LLM
echo "å¼€å§‹è®­ç»ƒ..."
CUDA_VISIBLE_DEVICES="$TRAIN_CUDA_VISIBLE_DEVICES" torchrun \
    --master_port="$TRAIN_MASTER_PORT" \
    --nproc_per_node="$NUM_GPUS" \
    "$TRAIN_SCRIPT" \
    --config "$ACTUAL_TRAIN_CONFIG" \
    --output_dir "$TRAIN_OUTPUT_DIR"

# æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
if [ -f "$TEMP_CONFIG" ]; then
    rm "$TEMP_CONFIG"
    echo "âœ… å·²æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶"
fi

# æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸï¼ˆæŸ¥æ‰¾æœ€æ–°çš„ checkpointï¼‰
# ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿æ–‡ä»¶ç³»ç»ŸåŒæ­¥
sleep 2

LATEST_CKPT=$(find "$TRAIN_OUTPUT_DIR" -name "checkpoint-*" -type d 2>/dev/null | sort -V | tail -1)
if [ -z "$LATEST_CKPT" ]; then
    echo "âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°è®­ç»ƒ checkpointï¼Œå¯èƒ½è®­ç»ƒå¤±è´¥æˆ–è¿˜åœ¨è®­ç»ƒä¸­"
    echo "   è¾“å‡ºç›®å½•: $TRAIN_OUTPUT_DIR"
    echo ""
    read -p "æ˜¯å¦ä½¿ç”¨è¾“å‡ºç›®å½•ä½œä¸º checkpoint ç»§ç»­è¯„æµ‹ï¼Ÿ(y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        echo "âŒ å·²å–æ¶ˆè¯„æµ‹"
        exit 1
    fi
    # ä½¿ç”¨è¾“å‡ºç›®å½•ä½œä¸º checkpoint
    MOLAWARE_CKPT="$TRAIN_OUTPUT_DIR"
    echo "   ä½¿ç”¨è¾“å‡ºç›®å½•: $MOLAWARE_CKPT"
else
    # ä½¿ç”¨æœ€æ–°çš„ checkpoint
    MOLAWARE_CKPT="$LATEST_CKPT"
    echo "âœ… è®­ç»ƒå®Œæˆï¼Œæ‰¾åˆ° checkpoint: $MOLAWARE_CKPT"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ llm å­ç›®å½•ï¼ˆæŸäº›é…ç½®å¯èƒ½ä½¿ç”¨ï¼‰
    if [ -d "$MOLAWARE_CKPT/llm" ]; then
        MOLAWARE_CKPT="$MOLAWARE_CKPT/llm"
        echo "   ä½¿ç”¨ llm å­ç›®å½•: $MOLAWARE_CKPT"
    fi
fi

# ============================================
# é˜¶æ®µ3: è¯„æµ‹ ChemBench
# ============================================

echo ""
echo "============================================"
echo "é˜¶æ®µ3: è¯„æµ‹ ChemBench"
echo "============================================"
echo "Checkpoint:  $MOLAWARE_CKPT"
echo "è¾“å‡ºç›®å½•:    $EVAL_OUTPUT_DIR"
echo "è¯„æµ‹åˆ’åˆ†:    $EVAL_SPLIT"
echo ""

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$EVAL_OUTPUT_DIR"

# è¯„æµ‹ä¸‰ä¸ªä»»åŠ¡
TASKS=("product" "retro" "yield")

for task in "${TASKS[@]}"; do
    echo ""
    echo "--------------------------------------------"
    echo "è¯„æµ‹ä»»åŠ¡: $task"
    echo "--------------------------------------------"
    
    # å¤„ç†è®¾å¤‡æ˜ å°„ï¼šå¦‚æœ EVAL_DEVICE æ˜¯ cuda:Xï¼Œè®¾ç½® CUDA_VISIBLE_DEVICES=Xï¼Œç„¶åä½¿ç”¨ cuda:0
    EVAL_CUDA_VISIBLE_DEVICES=""
    EVAL_DEVICE_MAPPED="$EVAL_DEVICE"
    if [[ "$EVAL_DEVICE" == cuda:* ]]; then
        EVAL_GPU_ID=$(echo "$EVAL_DEVICE" | sed 's/cuda://')
        EVAL_CUDA_VISIBLE_DEVICES="$EVAL_GPU_ID"
        EVAL_DEVICE_MAPPED="cuda:0"
        echo "ğŸ“Œ è¯„æµ‹è®¾å¤‡æ˜ å°„: ç‰©ç† GPU $EVAL_GPU_ID -> é€»è¾‘ GPU 0"
    fi
    
    CUDA_VISIBLE_DEVICES="$EVAL_CUDA_VISIBLE_DEVICES" python "$EVAL_SCRIPT" \
        --task "$task" \
        --split "$EVAL_SPLIT" \
        --molaware_ckpt "$MOLAWARE_CKPT" \
        --token_classifier_path "$TOKEN_CLASSIFIER_PATH" \
        --device "$EVAL_DEVICE_MAPPED" \
        --dtype bf16 \
        --out_dir "$EVAL_OUTPUT_DIR" \
        --use_layer2_pipeline 1 \
        --max_new_tokens 256 \
        --temperature 0.2 \
        --top_p 0.9
    
    echo "âœ… ä»»åŠ¡ $task è¯„æµ‹å®Œæˆ"
done

# ============================================
# æ€»ç»“
# ============================================

echo ""
echo "============================================"
echo "âœ… å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆï¼"
echo "============================================"
echo ""
echo "è®­ç»ƒæ•°æ®:     $TRAIN_DATA_OUTPUT"
echo "è®­ç»ƒè¾“å‡º:     $TRAIN_OUTPUT_DIR"
echo "è¯„æµ‹è¾“å‡º:     $EVAL_OUTPUT_DIR"
echo ""
echo "è¯„æµ‹ç»“æœæ–‡ä»¶:"
echo "  - $EVAL_OUTPUT_DIR/pred_product.jsonl"
echo "  - $EVAL_OUTPUT_DIR/pred_retro.jsonl"
echo "  - $EVAL_OUTPUT_DIR/pred_yield.jsonl"
echo ""
echo "è¯¦ç»†ç»“æœ:"
for task in "${TASKS[@]}"; do
    echo "  - $EVAL_OUTPUT_DIR/chembench4k_${task}_${EVAL_SPLIT}_predictions.jsonl"
    echo "  - $EVAL_OUTPUT_DIR/chembench4k_${task}_${EVAL_SPLIT}_summary.json"
done
echo ""
