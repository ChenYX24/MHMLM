#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šæ¯”è¾ƒä¸åŒæ¨¡å‹å’Œæ¨ç†æ–¹å¼çš„ç»“æœ
- æ¨¡å‹ï¼šllasmol, intern-s1
- æ¨ç†æ–¹å¼ï¼šsft_tester, æ™®é€šæ¨ç†ï¼ˆtransformersï¼‰
- æµ‹è¯•æ‰€æœ‰jsonlæ–‡ä»¶çš„ç¬¬ä¸€ä¸ªprompt

ä½¿ç”¨æ–¹æ³•ï¼š
    python test_prompts_comparison.py --model llasmol --type sft_tester
    python test_prompts_comparison.py --model llasmol --type transformers
    python test_prompts_comparison.py --model intern-s1 --type sft_tester
    python test_prompts_comparison.py --model intern-s1 --type transformers
"""

import json
import os
import sys
import argparse
import inspect
from pathlib import Path
from typing import Dict, List, Any, Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# æ·»åŠ é¡¹ç›®è·¯å¾„
BASE_DIR = Path("/data1/chenyuxuan/MHMLM")
sys.path.insert(0, str(BASE_DIR))

from sft_tester import MolAwareGenerator2

# é…ç½®
TEST_DIR = BASE_DIR / "1223results_baseline/LlaSMol-Mistral-7B-merged_fewshot"
MODEL_DIR = Path("/data1/chenyuxuan/base_model")

# æ¨¡å‹è·¯å¾„é…ç½®
MODELS = {
    "llasmol": MODEL_DIR / "LlaSMol-Mistral-7B-merged",
    "intern-s1": MODEL_DIR / "Intern-S1-mini",
}

# Intern-S1 éœ€è¦ base_llm_path
BASE_LLM_PATHS = {
    "llasmol": None,
    "intern-s1": MODEL_DIR / "qwen3_8b",
}

TOKEN_CLS_PATH = "/data1/lvchangwei/LLM/Lora/llama_mlp_token_classifier.pt"
OUTPUT_FILE = BASE_DIR / "1223results_baseline/prompt_comparison_results.jsonl"

# ç»Ÿä¸€åå¤„ç†å‡½æ•°
import re

YESNO_RE = re.compile(r"\b(Yes|No)\b", re.IGNORECASE)
FLOAT_RE = re.compile(r"[-+]?\d+(\.\d+)?")

# æ’é™¤çš„ jsonl æ–‡ä»¶å…³é”®å­—
EXCLUDE_SUBSTR = ["metrics", "eval_summary", "evaluation", "result", "score"]


def safe_apply_chat_template(tokenizer, messages, extra_kwargs=None):
    """å®‰å…¨åœ°è°ƒç”¨ apply_chat_templateï¼Œåªä¼ é€’æ”¯æŒçš„å‚æ•°"""
    extra_kwargs = extra_kwargs or {}
    if not hasattr(tokenizer, "apply_chat_template") or getattr(tokenizer, "chat_template", None) is None:
        return None
    try:
        sig = inspect.signature(tokenizer.apply_chat_template)
        supported = set(sig.parameters.keys())
        kwargs = {"tokenize": False, "add_generation_prompt": True}
        for k, v in extra_kwargs.items():
            if k in supported:
                kwargs[k] = v
        return tokenizer.apply_chat_template(messages, **kwargs)
    except Exception:
        return None


def format_chat_prompt(tokenizer, prompt, model_name, system_msg="You are a careful chemist. Follow the requested output format exactly."):
    """ç»Ÿä¸€æ ¼å¼åŒ– chat promptï¼Œç¡®ä¿ä¸¤ç§æ¨ç†è·¯å¾„ä½¿ç”¨ç›¸åŒçš„æ ¼å¼"""
    messages = [{"role": "system", "content": system_msg}, {"role": "user", "content": prompt}]
    extra = {}
    if model_name == "intern-s1":
        extra["enable_thinking"] = False
    text = safe_apply_chat_template(tokenizer, messages, extra_kwargs=extra)
    if text is not None:
        return text
    # fallback
    return f"System: {system_msg}\n\nUser: {prompt}\n\nAssistant: "


def infer_input_device(model):
    """æ¨æ–­è¾“å…¥åº”è¯¥æ”¾åœ¨å“ªä¸ªè®¾å¤‡ä¸Šï¼ˆå¤„ç†å¤šå¡æƒ…å†µï¼‰"""
    if hasattr(model, "device") and str(model.device) not in ("meta", "cpu"):
        return model.device
    if hasattr(model, "hf_device_map"):
        # ä¼˜å…ˆæ‰¾ embedding ç›¸å…³ key
        for k, v in model.hf_device_map.items():
            if any(x in k for x in ["embed", "wte", "word_embeddings", "tok_embeddings"]):
                return v
        # å†æ‰¾ç¬¬ä¸€ä¸ª cuda
        for v in model.hf_device_map.values():
            if isinstance(v, str) and v.startswith("cuda"):
                return v
    return "cuda:0" if torch.cuda.is_available() else "cpu"


def strip_thinking(t: str) -> str:
    """æå– thinking æ ‡ç­¾åçš„æœ€ç»ˆç­”æ¡ˆï¼ˆå¤šåˆ†éš”ç¬¦å…œåº•ï¼‰"""
    if not t:
        return t
    # å…ˆå¤„ç†æ˜¾å¼ think æ ‡ç­¾
    if "<think>" in t and "</think>" in t:
        return t.split("</think>", 1)[1].strip()
    # å†å°è¯•ç”¨å¸¸è§åˆ†éš”ç¬¦æˆªå–æœ€åä¸€æ®µ"åƒç­”æ¡ˆ"çš„å†…å®¹
    THINK_SPLITS = ["</think>", "</think>", "</thinking>", "<|im_end|>", "<|endoftext|>"]
    for sp in THINK_SPLITS:
        if sp in t:
            tail = t.split(sp)[-1].strip()
            if tail:
                return tail
    return t


def clean_output(task: str, text: str) -> str:
    """ç»Ÿä¸€åå¤„ç†ï¼šæ¸…ç†è¾“å‡ºæ ¼å¼ï¼Œæå–æœ€ç»ˆç­”æ¡ˆ"""
    if text is None or not text:
        return ""
    
    # 1) å»æ‰å¸¸è§ special token / å°¾å·´
    t = text.replace("<|im_end|>", "").replace("</s>", "").strip()
    
    # 2) å¤„ç† thinkingï¼šæå–æœ€ç»ˆç­”æ¡ˆ
    t = strip_thinking(t)
    
    # 3) æå–æ ‡ç­¾å†…å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    #   <SMILES> ... </SMILES> / <SOL> ... </SOL> / <SOLUTION> ... </SOLUTION>
    for tag in ["SMILES", "SOL", "SOLUTION", "MOLFORMULA"]:
        m = re.search(rf"<{tag}>\s*(.*?)\s*</{tag}>", t, flags=re.S)
        if m:
            t = m.group(1).strip()
            break
    
    # 4) åªå–ç¬¬ä¸€è¡Œï¼ˆå¤§å¤šæ•°ä»»åŠ¡éƒ½è¦æ±‚å•è¡Œï¼‰
    t = t.splitlines()[0].strip()
    
    # 5) task-specific å…œåº•æŠ½å–
    if task.startswith("property_prediction-") and task not in ["property_prediction-esol", "property_prediction-lipo"]:
        m = YESNO_RE.search(t)
        if m:
            return "Yes" if m.group(1).lower() == "yes" else "No"
        return t  # å®åœ¨ä¸è¡ŒåŸæ ·è¿”å›ï¼Œæ–¹ä¾¿æ’æŸ¥
    
    if task in ["property_prediction-esol", "property_prediction-lipo"]:
        m = FLOAT_RE.search(t)
        return m.group(0) if m else t
    
    # smiles ç±»ä»»åŠ¡ï¼šç®€å•å»ç©ºæ ¼
    if task in ["forward_synthesis", "retrosynthesis", "molecule_generation"]:
        return t.replace(" ", "")
    
    return t


def gen_config_for_task(task: str) -> Dict[str, Any]:
    """æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›åˆé€‚çš„ç”Ÿæˆé…ç½®"""
    if task.startswith("property_prediction-") and task not in ["property_prediction-esol", "property_prediction-lipo"]:
        # Yes/No åˆ†ç±»ä»»åŠ¡ï¼šgreedy + å° max_new_tokens
        return {
            "max_new_tokens": 8,
            "do_sample": False,
            "temperature": 0.0,
            "top_p": 1.0,
            "repetition_penalty": 1.06,
            "no_repeat_ngram_size": 3,
        }
    if task in ["property_prediction-esol", "property_prediction-lipo"]:
        # æ•°å€¼å›å½’ä»»åŠ¡
        return {
            "max_new_tokens": 32,
            "do_sample": False,
            "temperature": 0.0,
            "top_p": 1.0,
            "repetition_penalty": 1.06,
            "no_repeat_ngram_size": 3,
        }
    if task in ["forward_synthesis", "retrosynthesis", "molecule_generation"]:
        # SMILES ç”Ÿæˆä»»åŠ¡
        return {
            "max_new_tokens": 256,
            "do_sample": False,
            "temperature": 0.0,
            "top_p": 1.0,
            "repetition_penalty": 1.06,
            "no_repeat_ngram_size": 3,
        }
    # å…¶ä»–ä»»åŠ¡ï¼ˆname_conversion, molecule_captioning ç­‰ï¼‰
    return {
        "max_new_tokens": 256,
        "do_sample": True,
        "temperature": 0.2,
        "top_p": 0.9,
        "repetition_penalty": 1.06,
        "no_repeat_ngram_size": 3,
    }


def load_first_prompt_from_jsonl(jsonl_path: Path) -> Dict[str, Any]:
    """ä»jsonlæ–‡ä»¶åŠ è½½ç¬¬ä¸€ä¸ªprompt"""
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        first_line = f.readline().strip()
        if first_line:
            return json.loads(first_line)
    return None


def find_all_jsonl_files(test_dir: Path) -> List[Path]:
    """æŸ¥æ‰¾æ‰€æœ‰jsonlæ–‡ä»¶ï¼ˆæ’é™¤è¯„ä¼°ç»“æœç›¸å…³æ–‡ä»¶ï¼‰"""
    jsonl_files = []
    
    for file in test_dir.glob("*.jsonl"):
        # æŒ‰åå­—åŒ…å«å…³é”®å­—æ’é™¤
        if any(s in file.name.lower() for s in EXCLUDE_SUBSTR):
            continue
        jsonl_files.append(file)
    
    return sorted(jsonl_files)


def load_sft_tester_generator(
    model_name: str,
    model_path: Path,
    base_llm_path: Path = None
) -> MolAwareGenerator2:
    """åŠ è½½ sft_tester generatorï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰"""
    generator = MolAwareGenerator2()
    
    cfg = {
        "ckpt_dir": str(model_path),
        "device": "cuda:0" if torch.cuda.is_available() else "cpu",
        "dtype": "bf16",
        "debug": False,
        "enable_thinking": False,
        "realtime_mol": False,
    }
    
    if base_llm_path:
        cfg["base_llm_path"] = str(base_llm_path)
    
    if model_name == "llasmol":
        cfg["token_classifier_path"] = TOKEN_CLS_PATH
    
    generator.load(cfg)
    return generator


def inference_with_sft_tester(
    generator: MolAwareGenerator2,
    prompt: str,
    task_name: str,
    model_name: str
) -> str:
    """ä½¿ç”¨å·²åŠ è½½çš„ sft_tester generator è¿›è¡Œæ¨ç†"""
    # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ç”Ÿæˆé…ç½®
    gen_config = gen_config_for_task(task_name)
    
    # å¯¹äºé SMILES ä»»åŠ¡ï¼ˆproperty_prediction, name_conversion-s2iï¼‰ï¼Œ
    # llasmol çš„ sft_tester å¯èƒ½ä¼šè¿‡æ»¤æ‰éåˆ†å­å†…å®¹ï¼Œå»ºè®®ç”¨ transformers
    # ä½†è¿™é‡Œå…ˆå°è¯•ï¼Œå¦‚æœè¿”å›ç©ºå†å¤„ç†
    
    # Intern-S1 éœ€è¦å¼ºåˆ¶å…³é—­ thinking
    kwargs = {
        "add_dialog_wrapper": True,
        "realtime_mol": False,
        **gen_config
    }
    
    # å¦‚æœ generator æ”¯æŒ enable_thinking å‚æ•°ï¼Œä¼ é€’å®ƒ
    # æ³¨æ„ï¼šè¿™å–å†³äº sft_tester çš„å®ç°
    try:
        result = generator.generate(prompt, **kwargs)
    except TypeError:
        # å¦‚æœä¸æ”¯æŒæŸäº›å‚æ•°ï¼Œä½¿ç”¨åŸºæœ¬å‚æ•°
        result = generator.generate(
            prompt,
            add_dialog_wrapper=True,
            realtime_mol=False,
            max_new_tokens=gen_config.get("max_new_tokens", 256),
            temperature=gen_config.get("temperature", 0.2),
            top_p=gen_config.get("top_p", 0.9),
            repetition_penalty=gen_config.get("repetition_penalty", 1.06),
            no_repeat_ngram_size=gen_config.get("no_repeat_ngram_size", 3),
        )
    
    return result


def load_transformers_model(
    model_name: str,
    model_path: Path,
    base_llm_path: Path = None
) -> tuple:
    """åŠ è½½ transformers æ¨¡å‹å’Œ tokenizerï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰"""
    # åŠ è½½ tokenizer
    if base_llm_path and model_name == "intern-s1":
        tokenizer_path = base_llm_path
    else:
        tokenizer_path = model_path
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            str(tokenizer_path),
            trust_remote_code=True
        )
    except Exception as e:
        raise RuntimeError(f"åŠ è½½ tokenizer å¤±è´¥: {e}")
    
    # åŠ è½½æ¨¡å‹
    try:
        model = AutoModelForCausalLM.from_pretrained(
            str(model_path),
            torch_dtype=torch.bfloat16,  # ä½¿ç”¨ torch_dtypeï¼ˆæ ‡å‡† APIï¼‰
            device_map="auto",
            trust_remote_code=True
        )
        model.eval()
    except Exception as e:
        raise RuntimeError(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    
    return tokenizer, model


def inference_with_transformers(
    model_name: str,
    tokenizer: AutoTokenizer,
    model: AutoModelForCausalLM,
    prompt: str,
    task_name: str
) -> str:
    """ä½¿ç”¨å·²åŠ è½½çš„ transformers æ¨¡å‹è¿›è¡Œæ¨ç†"""
    # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ç”Ÿæˆé…ç½®
    gen_config = gen_config_for_task(task_name)
    
    # æ ¼å¼åŒ– promptï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ format_chat_promptï¼Œç¡®ä¿ä¸ sft_tester ä¸€è‡´ï¼‰
    system_msg = "You are a careful chemist. Follow the requested output format exactly."
    formatted_prompt = format_chat_prompt(tokenizer, prompt, model_name, system_msg)
    
    # Tokenize
    try:
        inputs = tokenizer(formatted_prompt, return_tensors="pt")
        input_ids_len = inputs["input_ids"].shape[1]  # ä¿å­˜åŸå§‹é•¿åº¦ï¼Œç”¨äºåç»­è§£ç 
        
        # ç§»åŠ¨è¾“å…¥åˆ°æ¨¡å‹è®¾å¤‡ï¼ˆä½¿ç”¨æ”¹è¿›çš„è®¾å¤‡æ¨æ–­ï¼‰
        dev = infer_input_device(model)
        inputs = {k: v.to(dev) for k, v in inputs.items()}
    except Exception as e:
        raise RuntimeError(f"Tokenize å¤±è´¥: {e}")
    
    # Generate
    try:
        # è®¾ç½® eos_token_id å’Œ pad_token_id
        generate_kwargs = {
            **inputs,
            **gen_config,
        }
        
        # ç¡®ä¿è®¾ç½®äº† eos_token_id
        if "eos_token_id" not in generate_kwargs:
            generate_kwargs["eos_token_id"] = tokenizer.eos_token_id
        if "pad_token_id" not in generate_kwargs:
            generate_kwargs["pad_token_id"] = tokenizer.eos_token_id
        
        with torch.no_grad():
            outputs = model.generate(**generate_kwargs)
    except Exception as e:
        raise RuntimeError(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    # Decode
    try:
        # å¯¹äº intern-s1ï¼Œå…ˆä¸ skip special tokensï¼Œé¿å…æŠŠå…³é”®å†…å®¹è·³æ‰
        # åç»­é€šè¿‡ clean_output ç»Ÿä¸€å¤„ç†
        skip_special = model_name != "intern-s1"
        generated_text = tokenizer.decode(
            outputs[0][input_ids_len:],
            skip_special_tokens=skip_special
        )
    except Exception as e:
        raise RuntimeError(f"è§£ç å¤±è´¥: {e}")
    
    return generated_text


def collect_all_prompts(jsonl_files: List[Path]) -> List[Dict[str, Any]]:
    """é¢„å…ˆæ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„ prompt"""
    tasks = []
    print(f"ğŸ“‹ æ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„ prompt...")
    
    for jsonl_file in jsonl_files:
        task_name = jsonl_file.stem
        print(f"  ğŸ“ {task_name}...")
        
        # åŠ è½½ç¬¬ä¸€ä¸ª prompt
        data = load_first_prompt_from_jsonl(jsonl_file)
        if not data:
            print(f"    âš ï¸  è·³è¿‡ï¼ˆæ–‡ä»¶ä¸ºç©ºï¼‰")
            continue
        
        prompt = data.get("prompt", "")
        gold = data.get("gold", "")
        input_text = data.get("input", "")
        
        if not prompt:
            print(f"    âš ï¸  è·³è¿‡ï¼ˆæ²¡æœ‰ promptï¼‰")
            continue
        
        tasks.append({
            "task": task_name,
            "prompt": prompt,
            "input": input_text,
            "gold": gold,
        })
        print(f"    âœ“ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
    
    print(f"\nâœ… å…±æ”¶é›†åˆ° {len(tasks)} ä¸ªä»»åŠ¡\n")
    return tasks


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æµ‹è¯•è„šæœ¬ï¼šæ¯”è¾ƒä¸åŒæ¨¡å‹å’Œæ¨ç†æ–¹å¼çš„ç»“æœ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•ï¼ˆéœ€è¦è¿è¡Œå››æ¬¡ï¼Œæ¯æ¬¡ä¸åŒçš„ settingï¼‰ï¼š
  python test_prompts_comparison.py --model llasmol --type sft_tester
  python test_prompts_comparison.py --model llasmol --type transformers
  python test_prompts_comparison.py --model intern-s1 --type sft_tester
  python test_prompts_comparison.py --model intern-s1 --type transformers
        """
    )
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        choices=["llasmol", "intern-s1"],
        help="è¦ä½¿ç”¨çš„æ¨¡å‹åç§°"
    )
    parser.add_argument(
        "--type",
        type=str,
        required=True,
        choices=["sft_tester", "transformers"],
        help="æ¨ç†æ–¹å¼"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šæ ¹æ® model å’Œ type è‡ªåŠ¨ç”Ÿæˆï¼‰"
    )
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    model_name = args.model
    inference_type = args.type
    
    print(f"ğŸš€ è¿è¡Œè®¾ç½®: model={model_name}, type={inference_type}\n")
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if model_name not in MODELS:
        print(f"âŒ æœªçŸ¥çš„æ¨¡å‹: {model_name}")
        return
    
    model_path = MODELS[model_name]
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    base_llm_path = BASE_LLM_PATHS.get(model_name)
    if base_llm_path and not base_llm_path.exists():
        print(f"âŒ Base LLM è·¯å¾„ä¸å­˜åœ¨: {base_llm_path}")
        return
    
    # æ­¥éª¤1: æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶å¹¶æ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„ prompt
    print(f"ğŸ” æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶...")
    jsonl_files = find_all_jsonl_files(TEST_DIR)
    print(f"æ‰¾åˆ° {len(jsonl_files)} ä¸ª jsonl æ–‡ä»¶\n")
    
    tasks = collect_all_prompts(jsonl_files)
    
    if not tasks:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ï¼Œé€€å‡º")
        return
    
    # æ­¥éª¤2: åŠ è½½æŒ‡å®šçš„æ¨¡å‹å’Œæ¨ç†æ–¹å¼
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_name} ({inference_type})...")
    
    loaded_model = None
    if inference_type == "sft_tester":
        try:
            sft_generator = load_sft_tester_generator(model_name, model_path, base_llm_path)
            loaded_model = sft_generator
            print(f"âœ… sft_tester generator åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"âŒ sft_tester generator åŠ è½½å¤±è´¥: {e}")
            return
    else:  # transformers
        try:
            tokenizer, model = load_transformers_model(model_name, model_path, base_llm_path)
            loaded_model = (tokenizer, model)
            print(f"âœ… transformers æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"âŒ transformers æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return
    
    print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡...\n")
    
    # æ­¥éª¤3: å¤„ç†æ‰€æœ‰ä»»åŠ¡
    results = []
    
    for task_idx, task_data in enumerate(tasks, 1):
        task_name = task_data["task"]
        prompt = task_data["prompt"]
        gold = task_data["gold"]
        input_text = task_data["input"]
        
        print(f"[{task_idx}/{len(tasks)}] ğŸ“ å¤„ç†ä»»åŠ¡: {task_name}")
        print(f"  Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        try:
            if inference_type == "sft_tester":
                print(f"    â†’ sft_tester æ¨ç†ä¸­...")
                result_raw = inference_with_sft_tester(loaded_model, prompt, task_name, model_name)
            else:  # transformers
                print(f"    â†’ transformers æ¨ç†ä¸­...")
                tokenizer, model = loaded_model
                result_raw = inference_with_transformers(model_name, tokenizer, model, prompt, task_name)
            
            # ç»Ÿä¸€åå¤„ç†ï¼šæ¸…ç†è¾“å‡º
            result_clean = clean_output(task_name, result_raw)
            
            results.append({
                "task": task_name,
                "model": model_name,
                "type": inference_type,
                "prompt": prompt,
                "input": input_text,
                "gold": gold,
                "prediction_raw": result_raw,  # ä¿ç•™åŸå§‹è¾“å‡ºç”¨äº debug
                "prediction": result_clean,    # æ¸…ç†åçš„è¾“å‡ºç”¨äºè¯„æµ‹
            })
            
            # å¦‚æœæ¸…ç†åä¸ºç©ºä½†åŸå§‹ä¸ä¸ºç©ºï¼Œç»™å‡ºè­¦å‘Š
            if not result_clean and result_raw:
                print(f"    âš ï¸  è­¦å‘Šï¼šæ¸…ç†åè¾“å‡ºä¸ºç©ºï¼ˆåŸå§‹è¾“å‡ºé•¿åº¦: {len(result_raw)}ï¼‰")
            
            print(f"    âœ“ å®Œæˆ (åŸå§‹: {len(result_raw)} å­—ç¬¦, æ¸…ç†å: {len(result_clean)} å­—ç¬¦)")
        except Exception as e:
            print(f"    âœ— å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results.append({
                "task": task_name,
                "model": model_name,
                "type": inference_type,
                "prompt": prompt,
                "input": input_text,
                "gold": gold,
                "prediction_raw": f"ERROR: {str(e)}",
                "prediction": f"ERROR: {str(e)}",
            })
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
    if args.output:
        output_file = Path(args.output)
    else:
        # æ ¹æ® model å’Œ type è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
        output_file = OUTPUT_FILE.parent / f"prompt_comparison_{model_name}_{inference_type}.jsonl"
    
    # ä¿å­˜ç»“æœ
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    print(f"\nâœ… å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"   å…± {len(results)} æ¡ç»“æœ")
    print(f"   è®¾ç½®: model={model_name}, type={inference_type}")


if __name__ == "__main__":
    main()

