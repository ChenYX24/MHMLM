# train_sft.py - ç®€åŒ–ç‰ˆï¼Œä½¿ç”¨æ¨¡å—åŒ–åˆå§‹åŒ–
import os
import re
import glob
import json
import time
import yaml
import random
import numpy as np
from typing import Optional
from datetime import timedelta
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, TrainerCallback
)
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM
try:
    from trl import SFTConfig
except ImportError:
    # å¦‚æœ SFTConfig ä¸å¯ç”¨ï¼Œä½¿ç”¨ None ä½œä¸ºæ ‡è®°
    SFTConfig = None
from dataclasses import dataclass
from typing import List, Dict, Any

# ---------------------------------------------------------------------
# å®‰å…¨ååºåˆ—åŒ–ï¼ˆé«˜ç‰ˆæœ¬ PyTorch æ”¯æŒ add_safe_globalsï¼Œä½ç‰ˆæœ¬ç›´æ¥è·³è¿‡ï¼‰â€”â€”å…è®¸ numpy ndarray
import numpy
if hasattr(torch.serialization, "add_safe_globals"):
    torch.serialization.add_safe_globals([numpy.dtype])

# ---------------------------------------------------------------------
# ç¯å¢ƒä¸æ—¥å¿—
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
# è®¾ç½® PyTorch CUDA å†…å­˜åˆ†é…å™¨ä»¥å‡å°‘ç¢ç‰‡åŒ–
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

# SwanLab ä»…åœ¨ rank0 åˆå§‹åŒ–
def _env_rank():
    r = os.environ.get("RANK")
    try:
        return int(r) if r is not None else 0
    except Exception:
        return 0

_IS_RANK0_ENV = (_env_rank() == 0)
import swanlab
swanlab.init(
    project="mol-sft-simple",
    experiment_name="exp-001",
    description="SFT with <mol> embedding-append",
    mode="online" if _IS_RANK0_ENV else "offline"
)

# å¯¼å…¥æ¨¡å—
from modules.model_init import (
    init_tokenizer, init_llm, init_model, init_offline_token_classifier
)
from modules.data_loader import (
    load_training_data, compute_qm9_stats_from_dataset
)
from modules.mol_aware_lm import MolAwareCausalLM

# LDMol æ”¯æŒï¼ˆä»…ç”¨äºæ¨ç†ï¼Œè®­ç»ƒæ—¶ä¸éœ€è¦ï¼‰
# æ³¨æ„ï¼štrainer.py ä¸­çš„ init_ldmol_components å’Œ compute_ldmol_loss åœ¨è®­ç»ƒæ—¶ä¸éœ€è¦
# LDMol ç»„ä»¶åœ¨æ¨ç†æ—¶é€šè¿‡ sft_tester.py å’Œ mol_aware_lm.py åˆå§‹åŒ–
LDMOL_AVAILABLE = True  # æ ‡è®°ä¸ºå¯ç”¨ï¼Œä½†è®­ç»ƒæ—¶ä¸ä½¿ç”¨


# ======================== å·¥å…·å‡½æ•° ========================
def safe_barrier():
    try:
        if dist.is_available() and dist.is_initialized():
            dist.barrier()
    except Exception:
        pass

def latest_checkpoint(output_dir: str) -> Optional[str]:
    """è·å–æœ€æ–°çš„checkpoint"""
    if not os.path.isdir(output_dir):
        return None
    checkpoints = glob.glob(os.path.join(output_dir, "checkpoint-*"))
    if not checkpoints:
        return None
    checkpoints = sorted(
        checkpoints,
        key=lambda x: int(x.split("-")[-1]) if x.split("-")[-1].isdigit() else -1
    )
    return checkpoints[-1]

def _ensure_root_weight_link(ckpt_dir: str, llm_dir: str):
    """ç¡®ä¿ checkpoint æ ¹ç›®å½•æœ‰ä¸€ä»½ HF è®¤å¯çš„æƒé‡"""
    src = Path(llm_dir) / "model.safetensors"
    dst = Path(ckpt_dir) / "model.safetensors"
    if not src.exists():
        return
    if dst.exists():
        return
    try:
        os.link(src, dst)
        print(f"[SaveMolAwareCallback] ğŸ”— hardlink {dst} -> {src}")
    except Exception:
        import shutil
        shutil.copy2(src, dst)
        print(f"[SaveMolAwareCallback] ğŸ“„ copied {dst} from {src}")

def _cleanup_old_weights(ckpt_dir: str):
    """æ¸…ç†å†—ä½™æ—§æ–‡ä»¶"""
    ckpt = Path(ckpt_dir)
    extras_dir = ckpt / "extras"
    llm_dir = ckpt / "llm"

    keep_names = {
        "trainer_state.json", "optimizer.pt", "scheduler.pt",
        "training_args.bin", "config.json", "tokenizer.json",
        "tokenizer.model", "tokenizer_config.json", "vocab.json",
        "merges.txt", "special_tokens_map.json", "molaware_metadata.json",
        "model.safetensors", "pytorch_model.bin", "pytorch_model.bin.index.json",
    }

    patterns = ["*.bin", "*.pt", "*.pth", "*.safetensors"]
    removed = 0
    for pat in patterns:
        for fp in ckpt.glob(pat):
            name = fp.name
            if name in keep_names:
                continue
            if llm_dir in fp.parents or extras_dir in fp.parents:
                continue
            if fp.is_file():
                try:
                    fp.unlink()
                    removed += 1
                except Exception as e:
                    print(f"[SaveMolAwareCallback] WARN: remove {fp} failed: {e}")
    if removed:
        print(f"[SaveMolAwareCallback] ğŸ—‘ Cleaned {removed} stale file(s)")

def _cleanup_old_checkpoints(output_dir: str, keep_last_n: int = 3):
    """æ¸…ç†æ—§çš„checkpointç›®å½•ï¼Œåªä¿ç•™æœ€åNä¸ª"""
    if not os.path.isdir(output_dir):
        return
    
    # æ‰¾åˆ°æ‰€æœ‰checkpointç›®å½•
    checkpoints = []
    for item in os.listdir(output_dir):
        if item.startswith("checkpoint-") and os.path.isdir(os.path.join(output_dir, item)):
            try:
                # æå–stepæ•°å­—
                step = int(item.split("-")[1])
                checkpoints.append((step, os.path.join(output_dir, item)))
            except (ValueError, IndexError):
                continue
    
    if len(checkpoints) <= keep_last_n:
        return
    
    # æŒ‰stepæ’åºï¼Œä¿ç•™æœ€åNä¸ª
    checkpoints.sort(key=lambda x: x[0])
    to_remove = checkpoints[:-keep_last_n]
    
    removed = 0
    for step, ckpt_path in to_remove:
        try:
            import shutil
            shutil.rmtree(ckpt_path)
            removed += 1
            print(f"[SaveMolAwareCallback] ğŸ—‘ Removed old checkpoint: {os.path.basename(ckpt_path)} (step {step})")
        except Exception as e:
            print(f"[SaveMolAwareCallback] WARN: failed to remove {ckpt_path}: {e}")
    
    if removed > 0:
        print(f"[SaveMolAwareCallback] âœ… Cleaned {removed} old checkpoint(s), kept last {keep_last_n}")

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def break_gvp_shared_parameters(model: nn.Module):
    """
    å…‹éš† gvp_encoder çš„å‚æ•°ï¼Œæ‰“ç ´æ½œåœ¨çš„å…±äº«å­˜å‚¨ï¼Œé¿å… safetensors ä¿å­˜æŠ¥é”™ã€‚
    """
    gvp = getattr(model, "gvp_encoder", None)
    if gvp is None:
        return
    try:
        sd = gvp.state_dict()
        cloned = {k: v.clone() for k, v in sd.items()}
        gvp.load_state_dict(cloned, strict=False)
        print("[Init] Cloned gvp_encoder state_dict to break shared storage.")
    except Exception as e:
        print(f"[Init] WARN: failed to clone gvp_encoder params: {e}")


def infer_response_template_from_chat_template(tokenizer) -> str:
    """
    æ ¹æ® tokenizer çš„ chat_template è‡ªåŠ¨æ¨æ–­ response_templateï¼š
    - æ„é€ ä¸€ä¸ªå¸¦å ä½ç¬¦çš„å¯¹è¯ï¼Œé€šè¿‡ apply_chat_template å¾—åˆ°å®Œæ•´ prompt
    - å–å ä½ç¬¦ä¹‹åçš„é‚£ä¸€æ®µä½œä¸º response_template
    """
    dummy_user = "<DUMMY_USER_CONTENT_FOR_TEMPLATE>"
    system_msg = "You are a helpful chemist."
    try:
        formatted = tokenizer.apply_chat_template(
            [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": dummy_user},
            ],
            tokenize=False,
            add_generation_prompt=True,
        )
        pos = formatted.rfind(dummy_user)
        if pos != -1:
            # ä»å ä½ç¬¦ç»“æŸä½ç½®åˆ°å­—ç¬¦ä¸²æœ«å°¾å³ä¸ºç”¨äºæç¤º assistant å›å¤çš„æ¨¡æ¿å‰ç¼€
            tpl = formatted[pos + len(dummy_user) :]
            tpl = tpl.lstrip()  # å»æ‰å‰ç½®ç©ºç™½
            # é¿å…è¿”å›ç©ºå­—ç¬¦ä¸²
            if tpl:
                return tpl
    except Exception:
        pass

    # Fallbackï¼šå¦‚æœæ— æ³•ä» chat_template æ¨æ–­ï¼Œåˆ™é€€å›åˆ°ç®€å•è§„åˆ™
    vocab = tokenizer.get_vocab()
    if "<|start_header_id|>" in vocab and "<|end_header_id|>" in vocab:
        return "<|start_header_id|>assistant<|end_header_id|>"
    return "Assistant:"

# ======================== å›è°ƒ ========================
class BarrierCallback(TrainerCallback):
    def on_save(self, args, state, control, **kwargs):
        safe_barrier()
    def on_evaluate(self, args, state, control, **kwargs):
        safe_barrier()
    def on_train_begin(self, args, state, control, **kwargs):
        safe_barrier()
    def on_train_end(self, args, state, control, **kwargs):
        safe_barrier()
        
class SaveMolAwareCallback(TrainerCallback):
    """
    ç²¾ç®€ç‰ˆï¼šåªä¿å­˜ metadataï¼Œä¸ä¿å­˜æ¨¡å‹æƒé‡
    - config å’Œ tokenizer ç”± CopyConfigCallback ä¿å­˜
    - metadata ç”±æœ¬ callback ä¿å­˜
    """
    
    def _save_metadata(self, ckpt_dir: str, model):
        """ä¿å­˜ molaware_metadata.json"""
        if not dist.is_available() or not dist.is_initialized() or dist.get_rank() == 0:
            metadata = {
                "class": "MolAwareCausalLM",
                "version": 1,
                "mol_token": getattr(model, "mol_token", "<mol>"),
                "llm_dir": "llm/",
                "extras": {
                    "gvp_encoder": "extras/gvp_encoder.pt" if getattr(model, "gvp_encoder", None) else None,
                    "mol_adapter": "extras/mol_adapter.pt" if getattr(model, "mol_adapter", None) else None,
                    "diffusion_adapter": "extras/diffusion_adapter.pt" if getattr(model, "diffusion_adapter", None) else None,
                }
            }
            meta_path = os.path.join(ckpt_dir, "molaware_metadata.json")
            with open(meta_path, "w") as f:
                json.dump(metadata, f, indent=2)
            print(f"[Save] âœ” Metadata saved â†’ {meta_path}")

    def on_save(self, args, state, control, **kwargs):
        """åªä¿å­˜ metadataï¼Œä¸ä¿å­˜æ¨¡å‹æƒé‡"""
        model = kwargs.get("model")
        if model is None:
            # å°è¯•ä» trainer è·å–
            trainer = kwargs.get("trainer")
            if trainer is not None:
                model = getattr(trainer, "model", None)
        
        if model is None:
            return
        
        # checkpoint dir
        ckpt_dir = os.path.join(args.output_dir, f"checkpoint-{state.global_step}")
        os.makedirs(ckpt_dir, exist_ok=True)
        
        # ä¿å­˜ metadata
        self._save_metadata(ckpt_dir, model)
        
        # æ¸…ç†æ—§çš„checkpointï¼Œåªä¿ç•™æœ€æ–°çš„Nä¸ªï¼ˆåœ¨rank0æ‰§è¡Œï¼Œé¿å…å¤šè¿›ç¨‹é‡å¤åˆ é™¤ï¼‰
        if dist.is_available() and dist.is_initialized():
            if dist.get_rank() == 0:
                keep_last_n = getattr(args, "save_total_limit", 3)
                _cleanup_old_checkpoints(args.output_dir, keep_last_n=keep_last_n)
            dist.barrier()
        else:
            # å•å¡è®­ç»ƒæ—¶ç›´æ¥æ¸…ç†
            keep_last_n = getattr(args, "save_total_limit", 3)
            _cleanup_old_checkpoints(args.output_dir, keep_last_n=keep_last_n)
        

class SwanLabCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_world_process_zero:
            if logs is not None:
                swanlab.log(logs, step=state.global_step)

class CopyConfigCallback(TrainerCallback):
    def on_save(self, args, state, control, **kwargs):
        if state.is_world_process_zero:
            model = kwargs["model"]
            tok = kwargs.get("tokenizer", None)
            if getattr(model, "config", None) is not None:
                model.config.save_pretrained(args.output_dir)
            if tok is not None:
                tok.save_pretrained(args.output_dir)


# ---------------- DataCollator with meta ----------------
class DataCollatorForCompletionOnlyLMWithMeta(DataCollatorForCompletionOnlyLM):
    """ä¿ç•™metaä¿¡æ¯çš„DataCollator"""
    def __init__(self, response_template, tokenizer, instruction_template=None, mlm=False, ignore_index=-100, padding_free=False, **kwargs):
        super().__init__(
            response_template=response_template,
            instruction_template=instruction_template,
            tokenizer=tokenizer,
            mlm=mlm,
            ignore_index=ignore_index,
            padding_free=padding_free,
            **kwargs
        )
    
    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        # æå– meta ä¿¡æ¯ï¼ˆåœ¨è°ƒç”¨ torch_call ä¹‹å‰ï¼‰
        meta_info = []
        for f in features:
            meta_info.append({
                "meta": f.get("meta", None),
                "dataset": f.get("dataset", None),
                "task_type": f.get("task_type", None),
                "smiles": f.get("smiles", None),
                "class_label": f.get("class_label", None),
                "all_targets": f.get("all_targets", None),
            })
        
        # è°ƒç”¨ torch_call å¤„ç† tensor ç›¸å…³çš„å†…å®¹
        batch = self.torch_call(features)
        
        # å°† meta ä¿¡æ¯æ·»åŠ å› batchï¼ˆè¿™äº›å­—æ®µä¸ä¼šä¼ é€’ç»™ modelï¼Œåªç”¨äº loss è®¡ç®—ï¼‰
        batch["meta"] = [m["meta"] for m in meta_info]
        batch["dataset"] = [m["dataset"] for m in meta_info]
        batch["task_type"] = [m["task_type"] for m in meta_info]
        batch["smiles"] = [m["smiles"] for m in meta_info]
        batch["class_label"] = [m["class_label"] for m in meta_info]
        batch["all_targets"] = [m["all_targets"] for m in meta_info]
        
        return batch
    
    def torch_call(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        # --- è¿è¡Œæ—¶è‡ªæ£€ï¼šé˜²æ­¢ tokenizer åœ¨å¤šè¿›ç¨‹ä¸­ä¸¢å¤± pad_token ---
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        # -------------------------------------------------------

        cleaned_examples: List[Dict[str, Any]] = []

        for ex in features:
            ex = dict(ex)

            # 1. å‰¥ç¦»æ‰€æœ‰ Meta ä¿¡æ¯
            for k in ["meta", "dataset", "task_type", "smiles", "class_label", "all_targets"]:
                ex.pop(k, None)

            # 2. æ ¸å¿ƒæ•°æ®æ¸…æ´—
            if "input_ids" in ex:
                cleaned_ex = {}
                # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œç§»é™¤äº† "labels"ï¼Œé˜²æ­¢ tokenizer.pad å› ä¸ºæ— æ³• pad labels è€ŒæŠ¥é”™
                valid_keys = ["input_ids", "attention_mask", "special_tokens_mask"] 
                
                for k in valid_keys:
                    if k in ex:
                        v = ex[k]
                        # å¼ºåˆ¶è§£å¼€åµŒå¥—åˆ—è¡¨ [[1,2,3]] -> [1,2,3]
                        if isinstance(v, list) and len(v) > 0 and isinstance(v[0], list):
                            v = v[0]
                        # å¼ºåˆ¶è½¬ int
                        if k == "input_ids" and isinstance(v, list):
                            v = [int(x) for x in v]
                        
                        cleaned_ex[k] = v
                
                if "input_ids" in cleaned_ex:
                    cleaned_examples.append(cleaned_ex)
                continue

            # å¤„ç†çº¯æ–‡æœ¬çš„æƒ…å†µ
            if "text" in ex:
                text = ex["text"]
                # ç¡®ä¿ text æ˜¯å­—ç¬¦ä¸²ï¼Œå¤„ç†å„ç§å¯èƒ½çš„æ ¼å¼
                if isinstance(text, list):
                    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªå…ƒç´ æˆ–è¿æ¥
                    if len(text) > 0:
                        if isinstance(text[0], str):
                            text = text[0]  # å–ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²å…ƒç´ 
                        else:
                            text = " ".join(str(t) for t in text)  # è¿æ¥æ‰€æœ‰å…ƒç´ 
                    else:
                        text = ""
                elif not isinstance(text, str):
                    text = str(text) if text is not None else ""
                
                if not text or not text.strip(): 
                    continue
                
                # Tokenizeï¼ˆä¸åœ¨è¿™é‡Œåš paddingï¼Œpadding ä¼šåœ¨çˆ¶ç±»çš„ torch_call ä¸­å®Œæˆï¼‰
                tokenized = self.tokenizer(
                    text,
                    add_special_tokens=True,
                    truncation=True,
                    max_length=self.tokenizer.model_max_length,
                    padding=False,  # æ˜ç¡®ä¸åœ¨è¿™é‡Œ padding
                    return_tensors=None,  # è¿”å›åˆ—è¡¨è€Œä¸æ˜¯ tensor
                )
                
                # ç¡®ä¿ input_ids æ˜¯æ‰å¹³åˆ—è¡¨
                input_ids = tokenized["input_ids"]
                if isinstance(input_ids, list):
                    # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œç¡®ä¿æ˜¯æ‰å¹³åˆ—è¡¨
                    if len(input_ids) > 0 and isinstance(input_ids[0], list):
                        input_ids = input_ids[0]
                    # ç¡®ä¿æ‰€æœ‰å…ƒç´ éƒ½æ˜¯æ•´æ•°
                    input_ids = [int(x) for x in input_ids if isinstance(x, (int, float, str))]
                else:
                    # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                    input_ids = [int(x) for x in [input_ids] if isinstance(x, (int, float, str))]
                
                if not input_ids:
                    continue
                
                cleaned_ex = {"input_ids": input_ids}
                
                # å¤„ç† attention_mask
                if "attention_mask" in tokenized:
                    attn = tokenized["attention_mask"]
                    if isinstance(attn, list):
                        if len(attn) > 0 and isinstance(attn[0], list):
                            attn = attn[0]
                        attn = [int(x) for x in attn if isinstance(x, (int, float, str))]
                    else:
                        attn = [int(attn)] if isinstance(attn, (int, float, str)) else []
                    
                    # ç¡®ä¿ attention_mask é•¿åº¦ä¸ input_ids ä¸€è‡´
                    if len(attn) != len(input_ids):
                        attn = [1] * len(input_ids)  # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨å…¨1
                    cleaned_ex["attention_mask"] = attn
                else:
                    # å¦‚æœæ²¡æœ‰ attention_maskï¼Œåˆ›å»ºä¸€ä¸ªå…¨1çš„
                    cleaned_ex["attention_mask"] = [1] * len(input_ids)

                cleaned_examples.append(cleaned_ex)
                continue

        # 3. è°ƒç”¨çˆ¶ç±»è¿›è¡Œ Batch Padding
        # çˆ¶ç±» DataCollatorForCompletionOnlyLM ä¼šè‡ªåŠ¨æ ¹æ® response_template ç”Ÿæˆ labels
        # åªè¦è¿™é‡Œä¸ä¼ å‚å·®ä¸é½çš„ labelsï¼Œtokenizer.pad å°±èƒ½æ­£å¸¸å·¥ä½œ
        batch = super().torch_call(cleaned_examples)

        return batch
    
# ---------------- Multi-task SFTTrainer ----------------
class MultiTaskSFTTrainer(SFTTrainer):
    """æ”¯æŒGNNä»»åŠ¡çš„SFTTrainer"""
    QM9_TASKS = ["mu", "alpha", "homo", "lumo", "gap"]

    def __init__(
        self,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        
        # åˆå§‹åŒ–ç”¨äºè®°å½• loss çš„å®ä¾‹å˜é‡ï¼ˆç´¯ç§¯å€¼ï¼Œç”¨äºè®¡ç®—å¹³å‡å€¼ï¼‰
        self._loss_lm_sum = 0.0
        self._loss_count = 0

    def build_qm9_targets(self, all_targets_list: List[Optional[Dict[str, float]]], device: torch.device) -> torch.Tensor:
        rows = []
        for d in all_targets_list:
            if d is None:
                rows.append([0.0] * len(self.QM9_TASKS))
            else:
                rows.append([float(d.get(k, 0.0)) for k in self.QM9_TASKS])
        return torch.tensor(rows, dtype=torch.float32, device=device)
    
    def log(self, logs: Dict[str, float], start_time: Optional[float] = None) -> None:
        """
        é‡å†™ log æ–¹æ³•ï¼Œæ·»åŠ è‡ªå®šä¹‰çš„ GNN loss å€¼åˆ°æ—¥å¿—ä¸­
        è¿™æ · SwanLab å°±èƒ½è®°å½•è¿™äº›å€¼äº†
        """
        # åœ¨è°ƒç”¨çˆ¶ç±» log ä¹‹å‰ï¼Œæ·»åŠ è‡ªå®šä¹‰çš„ loss å€¼ï¼ˆä½¿ç”¨ç´¯ç§¯çš„å¹³å‡å€¼ï¼‰
        if hasattr(self, '_loss_count') and self._loss_count > 0:
            logs['train/loss_lm'] = self._loss_lm_sum / self._loss_count
            
            # é‡ç½®ç´¯ç§¯å€¼ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ª logging å‘¨æœŸ
            self._loss_lm_sum = 0.0
            self._loss_count = 0
        
        # è°ƒç”¨çˆ¶ç±»çš„ log æ–¹æ³•ï¼ˆè¿™ä¼šè§¦å‘ SwanLab è®°å½•ï¼‰
        # ä¼ é€’ start_time å‚æ•°ä»¥åŒ¹é…çˆ¶ç±»ç­¾å
        super().log(logs, start_time=start_time)

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """
        Lossè®¡ç®—é€»è¾‘ï¼š
        L_total = L_lm
        
        æ³¨æ„ï¼šGVPå’Œdiffusionä¸éœ€è¦å•ç‹¬çš„lossï¼Œåªä¿ç•™LM loss
        """
        # ç§»é™¤ä¸éœ€è¦çš„å­—æ®µï¼ˆä¿ç•™ç”¨äºæ•°æ®ä¼ é€’ï¼Œä½†ä¸ç”¨äºlossè®¡ç®—ï¼‰
        inputs.pop("dataset", None)
        inputs.pop("task_type", None)
        inputs.pop("smiles", None)
        inputs.pop("class_label", None)
        inputs.pop("all_targets", None)
        inputs.pop("meta", None)

        # åªè®¡ç®—è¯­è¨€æ¨¡å‹çš„SFT loss
        loss_lm, outputs = super().compute_loss(model, inputs, return_outputs=True)
        
        # æ£€æŸ¥ loss_lm æ˜¯å¦ä¸º None
        if loss_lm is None:
            raise ValueError("LM loss is None. This may indicate an issue with the model forward pass or loss computation.")
        
        # æ€»losså°±æ˜¯LM lossï¼ˆGVPå’Œdiffusionä¸éœ€è¦å•ç‹¬çš„lossï¼‰
        loss = loss_lm
        
        # ç´¯ç§¯ loss å€¼ï¼Œç”¨äºåœ¨ logging step æ—¶è®¡ç®—å¹³å‡å€¼
        loss_lm_val = loss_lm.detach().item() if isinstance(loss_lm, torch.Tensor) else float(loss_lm)
        self._loss_lm_sum += loss_lm_val
        self._loss_count += 1

        if return_outputs:
            if hasattr(outputs, "loss"):
                outputs.loss = loss
            outputs.loss_lm = loss_lm
            return loss, outputs
        return loss

# ---------------- Main ----------------
def main_worker(world_size, cfg):
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    device = f"cuda:{local_rank}"
    torch.cuda.set_device(local_rank)
    print(f"[rank {local_rank}] using {device}")

    set_seed(cfg["seed"] + local_rank)

    # åˆå§‹åŒ–tokenizerå’ŒLLM
    llm_name = cfg["paths"]["llm_name_or_path"]
    mol_token = cfg["tokens"]["mol_token"]
    
    print(f"[{local_rank}] Initializing tokenizer...")
    tokenizer = init_tokenizer(llm_name, mol_token)
    # ================= ä¿®å¤å¼€å§‹ =================
    # 1. å¼ºåˆ¶è®¾ç½® pad_tokenã€‚Llama 3 é»˜è®¤æ²¡æœ‰ pad_tokenï¼Œè¿™æ˜¯æŠ¥é”™çš„æ ¹æºã€‚
    if tokenizer.pad_token is None:
        print(f"[{local_rank}] âš ï¸ Tokenizer.pad_token is None, setting to eos_token_id: {tokenizer.eos_token_id}")
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # 2. ç¡®ä¿ padding_side ä¸º right (SFTTrainer éœ€è¦)
    tokenizer.padding_side = "right"
    
    # 3. è®¾ç½®æœ€å¤§é•¿åº¦
    tokenizer.model_max_length = int(cfg["train"]["max_seq_length"])
    # ================= ä¿®å¤ç»“æŸ =================

    print(f"[{local_rank}] Initializing LLM...")
    llm = init_llm(llm_name, tokenizer, cfg["train"]["bf16"], device)

    # åˆå§‹åŒ–æ¨¡å‹
    print(f"[{local_rank}] Initializing model...")
    model = init_model(cfg, tokenizer, llm, device)
    # æ‰“ç ´ gvp_encoder å…±äº«å‚æ•°ï¼Œé¿å… safetensors ä¿å­˜æ—¶å›  shared tensors æŠ¥é”™
    break_gvp_shared_parameters(model)
    
    # æ³¨æ„ï¼šLDMolç»„ä»¶å¯ä»¥åœ¨æ¨ç†æ—¶ä½¿ç”¨ï¼Œä½†è®­ç»ƒæ—¶ä¸éœ€è¦å•ç‹¬çš„loss
    # LDMolç›´æ¥ä½¿ç”¨LLMçš„embeddingï¼Œä¸éœ€è¦adapter
    
    # âœ… å¿…é¡»ç¦ç”¨ç¼“å­˜ï¼Œå¦åˆ™æ¢¯åº¦æ£€æŸ¥ç‚¹æ— æ•ˆï¼ˆæ¿€æ´»æ˜¾å­˜æš´æ¶¨ 3â€“4xï¼‰
    llm.config.use_cache = False
    model.config.use_cache = False
    
    # âœ… å¼ºåˆ¶å¯ç”¨ gradient checkpointingï¼ˆåŒæ—¶å¯¹åŒ…è£…æ¨¡å‹ä¸åº•å±‚ LLM å¼€å¯ï¼‰
    if cfg["train"].get("gradient_checkpointing", False):
        # å…ˆç¡®ä¿åº•å±‚ llm å…³é—­ç¼“å­˜å¹¶å¼€å¯ GC
        if hasattr(llm, "config"):
            llm.config.use_cache = False
        if hasattr(llm, "gradient_checkpointing_enable"):
            llm.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})

        # å†å¯¹åŒ…è£…åçš„æ•´ä½“æ¨¡å‹å¼€å¯ GC
        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})
        # Sanity check
        gc_enabled = getattr(model, "is_gradient_checkpointing", False)
        if hasattr(model, "module") and hasattr(model.module, "is_gradient_checkpointing"):
            gc_enabled = model.module.is_gradient_checkpointing
        print(f"[{local_rank}] GC enabled: {gc_enabled}")
        if not gc_enabled:
            print(f"[{local_rank}] âš ï¸  WARNING: Gradient checkpointing may not be enabled properly!")
    else:
        print(f"[{local_rank}] âš ï¸  Gradient checkpointing is disabled in config")

    # åˆå§‹åŒ–ç¦»çº¿tokenåˆ†ç±»å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
    use_offline_spans = cfg.get("train", {}).get("use_offline_spans", False)
    offline_token_head = None
    if use_offline_spans:
        mlp_token_classifier_path = cfg["paths"].get("mlp_token_classifier_path")
        offline_token_head = init_offline_token_classifier(llm, mlp_token_classifier_path, device)
        if offline_token_head is None:
            print(f"[{local_rank}] âš ï¸ use_offline_spans=True but token classifier not loaded, will use <mol> tags only")

    # å‚æ•°ç»Ÿè®¡
    if local_rank == 0:
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        print(f"[{local_rank}] Trainable params: {trainable_params} / Total: {total_params}")

    # åŠ è½½æ•°æ®
    print(f"[{local_rank}] Loading training data...")
    train_dataset, eval_dataset = load_training_data(
        cfg, tokenizer, llm, offline_token_head, local_rank
    )
    
    # æ‰“å°train_datasetçš„ç¬¬ä¸€æ¡æ•°æ®ï¼ˆåœ¨train_sft.pyä¸­ï¼‰
    if local_rank == 0 and len(train_dataset) > 0:
        print("\n" + "="*80)
        print("ğŸ“‹ First sample from train_dataset (in train_sft.py, after load_training_data):")
        print("="*80)
        first_sample = train_dataset[0]
        print(f"Type: {type(first_sample)}")
        print(f"Content: {first_sample}")
        if isinstance(first_sample, dict):
            print(f"Keys: {list(first_sample.keys())}")
            for key, value in first_sample.items():
                if key == "text":
                    print(f"  {key}: type={type(value)}, length={len(str(value))}, preview={str(value)[:200]}...")
                else:
                    print(f"  {key}: type={type(value)}, value={str(value)[:200]}...")
        print("="*80 + "\n")

    # è®¡ç®—QM9ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
    qm9_means, qm9_stds = None, None
    use_gnn_tasks = cfg.get("train", {}).get("use_gnn_tasks", False)
    if use_gnn_tasks:
        # å°è¯•ä»æ–‡ä»¶åŠ è½½
        qm9_stats_file = cfg.get("data", {}).get("qm9_stats_file")
        if qm9_stats_file and os.path.exists(qm9_stats_file):
            with open(qm9_stats_file, 'r') as f:
                stats = json.load(f)
                qm9_means = stats.get("means")
                qm9_stds = stats.get("stds")
        else:
            # ä»æ•°æ®é›†è®¡ç®—
            qm9_means, qm9_stds = compute_qm9_stats_from_dataset(train_dataset)
        
        if local_rank == 0:
            if qm9_means is None:
                print(f"[{local_rank}] âš ï¸ QM9 stats not found")
            else:
                print(f"[{local_rank}] âœ… QM9 stats: means={qm9_means}, stds={qm9_stds}")

    # DataCollator
    # ä¼˜å…ˆæ ¹æ® tokenizer çš„ chat_template è‡ªåŠ¨æ¨æ–­ response_templateï¼›
    # å¦‚æœå¤±è´¥ï¼Œå†æ ¹æ® vocab ä¸­çš„ç‰¹æ®Š token é€€å›åˆ°ç®€å•è§„åˆ™ã€‚
    response_template = infer_response_template_from_chat_template(tokenizer)
    
    # âœ… ç»Ÿä¸€ä½¿ç”¨è‡ªå®šä¹‰çš„ Collatorã€‚
    # å³ä½¿ use_gnn_tasks=Falseï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦å®ƒæ¥æ¸…æ´— 'text' åˆ—ï¼Œé˜²æ­¢æŠ¥é”™ã€‚
    # å®ƒçš„ torch_call æ–¹æ³•é‡Œçš„æ¸…æ´—é€»è¾‘æ˜¯é€šç”¨çš„ã€‚
    data_collator = DataCollatorForCompletionOnlyLMWithMeta(
        response_template=response_template, 
        tokenizer=tokenizer, 
        mlm=False
    )
    
    # (åŸæ¥çš„ if/else é€»è¾‘åˆ é™¤ï¼Œåªä¿ç•™ä¸Šé¢è¿™ä¸€æ®µ)

    # TrainingArguments
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DeepSpeed
    deepspeed_config_path = cfg["paths"].get("deepspeed_config")
    if deepspeed_config_path:
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºä»£ç ç›®å½•ï¼‰
        if not os.path.isabs(deepspeed_config_path):
            # è·å–ä»£ç ç›®å½•ï¼ˆtrain_sft.pyæ‰€åœ¨ç›®å½•ï¼‰
            code_dir = os.path.dirname(os.path.abspath(__file__))
            deepspeed_config_path = os.path.join(code_dir, deepspeed_config_path)
            deepspeed_config_path = os.path.abspath(deepspeed_config_path)
        use_deepspeed = os.path.exists(deepspeed_config_path)
        if use_deepspeed and local_rank == 0:
            print(f"ğŸš€ DeepSpeed config found: {deepspeed_config_path}")
    else:
        use_deepspeed = False
    
    # ================= ä¿®å¤ DeepSpeed ZeRO-3 ä¸ frozen å‚æ•°çš„å…¼å®¹æ€§é—®é¢˜ =================
    # DeepSpeed çš„ count_used_parameters_in_backward ä¼šéå†æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬ frozen å‚æ•°
    # å¦‚æœ frozen å‚æ•°çš„ grad_fn æ˜¯ Noneï¼Œè®¿é—® .next_functions ä¼šæŠ¥é”™
    # è§£å†³æ–¹æ¡ˆï¼šmonkey-patch PyTorch çš„ _get_grad_fn_or_grad_acc å‡½æ•°ï¼Œè·³è¿‡ frozen å‚æ•°
    if use_deepspeed:
        try:
            # ä½¿ç”¨å·²ç»å¯¼å…¥çš„ torch æ¨¡å—
            from torch.autograd.graph import _get_grad_fn_or_grad_acc as original_get_grad_fn
            
            # ä¿å­˜åŸå§‹å‡½æ•°
            _original_get_grad_fn_ds = original_get_grad_fn
            
            def safe_get_grad_fn_or_grad_acc(param):
                """å®‰å…¨ç‰ˆæœ¬çš„ _get_grad_fn_or_grad_accï¼Œè·³è¿‡ frozen å‚æ•°"""
                if not getattr(param, "requires_grad", False):
                    # å¦‚æœæ˜¯ frozen å‚æ•°ï¼Œè¿”å› Noneï¼Œé¿å… DeepSpeed å°è¯•è®¿é—® grad_fn.next_functions
                    return None
                try:
                    return _original_get_grad_fn_ds(param)
                except (AttributeError, TypeError) as e:
                    # å¦‚æœè®¿é—® grad_fn.next_functions å¤±è´¥ï¼Œè¿”å› None
                    if "NoneType" in str(e) or "next_functions" in str(e):
                        return None
                    raise
            
            # Monkey-patch PyTorch çš„ _get_grad_fn_or_grad_acc
            # ä½¿ç”¨å·²ç»å¯¼å…¥çš„ torch æ¨¡å—ï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜
            torch.autograd.graph._get_grad_fn_or_grad_acc = safe_get_grad_fn_or_grad_acc
            
            if local_rank == 0:
                print("[Fix] Patched PyTorch's _get_grad_fn_or_grad_acc to skip frozen parameters for DeepSpeed ZeRO-3 compatibility")
        except Exception as e:
            if local_rank == 0:
                print(f"[Fix] Warning: Failed to patch PyTorch for DeepSpeed frozen params compatibility: {e}")
    # ================= ä¿®å¤ç»“æŸ =================
    
    if use_deepspeed:
        if local_rank == 0:
            print(f"ğŸš€ Using DeepSpeed config: {deepspeed_config_path}")
        optim_name = "adamw_torch"
    else:
        optim_name = "paged_adamw_8bit"
    
    args = TrainingArguments(
        output_dir=cfg["paths"]["output_dir"],
        per_device_train_batch_size=cfg["train"]["per_device_train_batch_size"],
        per_device_eval_batch_size=cfg["train"]["per_device_eval_batch_size"],
        gradient_accumulation_steps=cfg["train"]["gradient_accumulation_steps"],
        learning_rate=float(cfg["train"]["learning_rate"]),
        num_train_epochs=cfg["train"]["num_train_epochs"],
        logging_steps=cfg["train"]["logging_steps"],
        save_strategy="steps",
        save_steps=cfg["train"]["save_steps"],
        eval_strategy="steps",
        eval_steps=cfg["train"]["eval_steps"],
        warmup_ratio=cfg["train"]["warmup_ratio"],
        lr_scheduler_type=cfg["train"]["lr_scheduler_type"],
        bf16=cfg["train"]["bf16"],
        gradient_checkpointing=cfg["train"]["gradient_checkpointing"],
        gradient_checkpointing_kwargs={"use_reentrant": False},
        remove_unused_columns=False,
        ddp_find_unused_parameters=True,
        report_to="none",
        optim=optim_name,
        dataloader_pin_memory=cfg["train"].get("dataloader_pin_memory", True),
        dataloader_num_workers=cfg["train"].get("dataloader_num_workers", 0),
        max_grad_norm=cfg["train"].get("max_grad_norm", 1.0),
        weight_decay=cfg["train"].get("weight_decay", 0.01),
        deepspeed=deepspeed_config_path if use_deepspeed else None,
        save_safetensors=False,
        save_total_limit=cfg["train"].get("save_total_limit", 2),  # åªä¿ç•™æœ€æ–°çš„3ä¸ªcheckpoint
        disable_tqdm=False,  # å¯ç”¨è¿›åº¦æ¡æ˜¾ç¤ºï¼ˆåœ¨ rank 0 ä¸Šä¼šæ˜¾ç¤ºï¼‰
    )

    # Trainer
    callbacks = [
        SaveMolAwareCallback(),
        SwanLabCallback(),
        CopyConfigCallback(),
        BarrierCallback()
    ]

    # å‡†å¤‡ SFTTrainer çš„å‚æ•°
    # æ³¨æ„ï¼šå½“å‰ç‰ˆæœ¬çš„ trl å¯èƒ½ä¸æ”¯æŒ SFTConfig å‚æ•°ï¼Œæˆ–è€…å‚æ•°åä¸æ˜¯ sft_config
    # æ ¹æ®é”™è¯¯ä¿¡æ¯ï¼ŒSFTTrainer ä¸æ¥å— sft_config å‚æ•°
    # æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨æ—§çš„æ–¹å¼ï¼Œè™½ç„¶ä¼šæ˜¾ç¤ºè­¦å‘Šä½†ä¸å½±å“åŠŸèƒ½
    sft_kwargs = {
        "dataset_text_field": "text",
        "max_seq_length": int(cfg["train"]["max_seq_length"]),
        "packing": cfg["train"]["packing"],
    }
    # æ³¨æ„ï¼šè¿™äº›å‚æ•°åœ¨æ–°ç‰ˆæœ¬ä¸­å·²è¢«å¼ƒç”¨ï¼Œä¼šæ˜¾ç¤ºè­¦å‘Šï¼Œä½†ä¸å½±å“åŠŸèƒ½
    # å¦‚æœæœªæ¥ trl åº“æ›´æ–°æ”¯æŒ SFTConfigï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ç›¸åº”çš„é€»è¾‘

    # åˆ›å»º Trainerï¼ˆç»Ÿä¸€ä½¿ç”¨ MultiTaskSFTTrainerï¼Œåªè®¡ç®—LM lossï¼‰
    # æ³¨æ„ï¼šGVPå’Œdiffusionä¸éœ€è¦å•ç‹¬çš„lossï¼Œåªä¿ç•™LM loss
    trainer = MultiTaskSFTTrainer(
        model=model,
        tokenizer=tokenizer,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        callbacks=callbacks,
        **sft_kwargs,  # ä¼ é€’ SFT ç›¸å…³å‚æ•°
    )

    # è®­ç»ƒï¼ˆä¸ä½¿ç”¨checkpointæ¢å¤è®­ç»ƒçŠ¶æ€ï¼‰
    # æ³¨æ„ï¼šæ¨¡å‹æƒé‡å·²ç»åœ¨model_init.pyä¸­æ ¹æ®configåŠ è½½äº†
    trainer.train(resume_from_checkpoint=None)


def main(cfg_path="configs/config.yaml"):
    with open(cfg_path, "r") as f:
        cfg = yaml.safe_load(f)
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    main_worker(world_size, cfg)


if __name__ == "__main__":
    import sys
    cfg_path = sys.argv[1] if len(sys.argv) > 1 else "configs/config.yaml"
    main(cfg_path)

