# mol_aware_lm_integrated.py
# -*- coding: utf-8 -*-
import os
import json
import torch
import torch.nn as nn
from typing import Optional, Tuple, List, Dict
import logging

from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer
from transformers.modeling_outputs import CausalLMOutputWithPast

from .gnn import GVPEncoder
from .mlp import MLPAdapter
from .tools import extract_and_convert_online

# ä½¿ç”¨ LDMol è¿›è¡Œåˆ†å­ç”Ÿæˆ
# BUG: diffusion fallback å­˜åœ¨bug, éœ€è¦è°ƒæ•´æ¶æ„
ENABLE_DIFFUSION_FALLBACK = False
from .ldmol_component import LDMolInferer

# ä½¿ç”¨ Layer2 è¿›è¡Œååº”äº§ç‡é¢„æµ‹
from .layer2_component import Layer2Inferer

# RDKit
from rdkit import Chem

# æ—¥å¿—
import sys
import io
import os

# ç¡®ä¿stdoutå’Œstderrä½¿ç”¨UTF-8ç¼–ç 
os.environ.setdefault('PYTHONIOENCODING', 'utf-8')

# å¦‚æœstdout/stderrä¸æ˜¯UTF-8ï¼Œåˆ™é‡æ–°åŒ…è£…
if hasattr(sys.stdout, 'buffer') and (not hasattr(sys.stdout, 'encoding') or sys.stdout.encoding != 'utf-8'):
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)
    except (AttributeError, ValueError):
        pass
if hasattr(sys.stderr, 'buffer') and (not hasattr(sys.stderr, 'encoding') or sys.stderr.encoding != 'utf-8'):
    try:
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=True)
    except (AttributeError, ValueError):
        pass

logging.getLogger("rdkit").setLevel(logging.ERROR)
logger = logging.getLogger(__name__)

# é…ç½®loggingä½¿ç”¨UTF-8ç¼–ç 
class UTF8StreamHandler(logging.StreamHandler):
    """ç¡®ä¿æ—¥å¿—è¾“å‡ºä½¿ç”¨UTF-8ç¼–ç çš„StreamHandler"""
    def __init__(self, stream=None):
        if stream is None:
            stream = sys.stderr
        super().__init__(stream)
    
    def emit(self, record):
        try:
            msg = self.format(record)
            stream = self.stream
            # ç¡®ä¿ä½¿ç”¨UTF-8ç¼–ç å†™å…¥
            if hasattr(stream, 'buffer'):
                stream.buffer.write(msg.encode('utf-8', errors='replace'))
                stream.buffer.write(b'\n')
                self.flush()
            else:
                stream.write(msg)
                stream.write('\n')
                self.flush()
        except Exception:
            self.handleError(record)

# åªåœ¨æ²¡æœ‰é…ç½®è¿‡loggingæ—¶æ‰é…ç½®
if not logging.root.handlers:
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[UTF8StreamHandler()]
    )

import torch.distributed as dist
import os, glob
import re

# ä¸ data_loader.py ä¸­ _looks_like_molecule ä¿æŒä¸€è‡´çš„åˆ¤æ–­é€»è¾‘
_MOL_STOPWORDS = {"smiles", "Smiles", "SMILES", "logP", "NSAIDs"}

# åˆ†éš”ç¬¦å­—ç¬¦é›†åˆï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦åº”è¯¥æ£€æµ‹å®ä½“ï¼‰
_BOUNDARY_CHARS = set(" \n\t,;:!?>")

def _is_boundary_token(tokenizer, token_id: int) -> bool:
    """
    åˆ¤æ–­ä¸€ä¸ª token æ˜¯å¦æ˜¯åˆ†éš”ç¬¦ï¼ˆç©ºæ ¼ã€æ¢è¡Œã€æ ‡ç‚¹ç­‰ï¼‰
    åªæœ‰åœ¨é‡åˆ°åˆ†éš”ç¬¦æ—¶æ‰æ£€æµ‹å®ä½“ï¼Œé¿å…åœ¨å•è¯ä¸­é—´æ£€æµ‹
    """
    try:
        token_text = tokenizer.decode([token_id], skip_special_tokens=False)
        # æ£€æŸ¥ token æ–‡æœ¬æ˜¯å¦åŒ…å«åˆ†éš”ç¬¦ï¼Œæˆ–è€…æ•´ä¸ª token å°±æ˜¯åˆ†éš”ç¬¦
        if not token_text:
            return False
        # å¦‚æœ token æ–‡æœ¬ä¸­çš„ä»»ä½•å­—ç¬¦æ˜¯åˆ†éš”ç¬¦ï¼Œæˆ–è€…æ•´ä¸ª token éƒ½æ˜¯åˆ†éš”ç¬¦/ç©ºç™½
        if any(c in _BOUNDARY_CHARS for c in token_text):
            return True
        # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯ç©ºç™½å­—ç¬¦
        if token_text.strip() == "":
            return True
        return False
    except Exception:
        return False

def _looks_like_molecule(span_text: str) -> bool:
    """
    è½¯è§„åˆ™åˆ¤æ–­ä¸€ä¸ª span çœ‹èµ·æ¥åƒ"åˆ†å­ç›¸å…³å®ä½“"ï¼ˆä¸ data_loader.py ä¿æŒä¸€è‡´ï¼‰ï¼š
    - å¾ˆçŸ­çš„ç¢ç‰‡ï¼ˆé•¿åº¦ < 2ï¼‰ç›´æ¥ä¸¢æ‰
    - å«æœ‰æ•°å­— or å…¸å‹ SMILES / åŒ–å­¦å¼ç¬¦å·ï¼ˆ= # () [] @ + / -ï¼‰å°±è®¤ä¸ºæ˜¯
    - å¦åˆ™ï¼Œå¦‚æœæœ‰ >=4 ä¸ªå­—æ¯ï¼ˆtoluene, ethanol, ibuprofen ç­‰åŒ–å­¦åï¼‰ä¹Ÿè®¤ä¸ºæ˜¯
    è§„åˆ™æ•…æ„å†™å¾—æ¯”è¾ƒå®½æ¾ï¼Œé¿å…æ¼æ‰çœŸæ­£çš„åŒ–å­¦åã€‚
    """
    if not span_text:
        return False
    
    s = span_text.strip()
    if s in _MOL_STOPWORDS:
        return False
    if len(s) < 2:
        return False

    # å…¸å‹ SMILES / åŒ–å­¦å¼ç‰¹å¾ï¼šæ•°å­—ã€=ã€#ã€æ‹¬å·ã€@ã€+ã€/ã€-
    if any(c.isdigit() for c in s):
        return True
    if any(c in "=#()[]@+/-" for c in s):
        return True

    # å¯¹çº¯å­—æ¯çš„æƒ…å†µï¼šå¦‚æœæœ‰ >=4 ä¸ªå­—æ¯ï¼Œå½“æˆä¸€ä¸ª"åƒåŒ–å­¦å"çš„è¯
    letters = [c for c in s if c.isalpha()]
    if len(letters) >= 4:
        return True

    return False

def has_hf_model_files(d: str) -> bool:
    if not os.path.isdir(d):
        return False
    # å•æ–‡ä»¶ / ç´¢å¼•æ–‡ä»¶
    names = [
        "model.safetensors",
        "pytorch_model.bin",
        "model.safetensors.index.json",
        "pytorch_model.bin.index.json",
        "flax_model.msgpack",
        "tf_model.h5",
    ]
    if any(os.path.isfile(os.path.join(d, n)) for n in names):
        return True
    # åˆ†ç‰‡æ–‡ä»¶ï¼ˆæ— è®ºæ˜¯å¦æœ‰ indexï¼Œéƒ½å½“ä½œâ€œè¯¥ç›®å½•åŒ…å«æƒé‡â€ï¼‰
    if glob.glob(os.path.join(d, "model-*-of-*.safetensors")):
        return True
    if glob.glob(os.path.join(d, "pytorch_model-*-of-*.bin")):
        return True
    return False

def any_rank_true(flag: bool) -> bool:
    """åªè¦æœ‰ä¸€ä¸ª rank ä¸º Trueï¼Œå°±è®©æ‰€æœ‰ rank éƒ½ä¸º Trueã€‚"""
    if not dist.is_available() or not dist.is_initialized():
        return flag
    t = torch.tensor([1 if flag else 0], device=torch.cuda.current_device())
    dist.all_reduce(t, op=dist.ReduceOp.MAX)
    return bool(t.item())

def zero_touch_module(module: torch.nn.Module) -> torch.Tensor:
    """ç”¨ 0.0 * param.sum() æŠŠ module æ¥å…¥è®¡ç®—å›¾ï¼Œä¸æ”¹å˜ loss æ•°å€¼ã€‚"""
    if module is None:
        return torch.tensor(0.0, device=torch.cuda.current_device())
    z = torch.tensor(0.0, device=next(module.parameters()).device) if any(p.requires_grad for p in module.parameters()) else torch.tensor(0.0, device=torch.cuda.current_device())
    for p in module.parameters():
        if p.requires_grad:
            z = z + (0.0 * p.float().sum())
    return z

def build_position_ids(attention_mask: torch.Tensor) -> torch.Tensor:
    """
    ç®€æ˜“ position_idsï¼šå¯¹æ¯è¡Œçš„æœ‰æ•ˆ tokenï¼ˆmask=1ï¼‰åšé€’å¢è®¡æ•°ï¼Œpadding å¤„ä¿æŒ 0ã€‚
    ä¸å¾ˆå¤š LLM å…¼å®¹ï¼›è‹¥ä½ å·²æœ‰è‡ªå®šä¹‰å®ç°ï¼Œä¿ç•™ä½ è‡ªå·±çš„å³å¯ã€‚
    """
    # (B, T)
    cumsum = attention_mask.long().cumsum(dim=1) * attention_mask.long()
    # è®©ä» 0 å¼€å§‹ï¼šæŠŠéé›¶ä½ç½®å‡ 1
    pos_ids = (cumsum - attention_mask.long()).clamp(min=0)
    return pos_ids

class MolAwareCausalLM(nn.Module):
    """
    é›†æˆ NER/GNN/Diffusion çš„ç»„åˆæ¨¡å‹ï¼›æŒ‰å‡ºç°é¡ºåºæŠŠ <mol> å¯¹åº”çš„å‘é‡â€œè¿½åŠ åˆ°åºåˆ—æœ«å°¾â€çš„è™šæ‹Ÿæ­¥ï¼Œ
    è®­ç»ƒæ—¶ labels=-100 ä¸è®¡æŸï¼Œæ¨ç†æ—¶æ¨è¿› KV ä½†ä¸å‡º tokenã€‚
    """
    # --------------------------- åˆå§‹åŒ– ---------------------------
    def __init__(
        self,
        llm: nn.Module,
        tokenizer,
        mol_token: str = "<mol>",
        proxy: Optional[str] = None,
        debug: bool = False,
        target_layer_for_capture: int = -1,
        gvp_encoder_config: Optional[Dict] = None,
        mol_adapter_config: Optional[Dict] = None,
        diffusion_config: Optional[Dict] = None,
        diffusion_adapter_config: Optional[Dict] = None,
        token_classifier_head: Optional[nn.Module] = None,
        disable_gnn: bool = False,  # æ–°å¢ï¼šæ˜¯å¦ç¦ç”¨ GNN å¤„ç†
        use_ldmol: bool = True,  # æ˜¯å¦ä½¿ç”¨ LDMol @xyd
        ldmol_skip_text_encoder: bool = False,  # æ˜¯å¦è·³è¿‡ LDMol å†…ç½®çš„ text_encoder @xyd
        layer2_config: Optional[Dict] = None,  # Layer2 é…ç½®
        use_layer2: bool = False,  # æ˜¯å¦ä½¿ç”¨ Layer2
    ):
        super().__init__()
        self.llm = llm
        self.tokenizer = tokenizer
        self.mol_token = mol_token
        self.mol_token_id = tokenizer.convert_tokens_to_ids(mol_token)
        self.pad_token_id = tokenizer.pad_token_id
        self.eos_token_id = tokenizer.eos_token_id
        self.debug = debug
        self.proxy = proxy
        self.disable_gnn = disable_gnn  # æ–°å¢ï¼šç¦ç”¨ GNN æ ‡å¿—

        # if self.mol_token_id is None or self.mol_token_id < 0:
            # raise ValueError(f"Tokenizer does not contain mol_token '{mol_token}'. Please add it first.")

        layers_ref = None
        if hasattr(self.llm, "model") and hasattr(self.llm.model, "layers"):
            layers_ref = self.llm.model.layers
        elif hasattr(self.llm, "transformer") and hasattr(self.llm.transformer, "h"):
            layers_ref = self.llm.transformer.h
        object.__setattr__(self, "_layers_ref", layers_ref)
        self.num_layers = len(self._layers_ref) if self._layers_ref is not None else 0
        self.target_layer_for_capture = (
            self.num_layers - 1 if (target_layer_for_capture < 0 and self.num_layers > 0) else target_layer_for_capture
        )
        self._capture_bucket: List[List[torch.Tensor]] = []
        self._capture_hook = None

        # ---------- ç»„ä»¶ ----------
        try:
            llm_hidden_size = self.llm.config.hidden_size
        except Exception:
            llm_hidden_size = self.llm.config.text_config.hidden_size
        # GVPEncoder
        gvp_encoder_cfg = {
            "node_dims": (10, 1),
            "edge_dims": (1, 1),
            "hidden_scalar_dim": 256,
            "hidden_vector_dim": 16,
            "output_dim": 256,
            "num_layers": 4,
        }
        if gvp_encoder_config:
            gvp_encoder_cfg.update(gvp_encoder_config)

        # MLP Adapterï¼ˆæŠŠ GVP å‘é‡æ˜ å°„åˆ° LLM ç»´åº¦ï¼‰
        mol_adapter_cfg = {
            "input_dim": gvp_encoder_cfg["output_dim"],
            "output_dim": llm_hidden_size,
            "hidden_dim": 2048,
            "num_layers": 2,
        }
        if mol_adapter_config:
            mol_adapter_cfg.update(mol_adapter_config)

        ##############################
        #  LDMol åˆå§‹åŒ– @xyd
        # é€šè¿‡ use_ldmol å‚æ•°æ§åˆ¶æ˜¯å¦åŠ è½½ LDMolï¼ˆé˜¶æ®µ1 SFT è®­ç»ƒæ—¶å¯ç¦ç”¨ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
        # é€šè¿‡ ldmol_skip_text_encoder å‚æ•°æ§åˆ¶æ˜¯å¦è·³è¿‡åŠ è½½å†…ç½® text_encoderï¼ˆè”åˆæ¨ç†æ—¶å¤ç”¨ä¸» Qwenï¼‰

        # Temp
        self.use_ldmol = False
        # self.use_ldmol = use_ldmol 

        self.ldmol_skip_text_encoder = ldmol_skip_text_encoder
        
        if self.use_ldmol:
            self.ldmol = LDMolInferer(
                device=self._first_device(),
                skip_text_encoder=self.ldmol_skip_text_encoder,
            )
        else:
            self.ldmol = None
            logging.info(f"LDMol disabled (use_ldmol={use_ldmol})")
        self.enable_diffusion_fallback = ENABLE_DIFFUSION_FALLBACK and self.use_ldmol
        ##############################

        # å…ˆåˆå§‹åŒ– GVP encoderï¼ˆLayer2 éœ€è¦å®ƒï¼‰
        self.gvp_encoder = GVPEncoder(**gvp_encoder_cfg).to(self._first_device())
        self.mol_adapter = MLPAdapter(**mol_adapter_cfg).to(self._first_device())
        
        ##############################
        #  Layer2 åˆå§‹åŒ–ï¼ˆåœ¨ GVP encoder ä¹‹åï¼‰
        # é€šè¿‡ use_layer2 å‚æ•°æ§åˆ¶æ˜¯å¦åŠ è½½ Layer2ï¼ˆç”¨äºååº”äº§ç‡é¢„æµ‹ï¼‰
        self.use_layer2 = use_layer2
        if self.use_layer2:
            layer2_cfg = layer2_config or {}
            self.layer2_inferer = Layer2Inferer(
                config_path=layer2_cfg.get("config_path"),
                device=self._first_device(),
                gvp_encoder=self.gvp_encoder,  # å¤ç”¨ GVP encoder
                gvp_ckpt_path=layer2_cfg.get("gvp_ckpt_path"),
            )
            logging.info(f"Layer2 enabled")
        else:
            self.layer2_inferer = None
            logging.info(f"Layer2 disabled (use_layer2={use_layer2})")
        ##############################
        self.smiles_cache: Dict[str, str] = {}
        # å¯é€‰ï¼šå¤–éƒ¨æ³¨å…¥çš„ token åˆ†ç±»å¤´ï¼ˆç”¨äºæ£€æµ‹åˆ†å­å®ä½“çš„ä½ç½®ï¼‰
        self.token_classifier_head = token_classifier_head
        

        # ---------- GNN Pipeline æ—¥å¿—ç»Ÿè®¡ ----------
        self.gnn_stats = {
            "smiles_processed": 0,
            "gnn_cache_hits": 0,
            "gnn_cache_misses": 0,
            "smiles_valid": 0,
            "smiles_invalid": 0,
            "diffusion_fallback_count": 0,
            "total_mol_embeddings": 0,
        }
        # æ¯Nä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡ç»Ÿè®¡
        self.gnn_log_interval = 10

        # ---------- å…³é”®ï¼šHF Trainer å…¼å®¹å­—æ®µ ----------
        # è®© Trainer æŠŠå®ƒå½“ä½œ PreTrainedModel ä¸€æ ·ä¿å­˜
        self._config = getattr(self.llm, "config", None)
        self._keys_to_ignore_on_save = getattr(self.llm, "_keys_to_ignore_on_save", None)
        self._keys_to_ignore_on_load_missing = getattr(self.llm, "_keys_to_ignore_on_load_missing", None)
        self._keys_to_ignore_on_load_unexpected = getattr(self.llm, "_keys_to_ignore_on_load_unexpected", None)

    # --------------------------- HF å…¼å®¹æ¥å£ ---------------------------
    @property
    def config(self):
        return self._config

    @property
    def _keys_to_ignore_on_save(self):
        return getattr(self.llm, "_keys_to_ignore_on_save", [])

    @_keys_to_ignore_on_save.setter
    def _keys_to_ignore_on_save(self, v):
        # ä»…ä¸ºäº†é¿å… AttributeErrorï¼›Trainer ä¸éœ€è¦æˆ‘ä»¬çœŸæ­£æ”¹ llm çš„å­—æ®µ
        self.__dict__["__keys_to_ignore_on_save"] = v

    @property
    def _keys_to_ignore_on_load_missing(self):
        return getattr(self.llm, "_keys_to_ignore_on_load_missing", [])

    @_keys_to_ignore_on_load_missing.setter
    def _keys_to_ignore_on_load_missing(self, v):
        self.__dict__["__keys_to_ignore_on_load_missing"] = v

    @property
    def _keys_to_ignore_on_load_unexpected(self):
        return getattr(self.llm, "_keys_to_ignore_on_load_unexpected", [])

    @_keys_to_ignore_on_load_unexpected.setter
    def _keys_to_ignore_on_load_unexpected(self, v):
        self.__dict__["__keys_to_ignore_on_load_unexpected"] = v

    def to(self, *args, **kwargs):
        # åŒæ­¥æŠŠåº•åº§ LLM ä¸è‡ªå®šä¹‰æ¨¡å—éƒ½è¿ç§»è®¾å¤‡
        super().to(*args, **kwargs)
        self.llm.to(*args, **kwargs)
        self.gvp_encoder.to(*args, **kwargs)
        self.mol_adapter.to(*args, **kwargs)
        if self.ldmol is not None:
            self.ldmol.to(*args, **kwargs)
        if self.layer2_inferer is not None:
            self.layer2_inferer.to(*args, **kwargs)
        return self

    # --------------------------- è¾…åŠ© ---------------------------
    def _first_device(self):
        try:
            return self.llm.model.layers[0].input_layernorm.weight.device
        except Exception:
            return next(self.llm.parameters()).device

    def _get_smiles_from_context(self, llm_context: str) -> Optional[str]:
        if llm_context in self.smiles_cache:
            smiles_map = self.smiles_cache[llm_context]
        else:
            smiles_map = extract_and_convert_online(llm_context, self.proxy)
            self.smiles_cache[llm_context] = smiles_map
        if not smiles_map:
            return None
        last_cem = ""
        last_idx = -1
        for cem_name in smiles_map:
            idx = llm_context.rfind(cem_name)
            if idx > last_idx:
                last_idx = idx
                last_cem = cem_name
        return smiles_map.get(last_cem)

    def _extract_last_between_mol_tags(self, text: str) -> Optional[str]:
        """
        æå–æ–‡æœ¬ä¸­æœ€åä¸€å¯¹ <mol>...</mol> çš„å†…éƒ¨å†…å®¹ï¼›æ‰¾ä¸åˆ°è¿”å› Noneã€‚
        """
        if not text:
            return None
        start = text.rfind("<mol>")
        end = text.rfind("</mol>")
        if start == -1 or end == -1 or end <= start:
            return None
        inner = text[start + len("<mol>"):end].strip()
        return inner if inner else None

    def _find_all_mol_spans(self, text: str):
        """
        è¿”å›æ‰€æœ‰ <mol>...</mol> çš„ (inner_text, end_char_index) åˆ—è¡¨ï¼Œend_char_index æŒ‡å‘ </mol> æœ«å°¾åœ¨ text ä¸­çš„å­—ç¬¦ä½ç½®ã€‚
        """
        if not text:
            return []
        try:
            spans = []
            for m in re.finditer(r"<mol>(.*?)</mol>", text, flags=re.DOTALL):
                inner = (m.group(1) or "").strip()
                spans.append((inner, m.end()))
            return spans
        except Exception:
            return []

    def _detect_mol_entities_with_classifier(self, input_ids: torch.Tensor, dec_text: str, enable_thinking: bool = False) -> List[Tuple[str, int]]:
        """
        ä½¿ç”¨ token_classifier_head æ£€æµ‹åˆ†å­å®ä½“ï¼Œå‚è€ƒ mlp_inference.py çš„å®ç°ã€‚
        å¦‚æœæ²¡æœ‰ token_classifier_head æˆ–æ£€æµ‹å¤±è´¥ï¼Œfallback åˆ°æ–‡æœ¬åŒ¹é…æ–¹æ³•ã€‚
        
        Args:
            input_ids: è¾“å…¥ token ids
            dec_text: è§£ç åçš„æ–‡æœ¬
        Returns:
            List[(inner_text, end_char_index)]: æ£€æµ‹åˆ°çš„åˆ†å­å®ä½“ spans
        """
        # ä¼˜å…ˆï¼šè‹¥æ–‡æœ¬å·²åŒ…å«ç¦»çº¿æ ‡æ³¨çš„ <mol>...</mol>ï¼Œç›´æ¥ç”¨æ–‡æœ¬åŒ¹é…ï¼Œé¿å…é¢å¤–å‰å‘
        if ("<mol>" in dec_text) and ("</mol>" in dec_text):
            return self._find_all_mol_spans(dec_text)
        # å…¶æ¬¡ï¼šè‹¥æ²¡æœ‰åˆ†ç±»å™¨ï¼Œåˆ™å›é€€åˆ°æ–‡æœ¬åŒ¹é…
        if self.token_classifier_head is None:
            # logger.info("[TokenClassifier] âŒ No token_classifier_head, using text matching fallback")
            return self._find_all_mol_spans(dec_text)
        
        # ä¼˜åŒ–ï¼šä½¿ç”¨tokenæ•°é‡è€Œä¸æ˜¯å­—ç¬¦æ•°æ¥åˆ¤æ–­ï¼Œæ›´å‡†ç¡®
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºtokenæ•°æ¥ä¼°ç®—
        try:
            # å¿«é€Ÿä¼°ç®—ï¼šå¤§çº¦4ä¸ªå­—ç¬¦ = 1ä¸ªtokenï¼ˆå¯¹äºè‹±æ–‡å’ŒSMILESï¼‰
            estimated_tokens = len(dec_text) // 4
            max_tokens = getattr(self, '_max_text_length_for_detection', 4096) // 4  # è½¬æ¢ä¸ºtokenä¼°ç®—
            if estimated_tokens > max_tokens * 2:  # å…è®¸æ›´å¤§çš„å®¹å·®
                if getattr(self, '_verbose_logging', False):
                    logging.debug(f"[TokenClassifier] âš ï¸  Text too long (est. {estimated_tokens} tokens), will use truncation")
        except Exception:
            pass  # å¦‚æœä¼°ç®—å¤±è´¥ï¼Œç»§ç»­å¤„ç†
        
        try:
            # åªåœ¨verboseæ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
            if getattr(self, '_verbose_logging', False):
                text_preview = dec_text[:500] if len(dec_text) > 500 else dec_text
                preview_suffix = "..." if len(dec_text) > 500 else ""
                logger.info(f"[TokenClassifier] ğŸ” Starting entity detection with classifier. Text length: {len(dec_text)} chars. Preview:\n{text_preview}{preview_suffix}")
            
            # 1) æ¸…é™¤åŸæœ‰æ ‡ç­¾çš„ä¸´æ—¶æ–‡æœ¬ï¼ˆç”¨äºåˆ†ç±»å™¨æ£€æµ‹ï¼‰
            text_clean = re.sub(r"</?mol>", "", dec_text)
            
            # 2) Tokenize æ¸…é™¤æ ‡ç­¾åçš„æ–‡æœ¬è·å– offsets
            # ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨tokenizerçš„truncationæœºåˆ¶ï¼Œè®©tokenizerè‡ªåŠ¨å¤„ç†é•¿åº¦é™åˆ¶
            # ä½¿ç”¨æ›´å¤§çš„max_lengthä»¥æ”¯æŒé•¿æ–‡æœ¬ï¼ˆ2048 tokensè¶³å¤Ÿå¤„ç†å¤§éƒ¨åˆ†æƒ…å†µï¼‰
            max_token_length = 2048
            _old_side = getattr(self.tokenizer, "truncation_side", "right")
            self.tokenizer.truncation_side = "left"
            try:
                enc = self.tokenizer(
                    text_clean,
                    return_tensors="pt",
                    return_offsets_mapping=True,
                    padding=False,
                    truncation=True,
                    max_length=max_token_length,
                    add_special_tokens=False
                )
            finally:
                self.tokenizer.truncation_side = _old_side
            clean_input_ids = enc["input_ids"].to(input_ids.device)
            attention_mask = enc["attention_mask"].to(input_ids.device)
            offsets = enc["offset_mapping"][0].tolist()
            
            # 3) ä½¿ç”¨ LLM è·å– hidden statesï¼ˆç”¨äºåˆ†ç±»å™¨ï¼‰
            device = input_ids.device
            with torch.no_grad():
                outputs = self.llm(
                    input_ids=clean_input_ids,
                    attention_mask=attention_mask,
                    output_hidden_states=True,
                    return_dict=True
                )
                hidden_states = outputs.hidden_states[-1]  # (1, T, H)
            
            # 4) ä½¿ç”¨ token_classifier_head è¿›è¡Œåˆ†ç±»
            with torch.no_grad():
                class_logits = self.token_classifier_head(hidden_states)  # (1, T, 2)
                preds = torch.argmax(class_logits, dim=-1)[0].cpu().tolist()
            
            if getattr(self, '_verbose_logging', False):
                logger.info(f"[TokenClassifier] âœ… Classifier prediction completed. Found {sum(1 for p in preds if p == 1)} entity tokens")
            
            # 5) æå–å®ä½“ spansï¼ˆåˆå¹¶è¿ç»­æ ‡ç­¾ä¸º1çš„ç‰‡æ®µï¼‰
            entity_spans = []
            current_start, current_end = None, None
            for label, (start, end) in zip(preds, offsets):
                if start == end:
                    continue
                if label == 1:  # åˆ†å­å®ä½“æ ‡ç­¾
                    if current_start is None:
                        current_start, current_end = start, end
                    else:
                        current_end = end
                else:
                    if current_start is not None:
                        entity_spans.append((current_start, current_end))
                        current_start, current_end = None, None
            if current_start is not None:
                entity_spans.append((current_start, current_end))
            
            # 6) åå¤„ç†ï¼šç¡®ä¿æ ‡ç­¾ä¸æ‰“æ–­å•è¯ï¼ˆæ‰©å±•åˆ°ç©ºæ ¼è¾¹ç•Œï¼‰
            expanded_spans = []
            for start, end in entity_spans:
                while start > 0 and text_clean[start-1] not in " \n\t.,;:!?()[]{}":
                    start -= 1
                while end < len(text_clean) and text_clean[end] not in " \n\t.,;:!?()[]{}":
                    end += 1
                expanded_spans.append((start, end))
            
            # 7) åˆå¹¶é‡å çš„ spanï¼ˆé˜²æ­¢é‡å¤æ ‡è®°ï¼‰
            final_spans = []
            for span in sorted(expanded_spans):
                if not final_spans or span[0] > final_spans[-1][1]:
                    final_spans.append(span)
                else:
                    final_spans[-1] = (final_spans[-1][0], max(final_spans[-1][1], span[1]))
            
            # 8) è½¬æ¢ä¸º (inner_text, end_char) æ ¼å¼
            # æ³¨æ„ï¼šend_char ç°åœ¨æ˜¯ç›¸å¯¹äº text_clean çš„ä½ç½®ï¼Œéœ€è¦æ˜ å°„å›åŸå§‹ dec_text
            result_spans = []
            for start, end in final_spans:
                inner_text = text_clean[start:end].strip()
                if inner_text:
                    # åœ¨åŸå§‹ dec_text ä¸­æœç´¢ï¼ˆè€ƒè™‘å¯èƒ½ç§»é™¤äº†<mol>æ ‡ç­¾ï¼‰
                    # ç›´æ¥åœ¨ text_clean ä¸­æœç´¢æ›´å‡†ç¡®
                    idx_in_clean = text_clean.find(inner_text, max(0, start - 50), min(len(text_clean), end + 50))
                    if idx_in_clean >= 0:
                        # å°† text_clean çš„ä½ç½®æ˜ å°„å› dec_textï¼ˆè€ƒè™‘<mol>æ ‡ç­¾ï¼‰
                        # ç®€åŒ–ï¼šç›´æ¥è¿”å›å®ä½“æ–‡æœ¬ï¼Œä½ç½®ä½¿ç”¨ä¼°ç®—å€¼
                        end_in_clean = idx_in_clean + len(inner_text)
                        result_spans.append((inner_text, end_in_clean))
            
            if getattr(self, '_verbose_logging', False):
                if result_spans:
                    logger.info(f"[TokenClassifier] ğŸ¯ Detected {len(result_spans)} entities: {[r[0] for r in result_spans]}")
                else:
                    logger.info("[TokenClassifier] âš ï¸  No entities detected")
            
            return result_spans
            
        except Exception as e:
            logger.warning(f"[TokenClassifier] Failed to detect entities: {e}, falling back to text matching")
            return self._find_all_mol_spans(dec_text)


    def _decide_smiles_or_diffusion(self, llm_context_text: Optional[str], fallback_hctx: Optional[torch.Tensor]) -> Optional[torch.Tensor]:
        """
        åŸºäº <mol>...</mol> çš„å†…éƒ¨å†…å®¹è¿›è¡Œåˆ¤åˆ«ï¼š
        - è‹¥å†…éƒ¨å¯è¢« RDKit è§£æä¸º SMILESï¼šèµ° GVP -> mol_adapter
        - å¦åˆ™ï¼šåœ¨å…è®¸æ—¶ä½¿ç”¨ diffusion è·¯å¾„
        è¿”å›æ˜ å°„åçš„ LLM ç»´åº¦å‘é‡ï¼Œæˆ– None è¡¨ç¤ºæ— æ³•ç”Ÿæˆã€‚
        """
        inner = self._extract_last_between_mol_tags(llm_context_text or "")
        if inner:
            # ç›´æ¥ç”¨ RDKit åˆ¤å®šæ˜¯å¦ä¸º SMILES
            is_smiles = False
            if Chem is not None:
                try:
                    is_smiles = (Chem.MolFromSmiles(inner) is not None)
                except Exception:
                    pass
            if is_smiles:
                try:
                    if getattr(self, '_verbose_logging', False):
                        logger.info(f"[GVP] ğŸ”µ è°ƒç”¨ GVP encoderï¼ŒSMILES: {inner[:100]}")
                    gvp_embedding = self.gvp_encoder.forward_from_smiles(inner).squeeze(0)
                    result = self.mol_adapter(gvp_embedding)
                    if getattr(self, '_verbose_logging', False):
                        logger.info(f"[GVP] âœ… GVP encoder å®Œæˆï¼Œembedding shape: {result.shape}")
                    return result
                except Exception as e:
                    if getattr(self, '_verbose_logging', False):
                        logger.warning(f"[GVP] âŒ GVP encoder å¤±è´¥: {e}")
                    return None
            # é SMILES æˆ–å¤±è´¥ -> diffusionï¼ˆä»…åœ¨å¯ç”¨å…œåº•æ—¶ï¼‰
            if self.enable_diffusion_fallback:
                if self._verbose_logging:
                    logger.info(f"[Diffusion] ğŸŸ£ è°ƒç”¨ Diffusion fallbackï¼Œå†…å®¹: {inner[:100] if inner else 'None'}")
                result = self._generate_smiles_convert_to_embedding(text=llm_context_text)
                return result
            return None

        # æ²¡æœ‰æˆå¯¹æ ‡ç­¾æ—¶ï¼Œå›é€€æ—§é€»è¾‘ï¼šä»ä¸Šä¸‹æ–‡æŠ½å– CEM å -> SMILES
        smiles = self._get_smiles_from_context(llm_context_text or "") if llm_context_text else None
        if smiles:
            try:
                if getattr(self, '_verbose_logging', False):
                    logger.info(f"[GVP] ğŸ”µ è°ƒç”¨ GVP encoderï¼ˆä»ä¸Šä¸‹æ–‡æå–ï¼‰ï¼ŒSMILES: {smiles[:100]}")
                gvp_embedding = self.gvp_encoder.forward_from_smiles(smiles).squeeze(0)
                result = self.mol_adapter(gvp_embedding)
                if getattr(self, '_verbose_logging', False):
                    logger.info(f"[GVP] âœ… GVP encoder å®Œæˆï¼Œembedding shape: {result.shape}")
                return result
            except Exception as e:
                if getattr(self, '_verbose_logging', False):
                    logger.warning(f"[GVP] âŒ GVP encoder å¤±è´¥: {e}")
                pass
        if fallback_hctx is not None:
            if getattr(self, '_verbose_logging', False):
                logger.info(f"[Diffusion] ğŸŸ£ è°ƒç”¨ Diffusion fallbackï¼ˆæ— æ ‡ç­¾å›é€€ï¼‰")
            result = self._black_box_from_hidden_hctx(fallback_hctx)
            if getattr(self, '_verbose_logging', False):
                logger.info(f"[Diffusion] âœ… Diffusion fallback å®Œæˆï¼Œembedding shape: {result.shape if result is not None else 'None'}")
            return result
        return None

    def _get_last_hidden_before_pos(self, row_ids: torch.Tensor, end_pos: int) -> torch.Tensor:
        assert end_pos > 0, "end_pos should be > 0"
        dev = self._first_device()
        prefix = row_ids[:end_pos].unsqueeze(0).to(dev)
        attn = (prefix != self.pad_token_id).long().to(dev)
        out = self.llm(input_ids=prefix, attention_mask=attn,
                       output_hidden_states=True, use_cache=False, return_dict=True)
        return out.hidden_states[-1][0, -1, :].detach()
    
    def _generate_smiles_convert_to_embedding(self, text: str) -> Optional[torch.Tensor]:
        """
        ä½¿ç”¨LDMolä»textç”Ÿæˆåˆ†å­SMILESï¼Œç„¶åè½¬æ¢ä¸ºgvp embedding
        
        :param text: text for SMILES
        :type text: str
        :return: é”™è¯¯æ—¶è¿”å›None,æ­£å¸¸è¿”å›gvp embedding
        :rtype: Tensor | None
        """
        if self.ldmol is None or not LDMOL_AVAILABLE:
            logger.warning("LDMol unavailable, return None.")
            return None
        if self._verbose_logging:
            logger.info("[Diffusion] ğŸŸ£ å¼€å§‹ Diffusion ç”Ÿæˆ")
        assert self.llm is not None and self.tokenizer is not None, "self.llm or self.tokenizer is None"
        generated_smiles = self.ldmol.generate_molecule(
            description=text,
            qwen=self.llm,
            qwen_tokenizer=self.tokenizer
        )
        if generated_smiles is None:
            logger.warning("LDMol fails to generate smiles, return None.")
            return None
        if self._verbose_logging:
            logger.info(f"âœ… LDMol ç”Ÿæˆ SMILES: {generated_smiles}")
            logger.info(f"[GVP] ğŸ”µ è°ƒç”¨ GVP encoderï¼ˆå¤„ç† Diffusion ç”Ÿæˆçš„ SMILESï¼‰")
        gvp_embedding = self.gvp_encoder.forward_from_smiles(generated_smiles).squeeze(0)
        mol_emb = self.mol_adapter(gvp_embedding)
        if self._verbose_logging:
            logger.info(f"[GVP] âœ… GVP encoder å®Œæˆï¼Œembedding shape: {mol_emb.shape}")
        return mol_emb
            

    def _black_box_from_hidden_hctx(self, h_ctx: torch.Tensor) -> Optional[torch.Tensor]:
        """
        ä½¿ç”¨LDMolä»LLMçš„hidden stateç”Ÿæˆåˆ†å­SMILESï¼Œç„¶åè½¬æ¢ä¸ºembedding
        """
        # TODO
        # raise ValueError("Updating @xyd")
        logger.info("[Diffusion] ğŸŸ£ å¼€å§‹ Diffusion ç”Ÿæˆï¼ˆä» hidden stateï¼‰")
        if self.ldmol_components is None or not LDMOL_AVAILABLE:
            logger.warning("[Diffusion] âŒ LDMol ä¸å¯ç”¨ï¼Œè·³è¿‡")
            return None
        
        dev = self._first_device()
        h_ctx = h_ctx.to(dev)
        
        try:
            # ä½¿ç”¨LDMolä»LLM hidden stateç”ŸæˆSMILES
            from .ldmol.inference import generate_molecule_from_llm_embedding
            gen_smiles = generate_molecule_from_llm_embedding(
                self.ldmol_components, h_ctx, dev
            )
            
            if not gen_smiles:
                if getattr(self, '_verbose_logging', False):
                    logger.warning("[Diffusion] âŒ æœªç”Ÿæˆæœ‰æ•ˆçš„ SMILES")
                return None
            
            if getattr(self, '_verbose_logging', False):
                logger.info(f"[Diffusion] âœ… Diffusion ç”Ÿæˆ SMILES: {gen_smiles}")
            
            # å°†ç”Ÿæˆçš„SMILESè½¬æ¢ä¸ºembeddingï¼ˆä½¿ç”¨GVP+mol_adapterï¼‰
            if getattr(self, '_verbose_logging', False):
                logger.info(f"[GVP] ğŸ”µ è°ƒç”¨ GVP encoderï¼ˆå¤„ç† Diffusion ç”Ÿæˆçš„ SMILESï¼‰")
            gvp_embedding = self.gvp_encoder.forward_from_smiles(gen_smiles).squeeze(0)
            mol_emb = self.mol_adapter(gvp_embedding)
            if getattr(self, '_verbose_logging', False):
                logger.info(f"[GVP] âœ… GVP encoder å®Œæˆï¼Œembedding shape: {mol_emb.shape}")
            return mol_emb
            
        except Exception as e:
            logger.warning(f"[BlackBox] âŒ LDMol generation failed: {e}")
            import traceback
            traceback.print_exc()
            return None

    # def _black_box_embed_offline(
    #     self,
    #     row_ids: torch.Tensor,
    #     row_embeds: torch.Tensor,
    #     row_mask: torch.Tensor,
    #     pos_mol: int,
    # ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], int]:
    #     # åŸºäº </mol> è§¦å‘ï¼šå–åˆ°ç›®å‰ä¸ºæ­¢çš„ä¸Šä¸‹æ–‡ï¼Œè§£æ <mol>...</mol> å†…éƒ¨å¹¶åˆ¤åˆ«
    #     raise ValueError("TODO: _decide_smiles_or_diffusion æ¥å£ä¿®æ”¹ï¼Œå¦‚éœ€ä½¿ç”¨ï¼Œä»£ç éœ€è¦ä¿®æ”¹ @xyd")
    #     llm_context = self.tokenizer.decode(row_ids[:pos_mol + 1].tolist(), skip_special_tokens=True)
    #     h_ctx = self._get_last_hidden_before_pos(row_ids, pos_mol)  # [H]
    #     emb = self._decide_smiles_or_diffusion(llm_context_text=llm_context, fallback_hctx=h_ctx)
    #     return emb

    def _black_box_embed_online(
        self,
        llm_context_text: Optional[str] = None,
        context_ids: Optional[torch.Tensor] = None,
        h_ctx: Optional[torch.Tensor] = None,
    ) -> Optional[torch.Tensor]:
        if getattr(self, '_verbose_logging', False):
            logger.info(f"[Diffusion] ğŸŸ£ è°ƒç”¨ Diffusionï¼ˆä»æ–‡æœ¬ï¼‰ï¼Œæ–‡æœ¬: {llm_context_text[:100] if llm_context_text else 'None'}...")
        if llm_context_text is not None:
            emb = self._decide_smiles_or_diffusion(llm_context_text=llm_context_text, fallback_hctx=h_ctx)
            if emb is not None:
                if getattr(self, '_verbose_logging', False):
                    logger.info(f"[Diffusion] âœ… Diffusion å®Œæˆï¼Œembedding shape: {emb.shape}")
                return emb
        if context_ids is None and llm_context_text is not None:
            dev = self._first_device()
            toks = self.tokenizer(llm_context_text, return_tensors="pt", add_special_tokens=False)
            context_ids = toks["input_ids"].to(dev)
        if context_ids is not None:
            attn = (context_ids != self.pad_token_id).long().to(context_ids.device)
            out = self.llm(
                input_ids=context_ids, attention_mask=attn,
                output_hidden_states=True, use_cache=False, return_dict=True
            )
            h_ctx = out.hidden_states[-1][0, -1, :].detach()
            return self._black_box_from_hidden_hctx(h_ctx)
        return None

    # --------------------------- è®­ç»ƒ/è¯„ä¼°å‰å‘ ---------------------------
    def forward(
            self,
            input_ids: Optional[torch.Tensor] = None,
            attention_mask: Optional[torch.Tensor] = None,
            labels: Optional[torch.Tensor] = None,
            **kwargs,
        ) -> CausalLMOutputWithPast:
            assert input_ids is not None, "MolAwareCausalLM éœ€è¦ input_ids"

            # 1) å…ˆç¦»çº¿æ‹¼æ¥ <mol> çš„åµŒå…¥åˆ°åºåˆ—æœ«å°¾ï¼ˆä»…åœ¨å­˜åœ¨ <mol> æ—¶è¿½åŠ ï¼‰
            new_embeds, new_masks, new_labels, appended_mol_cnt = self._append_mol_embeds_to_end_offline(
                input_ids, attention_mask, labels
            )

            # 2) å¸¸è§„ LLM å‰å‘
            position_ids = build_position_ids(new_masks).to(new_masks.device)
            
            outputs = self.llm(
                inputs_embeds=new_embeds,
                attention_mask=new_masks,
                position_ids=position_ids,
                labels=new_labels,
                return_dict=True,
                **kwargs,
            )

            # 3) â€”â€” DDP å®‰å…¨å¤„ç† â€”â€”ï¼š
            # "æœ¬ rank æ˜¯å¦çœŸçš„è¿½åŠ è¿‡ mol å‘é‡"
            used_mol_local = (appended_mol_cnt > 0)
            # "æ‰€æœ‰ rank æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªç”¨åˆ° mol åˆ†æ”¯"
            used_mol_global = any_rank_true(used_mol_local)

            if used_mol_global and (not used_mol_local) and (outputs.loss is not None):
                if hasattr(self, "mol_adapter"):
                    outputs.loss = outputs.loss + zero_touch_module(self.mol_adapter)
                if hasattr(self, "gnn_mlp"):
                    outputs.loss = outputs.loss + zero_touch_module(self.gnn_mlp)
                if hasattr(self, "diffusion_mlp"):
                    outputs.loss = outputs.loss + zero_touch_module(self.diffusion_mlp)

            return outputs

    def _append_mol_embeds_to_end_offline(
            self,
            input_ids: torch.Tensor,
            attention_mask: Optional[torch.Tensor],
            labels: Optional[torch.Tensor],
        ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], int]:
        """
        å°† batch å†…æ¯ä¸ªæ ·æœ¬ä¸­å‡ºç°çš„ <mol>...</mol> å¯¹ï¼ŒåŸºäº </mol> é—­åˆå¤„è§¦å‘ä¸€æ¬¡â€œè™šæ‹Ÿæ­¥â€ï¼š
        - è‹¥å†…éƒ¨æ˜¯å¯è§£æçš„ SMILESï¼šSMILES -> GVP -> mol_adapterï¼Œå¾—åˆ° LLM ç»´åº¦å‘é‡
        - å¦åˆ™ï¼ˆä¸”å¯ç”¨å…œåº•ï¼‰ï¼šç”¨ diffusion è·¯å¾„ï¼ˆåŸºäº h_ctx æˆ–ä¸Šä¸‹æ–‡ï¼‰å¾—åˆ°å‘é‡
        ç„¶åæŠŠè¿™äº›å‘é‡â€œè¿½åŠ åˆ°åºåˆ—æœ«å°¾â€ï¼›å¯¹åº” mask=1ï¼Œlabel=-100ï¼ˆä¸è®¡ LM lossï¼‰ã€‚
        è¿”å›ï¼š(new_embeds, new_masks, new_labels, appended_mol_cnt_total)
        """
        # å¦‚æœç¦ç”¨äº† GNNï¼Œç›´æ¥è¿”å›åŸå§‹ embeddingsï¼Œä¸å¤„ç†ä»»ä½• <mol> æ ‡ç­¾
        if self.disable_gnn:
            embed_tokens = self.llm.get_input_embeddings()
            embeds = embed_tokens(input_ids)
            if attention_mask is None:
                attention_mask = (input_ids != self.pad_token_id).long().to(input_ids.device)
            return embeds, attention_mask, labels, 0
        
        assert input_ids.dim() == 2, "input_ids å½¢çŠ¶åº”ä¸º (B, T)"
        embed_tokens = self.llm.get_input_embeddings()
        emb_dev = embed_tokens.weight.device

        input_ids = input_ids.to(emb_dev)
        if attention_mask is not None:
            attention_mask = attention_mask.to(emb_dev)
        if labels is not None:
            labels = labels.to(emb_dev)

        B, T = input_ids.shape
        device = input_ids.device
        embeds = embed_tokens(input_ids)         # (B, T, D)
        D = embeds.size(-1)

        if attention_mask is None:
            attention_mask = (input_ids != self.pad_token_id).long().to(device)
        has_labels = labels is not None

        rows_embeds, rows_masks, rows_labels = [], [], []
        max_len = 0
        appended_mol_cnt_total = 0  # æœ¬ batch å®é™…è¿½åŠ çš„ mol å‘é‡ä¸ªæ•°

        # per-forward å±€éƒ¨ç¼“å­˜ï¼šSMILES -> mol_embï¼ˆä¿ç•™è®¡ç®—å›¾ä»¥è®­ç»ƒ mol_adapterï¼‰
        per_forward_mol_emb_cache: Dict[str, torch.Tensor] = {}

        for b in range(B):
            row_ids = input_ids[b]          # (T,)
            row_emb = embeds[b]             # (T, D)
            row_msk = attention_mask[b]     # (T,)
            row_lbl = labels[b] if has_labels else None

            # å…ˆæŠŠåŸå§‹ token çš„ embed/mask/label æŒ‰é¡ºåºå‹å…¥
            new_emb_list = [row_emb[i] for i in range(T)]
            new_msk_list = [int(row_msk[i].item()) for i in range(T)]
            new_lbl_list = [int(row_lbl[i].item()) for i in range(T)] if has_labels else None

            # ä½¿ç”¨ token_classifier_head æ£€æµ‹åˆ†å­å®ä½“çš„ä½ç½®
            valid_len = int(row_msk.sum().item())
            dec_text = self.tokenizer.decode(row_ids[:valid_len].tolist(), skip_special_tokens=False)
            
            # è·å–å®ä½“ spansï¼ˆä½¿ç”¨ token_classifier_head æˆ– fallback åˆ°æ–‡æœ¬åŒ¹é…ï¼‰
            spans = self._detect_mol_entities_with_classifier(row_ids[:valid_len], dec_text)  # [(inner, end_char)]

            if spans:
                # ä½¿ç”¨ offsets_mapping å°†å­—ç¬¦ä½ç½®æ˜ å°„åˆ° token ç´¢å¼•
                toks = self.tokenizer(dec_text, return_offsets_mapping=True, add_special_tokens=False)
                offsets = toks.get("offset_mapping")
                trigger_idx_to_span = {}
                if offsets is not None:
                    # fast tokenizer: offsets æ˜¯ batch ç»´çš„
                    offsets = offsets[0].tolist() if hasattr(offsets, "tolist") else offsets
                    # ä¸ºæ¯ä¸ª span æ‰¾ token è¾¹ç•Œç´¢å¼•ï¼ˆç¬¬ä¸€ä¸ª end>=end_char çš„ tokenï¼‰
                    for inner, end_char in spans:
                        tok_idx = None
                        for i_off, (_s, _e) in enumerate(offsets):
                            if _e >= end_char and _e > 0:
                                tok_idx = i_off
                                break
                        if tok_idx is None:
                            tok_idx = len(offsets) - 1
                        # ä¿æŠ¤ï¼šä¸è¦è¶…è¿‡æœ‰æ•ˆé•¿åº¦
                        tok_idx = min(tok_idx, valid_len - 1)
                        trigger_idx_to_span[tok_idx] = (inner, end_char)

                # éå†è§¦å‘ç‚¹ï¼Œåœ¨è¯¥ token ç´¢å¼•å¤„ä¸ºè¯¥æ ·æœ¬è¿½åŠ ä¸€æ¬¡è™šæ‹Ÿå‘é‡
                for trig_idx, (inner_text, end_char) in sorted(trigger_idx_to_span.items()):
                    # å¯¹åº”ä½ç½®æ˜¯ padding åˆ™è·³è¿‡
                    if new_msk_list[trig_idx] == 0:
                        continue

                    mol_emb = None
                    # åˆ¤å®šæ˜¯å¦æ˜¯ SMILES
                    is_smiles = False
                    if Chem is not None and inner_text:
                        try:
                            is_smiles = (Chem.MolFromSmiles(inner_text) is not None)
                        except Exception:
                            is_smiles = False

                    if is_smiles and inner_text:
                        # logger.info(f"[EntityProcessing] âœ… Entity '{inner_text}' is valid SMILES, using GVP+adapter")
                        # â€”â€” å‘½ä¸­å±€éƒ¨ç¼“å­˜å°±ç›´æ¥å¤ç”¨ï¼ˆä¿ç•™è®¡ç®—å›¾ï¼‰â€”â€”
                        if inner_text in per_forward_mol_emb_cache:
                            mol_emb = per_forward_mol_emb_cache[inner_text]
                            self.gnn_stats["gnn_cache_hits"] += 1
                        else:
                            # ä¸€èˆ¬ GVP å†»ç»“ï¼Œå¯ no_gradï¼ŒèŠ‚çœæ˜¾å­˜/ç®—åŠ›ï¼›mol_adapter éœ€è¦æ¢¯åº¦ä»¥ä¾¿è®­ç»ƒ
                            with torch.no_grad():
                                gvp_embedding = self.gvp_encoder.forward_from_smiles(inner_text).squeeze(0)
                            mol_emb = self.mol_adapter(gvp_embedding)  # shape: [D]
                            per_forward_mol_emb_cache[inner_text] = mol_emb
                            self.gnn_stats["gnn_cache_misses"] += 1
                            self.gnn_stats["smiles_processed"] += 1
                            self.gnn_stats["smiles_valid"] += 1
                        self.gnn_stats["total_mol_embeddings"] += 1
                    elif self.enable_diffusion_fallback:
                        if getattr(self, '_verbose_logging', False):
                            logger.info(f"[EntityProcessing] ğŸ² Entity '{inner_text}' is NOT valid SMILES, calling BLACKBOX fallback")
                        # å…œåº•ï¼šç”¨æ–‡æœ¬åˆ° </mol> ç»“æŸä½ç½®ä½œä¸ºä¸Šä¸‹æ–‡è®¡ç®— h_ctx / æˆ–ç›´æ¥åœ¨çº¿é»‘ç›’
                        ctx_text = dec_text[:end_char]
                        mol_emb = self._black_box_embed_online(llm_context_text=ctx_text, context_ids=None, h_ctx=None)
                        if mol_emb is not None:
                            if getattr(self, '_verbose_logging', False):
                                logger.info(f"[EntityProcessing] âœ… Blackbox returned embedding successfully")
                            self.gnn_stats["diffusion_fallback_count"] += 1
                        else:
                            # å…³é”®ä¿¡æ¯ï¼šblackbox å¤±è´¥åº”è¯¥æ€»æ˜¯è­¦å‘Š
                            logger.warning(f"[EntityProcessing] âŒ Blackbox returned None for entity '{inner_text[:50]}...'")
                    else:
                        if getattr(self, '_verbose_logging', False):
                            logger.info(f"[EntityProcessing] âš ï¸  Entity '{inner_text}' is invalid and fallback disabled")
                        self.gnn_stats["smiles_processed"] += 1
                        self.gnn_stats["smiles_invalid"] += 1

                    if mol_emb is None:
                        # å…³é”®ä¿¡æ¯ï¼šå¦‚æœè·³è¿‡äº†å¾ˆå¤šå®ä½“ï¼Œåº”è¯¥è®°å½•
                        if getattr(self, "debug", False):
                            if getattr(self, '_verbose_logging', False):
                                logger.info("[Offline] Skip virtual step at </mol> (no embedding).")
                            else:
                                # é verbose æ¨¡å¼ä¸‹ï¼Œåªåœ¨ debug æ—¶è¾“å‡ºç®€è¦ä¿¡æ¯
                                pass
                        continue

                    new_emb_list.append(mol_emb)
                    new_msk_list.append(1)
                    if has_labels:
                        new_lbl_list.append(-100)
                    appended_mol_cnt_total += 1

            # æœ¬æ ·æœ¬çš„æ‹¼æ¥ç»“æœ -> tensor
            new_len = len(new_msk_list)
            max_len = max(max_len, new_len)

            new_emb = torch.stack(new_emb_list, dim=0)                                 # (L, D)
            new_msk = torch.tensor(new_msk_list, device=device, dtype=row_msk.dtype)   # (L,)
            new_lbl = (torch.tensor(new_lbl_list, device=device, dtype=input_ids.dtype)
                    if has_labels else None)

            rows_embeds.append(new_emb)
            rows_masks.append(new_msk)
            if has_labels:
                rows_labels.append(new_lbl)

        # å¯¹é½åˆ°åŒä¸€é•¿åº¦ï¼ˆå³ä¾§ paddingï¼‰
        padded_embeds, padded_masks = [], []
        padded_labels = [] if has_labels else None

        for b in range(B):
            E = rows_embeds[b]; M = rows_masks[b]
            pad_len = max_len - E.size(0)

            if pad_len > 0:
                E = torch.cat([E, torch.zeros(pad_len, D, device=E.device, dtype=E.dtype)], dim=0)
                M = torch.cat([M, torch.zeros(pad_len, device=M.device, dtype=M.dtype)], dim=0)
                if has_labels:
                    L = rows_labels[b]
                    L = torch.cat([L, torch.full((pad_len,), -100, device=L.device, dtype=L.dtype)], dim=0)
                else:
                    L = None
            else:
                L = rows_labels[b] if has_labels else None

            padded_embeds.append(E.unsqueeze(0))  # (1, max_len, D)
            padded_masks.append(M.unsqueeze(0))   # (1, max_len)
            if has_labels:
                padded_labels.append(L.unsqueeze(0) if L is not None else None)

        new_embeds = torch.cat(padded_embeds, dim=0)              # (B, max_len, D)
        new_masks  = torch.cat(padded_masks,  dim=0)              # (B, max_len)
        new_labels = torch.cat(padded_labels, dim=0) if has_labels else None  # (B, max_len) or None

        # å…³é”®ä¿¡æ¯ï¼šå¦‚æœæœ¬æ‰¹æ¬¡å¤„ç†äº†å®ä½“ï¼Œè¾“å‡ºç®€è¦æ‘˜è¦
        if appended_mol_cnt_total > 0:
            if getattr(self, '_verbose_logging', False):
                logger.info(f"[Offline] Batch processed: appended {appended_mol_cnt_total} mol embeddings")
            # é verbose æ¨¡å¼ä¸‹ä¸è¾“å‡ºï¼Œé¿å…è¿‡äºé¢‘ç¹

        # å®šæœŸæ‰“å°GNN pipelineç»Ÿè®¡ä¿¡æ¯ï¼ˆå…³é”®ä¿¡æ¯ï¼Œæ€»æ˜¯è¾“å‡ºï¼‰
        if hasattr(self, "gnn_stats") and appended_mol_cnt_total > 0:
            stats = self.gnn_stats
            total = stats["gnn_cache_hits"] + stats["gnn_cache_misses"]
            if total > 0 and (stats["total_mol_embeddings"] % self.gnn_log_interval == 0):
                hit_rate = stats["gnn_cache_hits"] / total * 100 if total > 0 else 0
                if getattr(self, '_verbose_logging', False):
                    # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
                    logger.info(
                        f"[GNN Pipeline] Stats: SMILES processed={stats['smiles_processed']}, "
                        f"valid={stats['smiles_valid']}, invalid={stats['smiles_invalid']}, "
                        f"cache_hits={stats['gnn_cache_hits']}, cache_misses={stats['gnn_cache_misses']}, "
                        f"hit_rate={hit_rate:.1f}%, diffusion_fallback={stats['diffusion_fallback_count']}, "
                        f"total_embeddings={stats['total_mol_embeddings']}"
                    )
                else:
                    # ç®€è¦ç»Ÿè®¡ä¿¡æ¯ï¼ˆå…³é”®ä¿¡æ¯ï¼‰
                    logger.info(
                        f"[GNN Pipeline] Processed {stats['total_mol_embeddings']} embeddings "
                        f"(valid: {stats['smiles_valid']}, invalid: {stats['smiles_invalid']}, "
                        f"cache hit rate: {hit_rate:.1f}%)"
                    )
        
        if getattr(self, "debug", False):
            orig_tokens = attention_mask.sum().item()
            new_tokens  = new_masks.sum().item()
            print(f"[MolAware/offline] appended {int(new_tokens - orig_tokens)} embeddings to batch end; "
                f"mol_appended_count={appended_mol_cnt_total}")

        return new_embeds, new_masks, new_labels, appended_mol_cnt_total


    @torch.no_grad()
    def generate(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        realtime_mol: bool = True,
        max_new_tokens: int = 256,
        do_sample: bool = False,
        temperature: float = 1.0,
        top_k: int = 0,
        top_p: float = 1.0,
        eos_token_id: Optional[int] = None,
        repetition_penalty: float = 1.05,
        verbose_logging: bool = False,  # æ§åˆ¶è¯¦ç»†æ—¥å¿—è¾“å‡º
        max_text_length_for_detection: int = 4096,  # è¶…å‡ºæ­¤é•¿åº¦è·³è¿‡å®ä½“æ£€æµ‹ï¼ˆä½†ä¸åœæ­¢ç”Ÿæˆï¼‰ï¼Œæ”¯æŒfew-shotç­‰é•¿prompt
        skip_special_tokens: bool = False,
        stop_on_eos: bool = True,  # âœ… æ–°å¢ï¼šæ˜¯å¦é‡åˆ°EOS/eotåœæ­¢ï¼ˆé»˜è®¤ Trueï¼‰
        **kwargs,
    ):
        """
        æ¨ç†é˜¶æ®µåœ¨çº¿å¤„ç†ï¼ˆonlineï¼‰ï¼š
        - é€ token é‡‡æ ·
        - åœ¨è¾¹ç•Œ token å¤„ï¼ˆæˆ–æ£€æµ‹åˆ°æ–°çš„ </mol> å¯¹ï¼‰æ£€æµ‹å®ä½“å¹¶æ’å…¥ä¸€æ¬¡ virtual stepï¼ˆinputs_embedsï¼‰
        å…³é”®ä¿®å¤ï¼š
        - âœ… æ¢å¤ stop tokensï¼ˆEOS / <|eot_id|>ï¼‰ï¼Œå¦åˆ™å¿…è·‘æ»¡ max_new_tokens
        - âœ… virtual step å»é‡ï¼šåŒä¸€æ¬¡â€œå‡ºç°(æ–‡æœ¬, end_char)â€åªæ’ä¸€æ¬¡ï¼Œé¿å…åå¤æ’å…¥é€ æˆé‡å¤/ä¸æ”¶æ•›
        """
        use_cache = kwargs.pop("use_cache", True)
        no_repeat_ngram_size = int(kwargs.pop("no_repeat_ngram_size", 0) or 0)

        try:
            self.llm.config.use_cache = True
        except Exception:
            pass

        if not realtime_mol:
            # èµ° HF è‡ªå¸¦ generateï¼ˆæ­£å¸¸ EOS åœæ­¢ï¼‰
            return self.llm.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                eos_token_id=eos_token_id,
                repetition_penalty=repetition_penalty,
                use_cache=use_cache,
                **kwargs,
            )

        # realtime_mol æ”¯æŒ input_ids å’Œ/æˆ– inputs_embeds
        # - å¦‚æœåŒæ—¶æä¾›ï¼šä½¿ç”¨ input_ids è¿›è¡Œ token æ£€æµ‹ï¼Œinputs_embeds ä½œä¸ºé¢å¤– embedding æ’å…¥ï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰
        # - å¦‚æœåªæä¾› input_idsï¼šæ­£å¸¸å¤„ç†
        # - å¦‚æœåªæä¾› inputs_embedsï¼šè·³è¿‡ token æ£€æµ‹ï¼ˆå› ä¸ºå·²ç»æ˜¯ embedding äº†ï¼‰
        inputs_embeds_extra = kwargs.pop("inputs_embeds", None)
        has_input_ids = input_ids is not None
        has_inputs_embeds = inputs_embeds_extra is not None
        
        if not has_input_ids and not has_inputs_embeds:
            raise ValueError("å¿…é¡»æä¾› input_ids æˆ– inputs_embeds")
        
        if not realtime_mol:
            # é realtime_mol æ¨¡å¼ï¼Œä½¿ç”¨æ ‡å‡† generate
            if has_inputs_embeds and not has_input_ids:
                # åªæœ‰ inputs_embeds
                return self.llm.generate(
                    inputs_embeds=inputs_embeds_extra,
                    attention_mask=attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=do_sample,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    eos_token_id=eos_token_id,
                    repetition_penalty=repetition_penalty,
                    use_cache=use_cache,
                    **kwargs,
                )
            else:
                # æœ‰ input_idsï¼ˆå¯èƒ½åŒæ—¶æœ‰ inputs_embedsï¼Œä½†æ ‡å‡† generate ä¸æ”¯æŒåŒæ—¶ä¼ å…¥ï¼‰
                return self.llm.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=do_sample,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    eos_token_id=eos_token_id,
                    repetition_penalty=repetition_penalty,
                    use_cache=use_cache,
                    **kwargs,
                )
        
        # realtime_mol æ¨¡å¼ï¼šæ”¯æŒ input_ids å’Œ/æˆ– inputs_embeds
        if has_input_ids:
            # æœ‰ input_idsï¼Œå¯ä»¥è¿›è¡Œ token æ£€æµ‹
            if input_ids.size(0) > 1:
                raise ValueError(f"realtime_mol ä»…æ”¯æŒ batch=1ï¼Œå½“å‰ batch={input_ids.size(0)}ã€‚è¯·åœ¨è°ƒç”¨æ—¶ç¡®ä¿ batch_size=1 æˆ–é€ä¸ªå¤„ç†")
            if attention_mask is None:
                attention_mask = (input_ids != self.pad_token_id).long()
        elif has_inputs_embeds:
            # åªæœ‰ inputs_embedsï¼Œè·³è¿‡ token æ£€æµ‹
            if inputs_embeds_extra.size(0) > 1:
                raise ValueError(f"realtime_mol ä»…æ”¯æŒ batch=1ï¼Œå½“å‰ batch={inputs_embeds_extra.size(0)}ã€‚è¯·åœ¨è°ƒç”¨æ—¶ç¡®ä¿ batch_size=1 æˆ–é€ä¸ªå¤„ç†")
            if attention_mask is None:
                # ä» inputs_embeds çš„å½¢çŠ¶æ¨æ–­ attention_mask
                attention_mask = torch.ones(inputs_embeds_extra.size(0), inputs_embeds_extra.size(1), dtype=torch.long, device=inputs_embeds_extra.device)
        
        llm = self.llm
        dev = self._first_device()
        
        if has_input_ids:
            input_ids = input_ids.to(dev)
        if has_inputs_embeds:
            inputs_embeds_extra = inputs_embeds_extra.to(dev)
        attention_mask = attention_mask.to(dev)

        # è®¾ç½®æ—¥å¿—ä¸æ£€æµ‹å‚æ•°
        self._verbose_logging = verbose_logging
        self._max_text_length_for_detection = max_text_length_for_detection
        self._detection_interval = getattr(self, "_detection_interval", 5)  # æ¯Nä¸ªè¾¹ç•Œtokenæ£€æµ‹ä¸€æ¬¡
        self._boundary_token_count = 0

        # per-generation cacheï¼šSMILES -> mol_embï¼ˆæ¨ç†æ— æ¢¯åº¦ï¼‰
        gen_mol_emb_cache: Dict[str, torch.Tensor] = {}

        # âœ… å…³é”®ï¼šå»é‡ â€œåŒä¸€æ¬¡å‡ºç°â€ çš„ virtual step
        # ç”¨ (effective_text, end_char_pos) ä½œä¸º keyï¼›åŒä¸€æ¬¡å‡ºç°åªæ’ä¸€æ¬¡
        processed_occurrences = set()

        # ä½ ä¹‹å‰ç”¨ processed_pair_count / processed_inner_textsï¼Œè¿™é‡Œä¿ç•™ä½†ä¸ä½œä¸ºæ’å…¥å”¯ä¸€ä¾æ®
        processed_pair_count = 0
        processed_inner_texts = set()

        # ====== stop tokensï¼ˆâœ… æ¢å¤ï¼‰======
        stop_token_ids = set()
        end_id = eos_token_id if eos_token_id is not None else self.eos_token_id
        if (end_id is None or end_id < 0) and self.tokenizer is not None:
            # å…œåº•å°è¯• <|eot_id|>
            try:
                eot_token_id = self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
                if eot_token_id is not None and eot_token_id >= 0:
                    end_id = eot_token_id
                    if verbose_logging:
                        logger.info(f"[Generate] Using <|eot_id|> (token_id={end_id}) as EOS token fallback")
            except Exception:
                pass

        if stop_on_eos:
            if end_id is not None and end_id >= 0:
                stop_token_ids.add(int(end_id))
            # å¼ºçƒˆå»ºè®®åŠ ä¸Š eotï¼ˆLlama ç³»åˆ—ç»å¸¸ç”¨å®ƒç»“æŸä¸€æ¡æ¶ˆæ¯ï¼‰
            try:
                eot = self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
                if eot is not None and eot >= 0:
                    stop_token_ids.add(int(eot))
            except Exception:
                pass

        def _prepare_probs(_logits: torch.Tensor) -> torch.Tensor:
            probs = torch.softmax(_logits, dim=-1)
            probs = torch.nan_to_num(probs, nan=0.0, posinf=0.0, neginf=0.0)
            probs = torch.clamp(probs, min=0.0)
            sum_probs = probs.sum(dim=-1, keepdim=True)
            probs = probs / sum_probs.clamp(min=1e-8)
            return probs

        def _apply_topk_topp_temp(_logits: torch.Tensor) -> torch.Tensor:
            logits2 = _logits
            if temperature and temperature != 1.0:
                logits2 = logits2 / float(temperature)
            if top_k and top_k > 0:
                v, _ = torch.topk(logits2, int(top_k))
                logits2 = logits2.masked_fill(logits2 < v[:, [-1]], float("-inf"))
            if top_p and top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits2, descending=True)
                probs = torch.softmax(sorted_logits, dim=-1)
                cumprobs = probs.cumsum(dim=-1)
                cutoff = (cumprobs > float(top_p)).float().cumsum(dim=-1).bool()
                sorted_logits[cutoff] = float("-inf")
                logits2 = torch.full_like(logits2, float("-inf")).scatter(1, sorted_indices, sorted_logits)
            return logits2

        def _apply_sampling(_logits: torch.Tensor) -> torch.Tensor:
            if do_sample:
                logits2 = _apply_topk_topp_temp(_logits)
                probs = _prepare_probs(logits2)
                if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs <= 0).all():
                    return torch.argmax(_logits, dim=-1, keepdim=True)
                return torch.multinomial(probs, num_samples=1)
            return torch.argmax(_logits, dim=-1, keepdim=True)

        def _block_no_repeat_ngrams(logits: torch.Tensor, prefix_ids: List[int], gen_ids: List[int], n: int) -> torch.Tensor:
            """
            ç®€æ˜“ no_repeat_ngram_sizeï¼ˆåªåŸºäºå½“å‰ prefix+generatedï¼‰
            å‚è€ƒ transformers çš„é€»è¾‘ï¼Œä½†è¿™é‡Œåšä¸€ä¸ªè½»é‡ç‰ˆï¼šåªç¦æ­¢â€œåˆšè¦å½¢æˆçš„ ngramâ€é‡å¤å‡ºç°è¿‡ã€‚
            """
            if n <= 0:
                return logits
            seq = prefix_ids + gen_ids
            if len(seq) < n:
                return logits
            # å½“å‰è¦é¢„æµ‹çš„æ˜¯ç¬¬ len(seq) ä½ç½® -> å®ƒä¼šå½¢æˆä¸€ä¸ª n-1 å‰ç¼€
            prev_ngram = tuple(seq[-(n - 1):]) if n > 1 else tuple()
            # æ”¶é›†å†å²å‡ºç°è¿‡çš„ ngramï¼šprev_ngram -> next_token åˆ—è¡¨
            banned = set()
            if n == 1:
                # n==1 å°±æ˜¯ç¦æ­¢é‡å¤ tokenï¼ˆå¤ªå¼ºï¼Œä¸å»ºè®®ï¼‰ï¼›è¿™é‡Œä¸åš
                return logits
            for i in range(len(seq) - n + 1):
                ng = tuple(seq[i:i + n])
                if ng[:-1] == prev_ngram:
                    banned.add(ng[-1])
            if banned:
                logits[:, list(banned)] = float("-inf")
            return logits

        # ====== åˆå§‹å‰å‘ ======
        if has_input_ids:
            # ä½¿ç”¨ input_ids è¿›è¡Œåˆå§‹å‰å‘
            outputs = llm(
                input_ids=input_ids,
                attention_mask=attention_mask,
                use_cache=use_cache,
                output_hidden_states=True,
                return_dict=True,
                **kwargs,
            )
        else:
            # åªæœ‰ inputs_embeds
            outputs = llm(
                inputs_embeds=inputs_embeds_extra,
                attention_mask=attention_mask,
                use_cache=use_cache,
                output_hidden_states=True,
                return_dict=True,
                **kwargs,
            )
        past = outputs.past_key_values
        attn_mask = attention_mask

        # ====== å¦‚æœåŒæ—¶æä¾›äº† inputs_embedsï¼Œå°†å…¶ä½œä¸ºé¢å¤– embedding æ’å…¥ï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰======
        if has_input_ids and has_inputs_embeds:
            # åŒæ—¶æœ‰ input_ids å’Œ inputs_embedsï¼Œå°† inputs_embeds æ’å…¥åˆ°åºåˆ—æœ«å°¾
            model_dtype = next(self.llm.parameters()).dtype
            if inputs_embeds_extra.dtype != model_dtype:
                inputs_embeds_extra = inputs_embeds_extra.to(dtype=model_dtype)
            if inputs_embeds_extra.device != dev:
                inputs_embeds_extra = inputs_embeds_extra.to(device=dev)
            
            # ç¡®ä¿ inputs_embeds çš„å½¢çŠ¶æ­£ç¡®ï¼š[1, seq_len, hidden_dim]
            if inputs_embeds_extra.dim() == 2:
                inputs_embeds_extra = inputs_embeds_extra.unsqueeze(0)  # [seq_len, hidden_dim] -> [1, seq_len, hidden_dim]
            
            # æ’å…¥ virtual stepï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰
            if verbose_logging:
                logger.info(f"[Generate] âœ… æ’å…¥é¢å¤– embeddingï¼ˆç±»ä¼¼ GVP è™šæ‹Ÿæ­¥ï¼‰ï¼Œå½¢çŠ¶: {inputs_embeds_extra.shape}")
            
            # æ›´æ–° attention_maskï¼šä¸ºé¢å¤–çš„ embedding æ·»åŠ  mask
            extra_seq_len = inputs_embeds_extra.size(1)
            extra_mask = torch.ones(1, extra_seq_len, device=dev, dtype=attn_mask.dtype)
            
            # å‰å‘ä¼ æ’­ï¼Œæ’å…¥é¢å¤– embedding
            # æ³¨æ„ï¼šè¿™é‡Œåªä¼ å…¥é¢å¤– embedding çš„ attention_maskï¼Œå› ä¸º past_key_values å·²ç»åŒ…å«äº†ä¹‹å‰çš„çŠ¶æ€
            outputs = llm(
                inputs_embeds=inputs_embeds_extra,
                attention_mask=extra_mask,  # åªä¼ å…¥é¢å¤– embedding çš„ mask
                past_key_values=past,
                use_cache=use_cache,
                output_hidden_states=True,
                return_dict=True,
                **kwargs,
            )
            past = outputs.past_key_values
            # æ›´æ–° attn_mask ä»¥åŒ…å«é¢å¤– embedding çš„éƒ¨åˆ†ï¼ˆç”¨äºåç»­ç”Ÿæˆï¼‰
            attn_mask = torch.cat([attn_mask, extra_mask], dim=1)

        # ====== å¤„ç†è¾“å…¥ä¸­å·²å­˜åœ¨å®ä½“ï¼ˆå¯é€‰ï¼Œä½†ä½ åŸé€»è¾‘æœ‰ï¼›è¿™é‡Œä¿ç•™å¹¶åŠ å»é‡ï¼‰======
        # æ³¨æ„ï¼šå¦‚æœåªæœ‰ inputs_embedsï¼ˆæ²¡æœ‰ input_idsï¼‰ï¼Œè·³è¿‡ token æ£€æµ‹ï¼ˆå› ä¸ºå·²ç»æ˜¯ embedding äº†ï¼‰
        if not has_input_ids:
            input_spans = []  # è·³è¿‡æ£€æµ‹
        else:
            try:
                input_text = self.tokenizer.decode(input_ids[0].tolist(), skip_special_tokens=False)

                if hasattr(self, "token_classifier_head") and self.token_classifier_head is not None:
                    input_spans = self._detect_mol_entities_with_classifier(input_ids[0], input_text)
                else:
                    input_spans = self._find_all_mol_spans(input_text)
            except Exception as e:
                if verbose_logging:
                    logger.warning(f"[Generate] âš ï¸  Failed to detect input entities: {e}")
                input_spans = []

            if input_spans:
                model_dtype = next(self.llm.parameters()).dtype

                for inner_text, end_char in input_spans:
                    cleaned_text = (inner_text or "").strip()
                    trailing_punct = ",.;:!?"
                    while cleaned_text and cleaned_text[-1] in trailing_punct:
                        cleaned_text = cleaned_text[:-1].strip()
                    while cleaned_text and cleaned_text[0] in trailing_punct:
                        cleaned_text = cleaned_text[1:].strip()

                    text_to_check = cleaned_text if cleaned_text else inner_text
                    is_smiles = False
                    if Chem is not None and text_to_check:
                        try:
                            mol = Chem.MolFromSmiles(text_to_check)
                            if mol is not None:
                                canonical_smiles = Chem.MolToSmiles(mol)
                                if canonical_smiles and len(text_to_check) >= 5:
                                    is_smiles = True
                        except Exception:
                            is_smiles = False

                    effective_text = text_to_check if is_smiles else (inner_text or "")
                    occ_key = (effective_text, int(end_char))
                    if occ_key in processed_occurrences:
                        continue
                    processed_occurrences.add(occ_key)

                    mol_emb = None
                    if is_smiles and effective_text:
                        cache_key = effective_text
                        if cache_key in gen_mol_emb_cache:
                            mol_emb = gen_mol_emb_cache[cache_key]
                            if verbose_logging:
                                logger.info(f"[Generate] âœ… Input entity (cached): '{effective_text}'")
                        else:
                            try:
                                with torch.no_grad():
                                    gvp_embedding_raw = self.gvp_encoder.forward_from_smiles(effective_text).squeeze(0)
                                    mol_adapter_dtype = next(self.mol_adapter.parameters()).dtype
                                    gvp_embedding = gvp_embedding_raw.to(dtype=mol_adapter_dtype) if gvp_embedding_raw.dtype != mol_adapter_dtype else gvp_embedding_raw
                                    mol_emb = self.mol_adapter(gvp_embedding)
                                    if mol_emb.dtype != model_dtype:
                                        mol_emb = mol_emb.to(dtype=model_dtype)
                                gen_mol_emb_cache[cache_key] = mol_emb
                                if verbose_logging:
                                    logger.info(f"[Generate] âœ… Input entity (fresh): '{effective_text}' -> GVP -> mol_adapter")
                            except Exception as e:
                                logger.warning(f"[Generate] âš ï¸  Failed to process input SMILES '{effective_text}': {e}")
                                mol_emb = None
                    elif self.enable_diffusion_fallback:
                        try:
                            h_ctx = outputs.hidden_states[-1][0, -1, :].detach()
                            mol_emb = self._black_box_from_hidden_hctx(h_ctx)
                            if mol_emb is not None and verbose_logging:
                                logger.info(f"[Generate] âœ… Input entity (diffusion): '{inner_text}'")
                        except Exception as e:
                            logger.warning(f"[Generate] âš ï¸  Diffusion failed on input entity '{inner_text}': {e}")
                            mol_emb = None

                    if mol_emb is not None:
                        if mol_emb.device != dev:
                            mol_emb = mol_emb.to(device=dev)
                        if mol_emb.dtype != model_dtype:
                            mol_emb = mol_emb.to(dtype=model_dtype)

                        # æ’å…¥ virtual step
                        outputs = llm(
                            inputs_embeds=mol_emb.view(1, 1, -1),
                            attention_mask=torch.cat([attn_mask, torch.ones(1, 1, device=dev, dtype=attn_mask.dtype)], dim=1),
                            past_key_values=past,
                            use_cache=use_cache,
                            output_hidden_states=True,
                            return_dict=True,
                            **kwargs,
                        )
                        past = outputs.past_key_values
                        attn_mask = torch.cat([attn_mask, torch.ones(1, 1, device=dev, dtype=attn_mask.dtype)], dim=1)
                        processed_inner_texts.add(effective_text)

                if input_text.count("</mol>") > 0:
                    processed_pair_count = input_text.count("</mol>")

        # ====== ä¸»å¾ªç¯ ======
        generated_ids: List[int] = []
        if has_input_ids:
            prefix_ids = input_ids[0].tolist()
        else:
            # åªæœ‰ inputs_embeds æ—¶ï¼Œæ— æ³•è·å– prefix_idsï¼Œè®¾ä¸ºç©ºåˆ—è¡¨
            prefix_ids = []

        force_detection_next = False  # æ’ virtual step åï¼Œä¸‹ä¸€ä¸ªè¾¹ç•Œæ£€æµ‹ä¸å¼ºåˆ¶ï¼ˆé¿å…æ­»å¾ªç¯ï¼‰ï¼›è¿™é‡Œä¿ç•™ä½†é»˜è®¤ä¸ç”¨

        for step in range(int(max_new_tokens)):
            logits = outputs.logits[:, -1, :]

            # repetition penaltyï¼ˆä½ åŸé€»è¾‘ï¼‰
            if repetition_penalty and repetition_penalty != 1.0 and generated_ids:
                uniq = list(set(generated_ids))
                logits[:, uniq] = logits[:, uniq] / float(repetition_penalty)
                recent_window = min(10, len(generated_ids))
                recent_tokens_penalty = generated_ids[-recent_window:]
                recent_uniq = list(set(recent_tokens_penalty))
                if recent_uniq:
                    logits[:, recent_uniq] = logits[:, recent_uniq] / (float(repetition_penalty) * 1.2)

            # no_repeat_ngramï¼ˆå¯é€‰ï¼Œåªæœ‰ inputs_embeds æ—¶è·³è¿‡ï¼‰
            if has_input_ids and no_repeat_ngram_size and no_repeat_ngram_size > 0:
                logits = _block_no_repeat_ngrams(logits, prefix_ids, generated_ids, no_repeat_ngram_size)

            next_token = _apply_sampling(logits)
            next_id = int(next_token.item())

            # ===== æƒ…å†µ Aï¼šé‡‡æ ·åˆ°äº† <mol> token -> ç«‹å³å°è¯•æ’å…¥ virtual stepï¼ˆä½ åŸé€»è¾‘ä¿ç•™ï¼‰=====
            # æ³¨æ„ï¼šåªæœ‰ inputs_embedsï¼ˆæ²¡æœ‰ input_idsï¼‰æ—¶ï¼Œè·³è¿‡ token æ£€æµ‹ï¼ˆå› ä¸ºå·²ç»æ˜¯ embedding äº†ï¼‰
            if not has_input_ids:
                # åªæœ‰ inputs_embeds æ—¶ï¼Œè·³è¿‡ <mol> token æ£€æµ‹å’Œå¤„ç†
                pass
            elif next_id == self.mol_token_id:
                current_context_ids = torch.cat(
                    [input_ids, torch.tensor([generated_ids], device=dev, dtype=input_ids.dtype)],
                    dim=1
                )
                llm_context_text = self.tokenizer.decode(current_context_ids[0].tolist(), skip_special_tokens=False)

                mol_embedding = None
                gnn_path = None
                inner = self._extract_last_between_mol_tags(llm_context_text or "")

                is_smiles = False
                if inner and Chem is not None:
                    try:
                        is_smiles = (Chem.MolFromSmiles(inner) is not None)
                    except Exception:
                        is_smiles = False

                if inner and is_smiles:
                    try:
                        if inner in gen_mol_emb_cache:
                            mol_embedding = gen_mol_emb_cache[inner]
                            gnn_path = "GNN (cached via <mol>)"
                        else:
                            model_dtype = next(self.llm.parameters()).dtype
                            with torch.no_grad():
                                gvp_embedding_raw = self.gvp_encoder.forward_from_smiles(inner).squeeze(0)
                                mol_adapter_dtype = next(self.mol_adapter.parameters()).dtype
                                gvp_embedding = gvp_embedding_raw.to(dtype=mol_adapter_dtype) if gvp_embedding_raw.dtype != mol_adapter_dtype else gvp_embedding_raw
                                mol_embedding = self.mol_adapter(gvp_embedding)
                                if mol_embedding.dtype != model_dtype:
                                    mol_embedding = mol_embedding.to(dtype=model_dtype)
                            gen_mol_emb_cache[inner] = mol_embedding
                            gnn_path = "GNN (fresh via <mol>)"
                    except Exception as e:
                        logger.warning(f"[Generate] âš ï¸  Failed to process SMILES '{inner}' via <mol>: {e}")
                        mol_embedding = None
                elif self.enable_diffusion_fallback:
                    try:
                        h_ctx_step = outputs.hidden_states[-1][0, -1, :].detach()
                        mol_embedding = self._black_box_from_hidden_hctx(h_ctx_step)
                        if mol_embedding is not None:
                            gnn_path = "Diffusion fallback (via <mol>)"
                    except Exception as e:
                        logger.warning(f"[Generate] âš ï¸  Diffusion fallback failed via <mol>: {e}")
                        mol_embedding = None

                if mol_embedding is None:
                    # ç¦æ­¢é€‰ä¸­ <mol>ï¼Œé‡æ–°é‡‡æ ·
                    logits_block = logits.clone()
                    logits_block[0, self.mol_token_id] = float("-inf")
                    next_token = _apply_sampling(logits_block)
                    next_id = int(next_token.item())
                else:
                    model_dtype = next(self.llm.parameters()).dtype
                    if mol_embedding.dtype != model_dtype:
                        mol_embedding = mol_embedding.to(dtype=model_dtype)
                    if mol_embedding.device != dev:
                        mol_embedding = mol_embedding.to(device=dev)

                    if verbose_logging:
                        logger.info(f"[Generate] ğŸ¯ Inserting virtual step via {gnn_path}")

                    outputs = llm(
                        inputs_embeds=mol_embedding.view(1, 1, -1),
                        attention_mask=torch.cat([attn_mask, torch.ones(1, 1, device=dev, dtype=attn_mask.dtype)], dim=1),
                        past_key_values=past,
                        use_cache=use_cache,
                        output_hidden_states=True,
                        return_dict=True,
                        **kwargs,
                    )
                    past = outputs.past_key_values
                    attn_mask = torch.cat([attn_mask, torch.ones(1, 1, device=dev, dtype=attn_mask.dtype)], dim=1)
                    # ä¸æŠŠ <mol> è®¡å…¥ generated_ids
                    continue

            # ===== å¸¸è§„ç”Ÿæˆä¸€ä¸ª token =====
            step_ids = next_token  # [1,1]
            attn_mask = torch.cat([attn_mask, torch.ones(1, 1, device=dev, dtype=attn_mask.dtype)], dim=1)
            outputs = llm(
                input_ids=step_ids,
                attention_mask=attn_mask,
                past_key_values=past,
                use_cache=use_cache,
                output_hidden_states=True,
                return_dict=True,
                **kwargs,
            )
            past = outputs.past_key_values
            generated_ids.append(next_id)

            # âœ… stop tokenï¼šåŠæ—¶ break
            if stop_on_eos and (next_id in stop_token_ids):
                if verbose_logging:
                    tok_txt = ""
                    try:
                        tok_txt = self.tokenizer.decode([next_id], skip_special_tokens=False)
                    except Exception:
                        pass
                    logger.info(f"[Generate] ğŸ›‘ Stop token hit: id={next_id} text={tok_txt!r}")
                break

            # ===== æƒ…å†µ Bï¼šè¾¹ç•Œå¤„æ£€æµ‹å®ä½“ï¼Œå¹¶æ’ virtual stepï¼ˆâœ… åŠ "å‡ºç°å»é‡"ï¼‰=====
            # æ³¨æ„ï¼šåªæœ‰ inputs_embedsï¼ˆæ²¡æœ‰ input_idsï¼‰æ—¶ï¼Œè·³è¿‡ token æ£€æµ‹ï¼ˆå› ä¸ºå·²ç»æ˜¯ embedding äº†ï¼‰
            if not has_input_ids:
                # åªæœ‰ inputs_embeds æ—¶ï¼Œè·³è¿‡è¾¹ç•Œæ£€æµ‹
                continue
            
            try:
                # æ˜¯å¦éœ€è¦æ£€æµ‹ï¼šè¾¹ç•Œtokenè®¡æ•°/é—´éš”
                should_detect_at_boundary = False
                if force_detection_next:
                    should_detect_at_boundary = True
                    force_detection_next = False
                elif _is_boundary_token(self.tokenizer, next_id):
                    self._boundary_token_count += 1
                    if self._boundary_token_count >= int(self._detection_interval):
                        should_detect_at_boundary = True
                        self._boundary_token_count = 0

                if not should_detect_at_boundary:
                    continue

                # åªæ£€æµ‹å°¾éƒ¨çª—å£ï¼Œçª—å£è¶³å¤Ÿå¤§ + overlapï¼Œé¿å…å®ä½“è·¨çª—è¢«æˆªæ–­
                WINDOW_TOKENS = 2048   # ä½ æ„¿æ„æ…¢å¯ä»¥æ›´å¤§ï¼š3072/4096ï¼ˆçœ‹æ˜¾å­˜/é€Ÿåº¦ï¼‰
                OVERLAP_TOKENS = 512   # é˜²æ­¢å®ä½“è·¨çª—ï¼ˆé•¿SMILES/é•¿åŒ–å­¦åï¼‰

                current_context_ids = torch.cat([input_ids, torch.tensor([generated_ids], device=dev, dtype=input_ids.dtype)], dim=1)
                seq = current_context_ids[0]
                L = seq.numel()

                start = max(0, L - WINDOW_TOKENS)
                tokens_to_detect = seq[start:]  # ä»…å°¾éƒ¨
                text_to_detect = self.tokenizer.decode(tokens_to_detect.tolist(), skip_special_tokens=False)

                # è®¡ç®— offsetï¼šçª—å£å‰é¢é‚£ä¸€æ®µå­—ç¬¦é•¿åº¦ï¼ˆç”¨äºæŠŠ end_char æ˜ å°„å›å…¨å±€ï¼‰
                # ä¸ºäº†ä¸æ¼ï¼Œoffset çš„è®¡ç®—å¯ä»¥æ…¢ä¸€ç‚¹ï¼šdecode ä¸€æ¬¡ prefix
                prefix_text = self.tokenizer.decode(seq[:start].tolist(), skip_special_tokens=False) if start > 0 else ""
                input_offset_chars = len(prefix_text)

                detected_spans = self._detect_mol_entities_with_classifier(tokens_to_detect, text_to_detect)
                # æ˜ å°„å›å…¨å±€å­—ç¬¦ä½ç½®ï¼ˆå¦‚æœä½ åé¢è¦ç”¨ end_char åšå…¨å±€æ¯”è¾ƒï¼‰
                detected_spans = [(t, p + input_offset_chars) for (t, p) in detected_spans]


                current_context_ids = torch.cat(
                    [input_ids, torch.tensor([generated_ids], device=dev, dtype=input_ids.dtype)],
                    dim=1
                )
                llm_context_text = self.tokenizer.decode(current_context_ids[0].tolist(), skip_special_tokens=False)

                full_text_mol_count = llm_context_text.count("</mol>")

                # æ£€æµ‹æ–‡æœ¬ï¼ˆè¿™é‡Œä½ å–æ¶ˆäº†æˆªæ–­ï¼Œæˆ‘ä¿æŒä¸€è‡´ï¼‰
                text_to_detect = llm_context_text
                tokens_to_detect = current_context_ids[0]
                input_offset_chars = 0

                spans: List[Tuple[str, int]] = []

                # å¦‚æœå‡ºç°æ–°çš„ </mol>ï¼Œä¼˜å…ˆè·‘ä¸€æ¬¡æ£€æµ‹
                if full_text_mol_count > processed_pair_count:
                    if hasattr(self, "token_classifier_head") and self.token_classifier_head is not None:
                        detected_spans = self._detect_mol_entities_with_classifier(tokens_to_detect, text_to_detect)
                        if input_offset_chars > 0:
                            detected_spans = [(t, p + input_offset_chars) for t, p in detected_spans]
                        spans.extend(detected_spans)

                # è¾¹ç•Œå¤„å¸¸è§„æ£€æµ‹
                detected_spans = self._detect_mol_entities_with_classifier(tokens_to_detect, text_to_detect)
                if input_offset_chars > 0:
                    detected_spans = [(t, p + input_offset_chars) for t, p in detected_spans]

                # è¿‡æ»¤ï¼šå°½é‡åªä¿ç•™ç”Ÿæˆéƒ¨åˆ†ï¼ˆä½ åŸé€»è¾‘ï¼‰
                input_text_only = self.tokenizer.decode(input_ids[0].tolist(), skip_special_tokens=False)
                generated_text_only = self.tokenizer.decode(generated_ids, skip_special_tokens=False)

                filtered_spans = []
                input_text_len = len(input_text_only)
                for inner_text, end_char_pos in detected_spans:
                    if inner_text in generated_text_only:
                        # å¦‚æœåœ¨ input_text ä¸­ä¹Ÿå‡ºç°è¿‡ï¼Œåˆ™è¦æ±‚ end_char_pos è½åœ¨ input_text_len ä¹‹å
                        if inner_text not in input_text_only:
                            filtered_spans.append((inner_text, end_char_pos))
                        else:
                            if int(end_char_pos) > int(input_text_len):
                                filtered_spans.append((inner_text, end_char_pos))

                # ç”¨ looks_like_molecule å†ç­›ä¸€æ¬¡ï¼ˆä½ åŸé€»è¾‘ï¼‰
                for inner_text, end_char_pos in filtered_spans:
                    inner = (inner_text or "").strip()
                    if _looks_like_molecule(inner):
                        spans.append((inner_text, end_char_pos))

                # å»é‡ spansï¼ˆåŒä¸€è½®é¿å…é‡å¤ï¼‰
                uniq = []
                seen_local = set()
                for t, p in spans:
                    key = (t, int(p))
                    if key in seen_local:
                        continue
                    seen_local.add(key)
                    uniq.append((t, int(p)))
                spans = uniq

                if spans:
                    inserted_virtual_steps = False
                    model_dtype = next(self.llm.parameters()).dtype

                    for inner_text, end_char_pos in spans:
                        # æ¸…ç†å®ä½“æ–‡æœ¬
                        cleaned_text = (inner_text or "").strip()
                        trailing_punct = ",.;:!?"
                        while cleaned_text and cleaned_text[-1] in trailing_punct:
                            cleaned_text = cleaned_text[:-1].strip()
                        while cleaned_text and cleaned_text[0] in trailing_punct:
                            cleaned_text = cleaned_text[1:].strip()

                        text_to_check = cleaned_text if cleaned_text else (inner_text or "")
                        is_smiles = False
                        if Chem is not None and text_to_check:
                            try:
                                mol = Chem.MolFromSmiles(text_to_check)
                                if mol is not None:
                                    canonical_smiles = Chem.MolToSmiles(mol)
                                    if canonical_smiles and len(text_to_check) >= 5:
                                        is_smiles = True
                            except Exception:
                                is_smiles = False

                        effective_text = text_to_check if is_smiles else (inner_text or "")

                        # âœ… å…³é”®ï¼šå‡ºç°å»é‡ â€”â€” åŒä¸€æ¬¡å‡ºç°åªæ’ä¸€æ¬¡
                        occ_key = (effective_text, int(end_char_pos))
                        if occ_key in processed_occurrences:
                            continue
                        processed_occurrences.add(occ_key)

                        mol_emb = None
                        if is_smiles and effective_text:
                            cache_key = effective_text
                            if cache_key in gen_mol_emb_cache:
                                mol_emb = gen_mol_emb_cache[cache_key]
                                if verbose_logging:
                                    logger.info(f"[Generate] âœ… Reuse cached embedding for '{effective_text}'")
                            else:
                                try:
                                    with torch.no_grad():
                                        gvp_embedding_raw = self.gvp_encoder.forward_from_smiles(effective_text).squeeze(0)
                                        mol_adapter_dtype = next(self.mol_adapter.parameters()).dtype
                                        gvp_embedding = gvp_embedding_raw.to(dtype=mol_adapter_dtype) if gvp_embedding_raw.dtype != mol_adapter_dtype else gvp_embedding_raw
                                        mol_emb = self.mol_adapter(gvp_embedding)
                                        if mol_emb.dtype != model_dtype:
                                            mol_emb = mol_emb.to(dtype=model_dtype)
                                    gen_mol_emb_cache[cache_key] = mol_emb
                                    if verbose_logging:
                                        logger.info(f"[Generate] âœ… Fresh embedding for '{effective_text}' (GVP+adapter)")
                                except Exception as e:
                                    logger.warning(f"[Generate] âš ï¸  Failed to process SMILES '{effective_text}': {e}")
                                    mol_emb = None
                        elif self.enable_diffusion_fallback:
                            try:
                                h_ctx_step2 = outputs.hidden_states[-1][0, -1, :].detach()
                                mol_emb = self._black_box_from_hidden_hctx(h_ctx_step2)
                                if mol_emb is not None and verbose_logging:
                                    logger.info(f"[Generate] âœ… Diffusion fallback embedding for '{inner_text}'")
                            except Exception as e:
                                logger.warning(f"[Generate] âš ï¸  Diffusion fallback failed for '{inner_text}': {e}")
                                mol_emb = None

                        if mol_emb is None:
                            continue

                        if mol_emb.device != dev:
                            mol_emb = mol_emb.to(device=dev)
                        if mol_emb.dtype != model_dtype:
                            mol_emb = mol_emb.to(dtype=model_dtype)

                        # æ’å…¥ virtual step
                        outputs = llm(
                            inputs_embeds=mol_emb.view(1, 1, -1),
                            attention_mask=torch.cat([attn_mask, torch.ones(1, 1, device=dev, dtype=attn_mask.dtype)], dim=1),
                            past_key_values=past,
                            use_cache=use_cache,
                            output_hidden_states=True,
                            return_dict=True,
                            **kwargs,
                        )
                        past = outputs.past_key_values
                        attn_mask = torch.cat([attn_mask, torch.ones(1, 1, device=dev, dtype=attn_mask.dtype)], dim=1)
                        inserted_virtual_steps = True
                        processed_inner_texts.add(effective_text)

                    processed_pair_count = max(processed_pair_count, full_text_mol_count)

                    # æ’å…¥äº†è™šæ‹Ÿæ­¥å°±ç»§ç»­ä¸‹ä¸€è½®é‡‡æ ·ï¼ˆé¿å…å½“å‰ token ä½ç½®åå¤æ£€æµ‹ï¼‰
                    if inserted_virtual_steps:
                        continue

            except Exception as e:
                logger.warning(f"[Generate] âš ï¸  Exception in entity detection/GNN logic: {e}", exc_info=False)

        if not generated_ids:
            # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½• tokenï¼Œè¿”å›åŸå§‹è¾“å…¥
            if has_input_ids:
                return input_ids
            else:
                # åªæœ‰ inputs_embeds æ—¶ï¼Œæ— æ³•è¿”å›åŸå§‹è¾“å…¥ï¼ˆå› ä¸ºæ²¡æœ‰ input_idsï¼‰
                # è¿”å›ä¸€ä¸ªç©ºçš„ tensor
                return torch.empty(1, 0, dtype=torch.long, device=dev)
        
        gen = torch.tensor([generated_ids], device=dev, dtype=torch.long)
        # è¿”å›ç»“æœï¼šå¦‚æœæœ‰ input_idsï¼Œæ‹¼æ¥è¿”å›ï¼›å¦åˆ™åªè¿”å›ç”Ÿæˆçš„ token IDs
        if has_input_ids:
            return torch.cat([input_ids, gen], dim=1)
        else:
            return gen


    # --------------------------- HF ä¿å­˜/åŠ è½½ ---------------------------
    def state_dict(self, *args, **kwargs):
        # ä¿å­˜æ•´ä¸ªç»„åˆæ¨¡å‹çš„æƒé‡ï¼ˆåŒ…å«è‡ªå®šä¹‰æ¨¡å— + åº•åº§ llm çš„å‚æ•°æ‹·è´ï¼‰
        sd = super().state_dict(*args, **kwargs)
        # å»é‡ç›¸åŒ storageï¼Œé¿å…ç¨€å¥‡çš„å…±äº« tensor è¢«é‡å¤å¼•ç”¨
        seen = {}
        for k, v in list(sd.items()):
            if not isinstance(v, torch.Tensor):
                continue
            sid = self._storage_id(v)
            if sid in seen:
                sd[k] = v.clone()
            else:
                seen[sid] = k
        return sd

    def save_pretrained(self, save_directory: str, **kwargs):
        """
        - å…ˆè°ƒç”¨åº•åº§ LLM çš„ save_pretrainedï¼ˆä¿å­˜æƒé‡ã€config ç­‰ï¼‰
        - å†é¢å¤–ä¿å­˜ç»„åˆæ¨¡å‹çš„è‡ªå®šä¹‰æ¨¡å—ï¼ˆ.ptï¼‰
        - å†™å…¥ä¸€ä¸ª metadata.json è®°å½•é¢å¤–æ–‡ä»¶åï¼Œä¾¿äº from_pretrained æ¢å¤
        """
        os.makedirs(save_directory, exist_ok=True)
        # 1) ä¿å­˜åº•åº§ LLM
        out = self.llm.save_pretrained(save_directory, **kwargs)

        # 2) é¢å¤–ä¿å­˜è‡ªå®šä¹‰æ¨¡å—
        extras = {}
        if hasattr(self, "gvp_encoder") and self.gvp_encoder is not None:
            torch.save(self.gvp_encoder.state_dict(), os.path.join(save_directory, "gvp_encoder.pt"))
            extras["gvp_encoder"] = "gvp_encoder.pt"
        if hasattr(self, "mol_adapter") and self.mol_adapter is not None:
            torch.save(self.mol_adapter.state_dict(), os.path.join(save_directory, "mol_adapter.pt"))
            extras["mol_adapter"] = "mol_adapter.pt"
        # æ³¨æ„ï¼šdiffusion_adapter å·²ç§»é™¤ï¼ŒLDMolç›´æ¥ä½¿ç”¨LLMçš„hidden states
        # ä¸å†ä¿å­˜ diffusion_adapter

        # diffusion ä¸»ä½“é€šå¸¸ä½“é‡è¾ƒå¤§ä¸”å¯é€‰ï¼Œä¸å¼ºåˆ¶ä¿å­˜ï¼›å¦‚æœéœ€è¦è‡ªè¡ŒåŠ ï¼š
        # if hasattr(self, "diffusion") and self.diffusion is not None:
        #     torch.save(self.diffusion.state_dict(), os.path.join(save_directory, "diffusion.pt"))
        #     extras["diffusion"] = "diffusion.pt"

        meta = {
            "class": "MolAwareCausalLM",
            "version": 1,
            "extras": extras,
            "mol_token": self.mol_token,
        }
        with open(os.path.join(save_directory, "molaware_metadata.json"), "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)

        return out

    @classmethod
    def from_pretrained(cls, save_directory: str, tokenizer=None,
                        diffusion_config=None, diffusion_adapter_config=None,
                        layer2_config=None, use_layer2=False,
                        **kwargs):
        root = save_directory
        meta_path = os.path.join(root, "molaware_metadata.json")
        has_meta = os.path.isfile(meta_path)

        # 1) è§£æ metadataï¼ˆè‹¥å­˜åœ¨ï¼‰
        meta = {}
        extras_map = {}
        if has_meta:
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
            extras_map = meta.get("extras", {}) or {}

        # 2) å†³å®š LLM ç›®å½•ï¼šä¼˜å…ˆ <root>/llmï¼Œå…¶æ¬¡ <root>
        llm_dir = os.path.join(root, "llm")
        if not has_hf_model_files(llm_dir):
            llm_dir = root
        print(f"[from_pretrained] using llm_dir={llm_dir}")

        # 3) åŠ è½½åº•åº§ LLM
        # å¤„ç† torch ç‰ˆæœ¬é™åˆ¶é—®é¢˜ï¼šå¦‚æœé‡åˆ° torch.load å®‰å…¨é™åˆ¶ï¼Œå°è¯•ä½¿ç”¨ safetensors æˆ–ç»•è¿‡æ£€æŸ¥
        try:
            base_llm = AutoModelForCausalLM.from_pretrained(llm_dir, **kwargs)
        except (ValueError, RuntimeError) as e:
            error_str = str(e)
            if ("torch.load" in error_str and "v2.6" in error_str) or ("CVE-2025-32434" in error_str):
                # torch ç‰ˆæœ¬é™åˆ¶ï¼Œå°è¯•ä½¿ç”¨ç¯å¢ƒå˜é‡ç»•è¿‡æ£€æŸ¥ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰
                # æ³¨æ„ï¼šè¿™éœ€è¦ transformers æ”¯æŒ TRANSFORMERS_SAFE_LOADING_DISABLED ç¯å¢ƒå˜é‡
                old_val = os.environ.get("TRANSFORMERS_SAFE_LOADING_DISABLED", None)
                try:
                    # å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡æ¥ç¦ç”¨å®‰å…¨æ£€æŸ¥
                    os.environ["TRANSFORMERS_SAFE_LOADING_DISABLED"] = "1"
                    # é‡æ–°å°è¯•åŠ è½½
                    base_llm = AutoModelForCausalLM.from_pretrained(llm_dir, **kwargs)
                except Exception as e2:
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ safetensorsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    import glob
                    safetensors_files = glob.glob(os.path.join(llm_dir, "*.safetensors"))
                    if safetensors_files:
                        try:
                            # å°è¯•åªåŠ è½½ safetensors
                            base_llm = AutoModelForCausalLM.from_pretrained(
                                llm_dir, 
                                use_safetensors=True,
                                **kwargs
                            )
                        except Exception as e3:
                            raise RuntimeError(
                                f"æ— æ³•åŠ è½½æ¨¡å‹ï¼Œtorch ç‰ˆæœ¬é™åˆ¶ã€‚è¯·å‡çº§ torch >= 2.6 æˆ–ä½¿ç”¨ safetensors æ ¼å¼çš„æ¨¡å‹ã€‚"
                                f"åŸå§‹é”™è¯¯: {e}\nå°è¯•ç»•è¿‡å¤±è´¥: {e2}\nå°è¯• safetensors å¤±è´¥: {e3}"
                            ) from e3
                    else:
                        raise RuntimeError(
                            f"æ— æ³•åŠ è½½æ¨¡å‹ï¼Œtorch ç‰ˆæœ¬é™åˆ¶ã€‚è¯·å‡çº§ torch >= 2.6 æˆ–ä½¿ç”¨ safetensors æ ¼å¼çš„æ¨¡å‹ã€‚"
                            f"åŸå§‹é”™è¯¯: {e}\nå°è¯•ç»•è¿‡å¤±è´¥: {e2}"
                        ) from e2
                finally:
                    # æ¢å¤ç¯å¢ƒå˜é‡
                    if old_val is not None:
                        os.environ["TRANSFORMERS_SAFE_LOADING_DISABLED"] = old_val
                    elif "TRANSFORMERS_SAFE_LOADING_DISABLED" in os.environ:
                        del os.environ["TRANSFORMERS_SAFE_LOADING_DISABLED"]
            else:
                raise

        # 4) tokenizerï¼šè‹¥æœªä¼ å…¥ï¼Œåˆ™ç”¨æ ¹ç›®å½•ï¼ˆå› ä¸º tokenizer ä¿å­˜åœ¨æ ¹ï¼‰
        if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(root, use_fast=True)

        # 5) æ„é€ å®ä¾‹
        model = cls(llm=base_llm, tokenizer=tokenizer,
                    diffusion_config=diffusion_config,
                    diffusion_adapter_config=diffusion_adapter_config,
                    layer2_config=layer2_config,
                    use_layer2=use_layer2)

        # 6) åŠ è½½ extrasï¼ˆæŒ‰ metadata çš„ç›¸å¯¹è·¯å¾„ï¼‰
        def _maybe_load_sub(sd_path, module_attr):
            if not sd_path:
                return
            path = os.path.join(root, sd_path) if not os.path.isabs(sd_path) else sd_path
            if os.path.isfile(path):
                sd = torch.load(path, map_location="cpu")
                mod = getattr(model, module_attr, None)
                if mod is not None and hasattr(mod, "load_state_dict"):
                    # å…¼å®¹ç›´æ¥å­˜ state_dictï¼ˆkeys è£¸çš„ï¼‰æˆ–è€…å¸¦å‰ç¼€ï¼›ä½¿ç”¨ strict=False æ›´éŸ§æ€§
                    mod.load_state_dict(sd, strict=False)

        if has_meta:
            _maybe_load_sub(extras_map.get("gvp_encoder"), "gvp_encoder")
            _maybe_load_sub(extras_map.get("mol_adapter"), "mol_adapter")
            # æ³¨æ„ï¼šdiffusion_adapter å’Œæ—§çš„ diffusion å·²ç§»é™¤
            # ä¸å†åŠ è½½è¿™äº›ç»„ä»¶

        return model


    # --------------------------- å…¶å®ƒè¾…åŠ© ---------------------------
    def gradient_checkpointing_enable(self, *args, **kwargs):
        if self.config is not None:
            try:
                self.config.use_cache = False
            except Exception:
                pass
        if hasattr(self.llm, "gradient_checkpointing_enable"):
            try:
                return self.llm.gradient_checkpointing_enable(*args, **kwargs)
            except TypeError:
                return self.llm.gradient_checkpointing_enable()
        return None

    def gradient_checkpointing_disable(self):
        if hasattr(self.llm, "gradient_checkpointing_disable"):
            try:
                out = self.llm.gradient_checkpointing_disable()
            except TypeError:
                out = None
        else:
            out = None
        if self.config is not None:
            try:
                self.config.use_cache = True
            except Exception:
                pass
        return out

    @staticmethod
    def _storage_id(t: torch.Tensor):
        try:
            return t.untyped_storage().data_ptr()
        except Exception:
            return t.storage().data_ptr()
