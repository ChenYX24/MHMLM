"""
æ¨¡å‹åˆå§‹åŒ–æ¨¡å— (Optimized)
ç»Ÿä¸€å¤„ç†æ¨¡å‹ã€tokenizerã€GNNç­‰çš„åˆå§‹åŒ–
"""
import os
import json
import gc
import torch
import torch.nn as nn
from typing import Optional, Dict, Any, Union
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
from collections import OrderedDict

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
from .mol_aware_lm import MolAwareCausalLM


def clean_state_dict(state_dict: Dict[str, Any]) -> OrderedDict:
    """å·¥å…·å‡½æ•°ï¼šç§»é™¤ DDP äº§ç”Ÿçš„ 'module.' å‰ç¼€ï¼Œå¹¶ç¡®ä¿è¿”å› OrderedDict"""
    new_state_dict = OrderedDict()
    for k, v in state_dict.items():
        name = k[7:] if k.startswith("module.") else k
        new_state_dict[name] = v
    return new_state_dict


def init_tokenizer(llm_name: str, mol_token: str = "<mol>") -> AutoTokenizer:
    """
    åˆå§‹åŒ–tokenizer
    ä¼˜åŒ–ç‚¹ï¼šæ˜¾å¼è®¾ç½® padding_side='right'ï¼Œè¿™å¯¹ SFT è®­ç»ƒè‡³å…³é‡è¦
    """
    tokenizer = AutoTokenizer.from_pretrained(llm_name, use_fast=True)
    
    # 1. å¼ºåˆ¶è®¾ç½® pad_token (ä¿®å¤ SFTTrainer æŠ¥é”™çš„æ ¸å¿ƒ)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        
    # 2. å¼ºåˆ¶è®¾ç½® padding_side (é˜²æ­¢ç”Ÿæˆä»»åŠ¡å‡ºç°é”™ä½)
    tokenizer.padding_side = "right"
    
    # 3. æ·»åŠ ç‰¹æ®Štoken
    to_add = []
    current_vocab = tokenizer.get_vocab()
    if mol_token not in current_vocab:
        to_add.append(mol_token)
    
    # ä»…å¯¹ Llama ç³»åˆ—æ¨¡å‹è¡¥å…… Llama 3 é£æ ¼çš„ç‰¹æ®Š tokenï¼›
    # å¯¹ Qwen / Mistral ç­‰å…¶ä»–æ¨¡å‹ï¼Œä¸å¼ºè¡Œæ³¨å…¥è¿™äº› tokenï¼Œé¿å…ä¸å„è‡ªçš„ chat æ¨¡æ¿å†²çªã€‚
    llm_name_lower = llm_name.lower()
    if "llama" in llm_name_lower:
        special_tokens = ["<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>"]
        for t in special_tokens:
            if t not in current_vocab:
                to_add.append(t)
            
    if to_add:
        tokenizer.add_special_tokens({"additional_special_tokens": to_add})
    
    return tokenizer


def init_llm(llm_name: str, tokenizer: AutoTokenizer, bf16: bool = True, device: str = "cuda:0") -> AutoModelForCausalLM:
    """
    åˆå§‹åŒ–LLM
    
    å¦‚æœç»™å®šçš„è·¯å¾„æ˜¯ checkpoint è·¯å¾„ï¼ˆåŒ…å« pytorch_model.bin æˆ– model.safetensorsï¼‰ï¼Œ
    ä½† llm/ å­ç›®å½•ä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨ split_llm_extras.py è¿›è¡Œæ‹†åˆ†
    """
    import os
    from pathlib import Path
    
    # æ£€æŸ¥ torch ç‰ˆæœ¬
    torch_version = tuple(map(int, torch.__version__.split('.')[:2]))
    requires_torch_26 = torch_version < (2, 6)
    
    # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦æœ‰ safetensors æ–‡ä»¶
    model_path = Path(llm_name)
    
    # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯ checkpoint è·¯å¾„éœ€è¦æ‹†åˆ†
    if not model_path.exists():
        # æ£€æŸ¥æ˜¯å¦æ˜¯ checkpoint/llm è·¯å¾„ï¼Œä½†çˆ¶ç›®å½•å­˜åœ¨ä¸”åŒ…å«æ··åˆæƒé‡
        parent_path = model_path.parent
        if parent_path.exists():
            bin_path = parent_path / "pytorch_model.bin"
            safetensors_path = parent_path / "model.safetensors"
            if bin_path.exists() or safetensors_path.exists():
                print(f"ğŸ“¦ æ£€æµ‹åˆ° checkpoint è·¯å¾„ä½† llm å­ç›®å½•ä¸å­˜åœ¨: {llm_name}")
                print(f"   å°è¯•æ‹†åˆ†çˆ¶ç›®å½•: {parent_path}")
                try:
                    split_script_path = Path(__file__).parent.parent / "scripts" / "ckpt" / "split_llm_extras.py"
                    if split_script_path.exists():
                        import importlib.util
                        spec = importlib.util.spec_from_file_location("split_llm_extras", split_script_path)
                        split_module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(split_module)
                        success = split_module.split_checkpoint(str(parent_path), str(parent_path))
                        if success:
                            print(f"âœ… Checkpoint æ‹†åˆ†å®Œæˆï¼Œé‡æ–°æ£€æŸ¥è·¯å¾„: {llm_name}")
                            model_path = Path(llm_name)  # é‡æ–°èµ‹å€¼
                except Exception as e:
                    print(f"âš ï¸ è‡ªåŠ¨æ‹†åˆ†å¤±è´¥: {e}")
    has_safetensors = False
    has_bin_files = False
    if model_path.exists():
        safetensors_files = list(model_path.glob("*.safetensors")) + list(model_path.glob("model*.safetensors"))
        has_safetensors = len(safetensors_files) > 0
        bin_files = list(model_path.glob("*.bin")) + list(model_path.glob("pytorch_model*.bin"))
        has_bin_files = len(bin_files) > 0
    
    # å¦‚æœ torch < 2.6 ä¸”åªæœ‰ .bin æ–‡ä»¶ï¼Œå°è¯•åœ¨ CPU ä¸Šè‡ªåŠ¨è½¬æ¢ä¸º safetensorsï¼ˆä¸´æ—¶å…³é—­å®‰å…¨æ£€æŸ¥ï¼‰
    if requires_torch_26 and not has_safetensors and has_bin_files:
        import warnings
        warnings.warn(
            f"âš ï¸  Torch version {torch.__version__} < 2.6, model has only .bin files. "
            f"Trying CPU-side auto-conversion to safetensors to bypass the security check."
        )
        try:
            print(f"[Model Init] Converting {llm_name} to safetensors (CPU, dtype={'bf16' if bf16 else 'fp32'})...")
            from transformers import AutoConfig
            # ä¸´æ—¶å…³é—­ transformers å®‰å…¨æ£€æŸ¥ï¼Œå…è®¸åŠ è½½ .bin
            os.environ["TRANSFORMERS_SAFE_LOADING_DISABLED"] = "1"
            # é€‰æ‹©å•ä¸€ bin æ–‡ä»¶ï¼›è‹¥å­˜åœ¨åˆ†ç‰‡ indexï¼Œæç¤ºæ‰‹åŠ¨å¤„ç†
            index_files = list(model_path.glob("pytorch_model*.bin.index.json"))
            if index_files:
                raise RuntimeError(
                    "Model appears to be sharded (.bin.index.json found); please convert manually "
                    "or upgrade torch>=2.6 to load sharded .bin safely."
                )
            # å–ç¬¬ä¸€ä¸ª bin æ–‡ä»¶
            bin_file = sorted(list(model_path.glob("*.bin")) + list(model_path.glob("pytorch_model*.bin")))[0]
            print(f"[Model Init] Loading bin weights from: {bin_file.name}")
            state = torch.load(bin_file, map_location="cpu")
            # å…¼å®¹ state_dict åŒ…è£¹
            if isinstance(state, dict) and "state_dict" in state:
                state = state["state_dict"]
            # åŠ è½½ config å¹¶æ„å»ºæ¨¡å‹
            config = AutoConfig.from_pretrained(llm_name)
            temp_model = AutoModelForCausalLM.from_config(config)
            missing, unexpected = temp_model.load_state_dict(state, strict=False)
            if missing or unexpected:
                print(f"[Model Init] load_state_dict: missing={len(missing)}, unexpected={len(unexpected)}")
            # ä¿å­˜ä¸º safetensors
            temp_model.save_pretrained(
                llm_name,
                safe_serialization=True,
                max_shard_size="5GB"
            )
            del temp_model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print(f"[Model Init] âœ… Conversion completed. Safetensors files saved.")
            has_safetensors = True
            # æ¢å¤é»˜è®¤è®¾ç½®
            os.environ.pop("TRANSFORMERS_SAFE_LOADING_DISABLED", None)
        except Exception as conv_e:
            import traceback
            traceback.print_exc()
            raise RuntimeError(
                f"Failed to auto-convert model to safetensors: {conv_e}\n"
                f"Please manually convert or upgrade torch to >= 2.6"
            ) from conv_e
    elif requires_torch_26 and not has_safetensors:
        import warnings
        warnings.warn(
            f"âš ï¸  Torch version {torch.__version__} < 2.6, and model has no safetensors files. "
            f"Transformers requires torch>=2.6 to load .bin files due to security (CVE-2025-32434).\n"
            f"Solutions:\n"
            f"  1. Upgrade torch: pip install torch>=2.6\n"
            f"  2. Convert model to safetensors format\n"
            f"  3. Downgrade transformers to a version before this check"
        )
    
    # ä½¿ç”¨ low_cpu_mem_usage=True åŠ é€ŸåŠ è½½å¹¶å‡å°‘å†…å­˜å ç”¨
    # ä¼˜å…ˆå°è¯• safetensorsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        llm = AutoModelForCausalLM.from_pretrained(
            llm_name,
            dtype=torch.bfloat16 if bf16 else torch.float32,  # ä½¿ç”¨ dtype æ›¿ä»£å·²å¼ƒç”¨çš„ torch_dtype
            low_cpu_mem_usage=True,
            use_safetensors=True if has_safetensors else None,  # å¦‚æœå­˜åœ¨ safetensors åˆ™ä¼˜å…ˆä½¿ç”¨
            device_map=None,  # æ‰‹åŠ¨æ§åˆ¶ to(device)
            trust_remote_code=True
        ).to(device)
    except Exception as e:
        # å¦‚æœ safetensors åŠ è½½å¤±è´¥ï¼Œå°è¯• .binï¼ˆéœ€è¦ torch >= 2.6ï¼‰
        if "safetensors" in str(e).lower() or "use_safetensors" in str(e).lower():
            if requires_torch_26:
                raise RuntimeError(
                    f"Failed to load model: {e}\n"
                    f"Your torch version ({torch.__version__}) is < 2.6, which is required to load .bin files.\n"
                    f"Please upgrade torch: pip install 'torch>=2.6'"
                ) from e
            # torch >= 2.6ï¼Œå¯ä»¥å°è¯• .bin
            llm = AutoModelForCausalLM.from_pretrained(
                llm_name,
                dtype=torch.bfloat16 if bf16 else torch.float32,
                low_cpu_mem_usage=True,
                use_safetensors=False,  # å¼ºåˆ¶ä½¿ç”¨ .bin
                device_map=None,
                trust_remote_code=True
            ).to(device)
        else:
            raise
    
    # è°ƒæ•´vocab size
    old_vocab_size = llm.get_input_embeddings().weight.shape[0]
    new_vocab_size = len(tokenizer)
    
    if old_vocab_size != new_vocab_size:
        # åªåœ¨ rank 0 æ‰“å°ä¸€æ¬¡
        import torch.distributed as dist
        if not dist.is_initialized() or dist.get_rank() == 0:
            print(f"[Model Init] Resizing token embeddings: {old_vocab_size} -> {new_vocab_size}")
        
        # mean_resizing=True ä½¿ç”¨æ—§ embedding çš„å‡å€¼åˆå§‹åŒ–æ–° tokenï¼Œæ¯”éšæœºåˆå§‹åŒ–æ”¶æ•›æ›´å¿«
        llm.resize_token_embeddings(new_vocab_size, mean_resizing=True)
        
    # åŒæ­¥é…ç½®
    llm.config.vocab_size = len(tokenizer)
    llm.config.pad_token_id = tokenizer.pad_token_id
    llm.config.eos_token_id = tokenizer.eos_token_id
    llm.config.bos_token_id = tokenizer.bos_token_id
    
    return llm


def init_model(
    cfg: Dict[str, Any],
    tokenizer: AutoTokenizer,
    llm: AutoModelForCausalLM,
    device: str = "cuda:0",
) -> MolAwareCausalLM:
    """åˆå§‹åŒ–MolAwareCausalLMæ¨¡å‹"""
    mol_token = cfg.get("tokens", {}).get("mol_token", "<mol>")
    train_conf = cfg.get("train", {}) or {}

    # ===== Diffusion å¼€å…³ =====
    # æ–°å¢ï¼šå¦‚æœ cfg["train"]["use_diffusion"] ä¸º Falseï¼Œåˆ™å®Œå…¨ç¦ç”¨ diffusionï¼Œ
    # ä¸å†åˆå§‹åŒ–ä»»ä½• diffusion/diffusion_adapter ç›¸å…³æ¨¡å—ï¼Œå‡å°æ˜¾å­˜å ç”¨ã€‚
    use_diffusion = train_conf.get("use_diffusion", True)

    if use_diffusion:
        diffusion_conf = cfg.get("diffusion", {}) or {}
        diff_conf = diffusion_conf.get("diffusion", {}) or {}
        diff_adp_conf = diffusion_conf.get("adapter", {}) or {}
    else:
        diffusion_conf = {}
        diff_conf = {}
        diff_adp_conf = {}
    
    # --- ä¼˜åŒ–è®¾å¤‡åˆ†é…é€»è¾‘ ---
    # å…è®¸é€šè¿‡ env æˆ– config çµæ´»åˆ†é… diffusion æ¨¡å‹ä½ç½®ï¼Œå‡è½»ä¸»å¡æ˜¾å­˜å‹åŠ›
    if diff_conf:
        diffusion_device = diff_conf.get("device")
        if not diffusion_device or diffusion_device == "cuda:0":
            env_device = os.environ.get("DIFFUSION_DEVICE")
            if env_device:
                diffusion_device = env_device
            elif device.startswith("cuda:"):
                # è‡ªåŠ¨å°è¯•åˆ†é…åˆ°ä¸‹ä¸€å¼ å¡
                try:
                    curr_id = int(device.split(":")[-1])
                    if torch.cuda.device_count() > curr_id + 1:
                        diffusion_device = f"cuda:{curr_id + 1}"
                    else:
                        diffusion_device = device
                except Exception:
                    diffusion_device = device
            else:
                diffusion_device = device
        diff_conf["device"] = diffusion_device
        if diffusion_device != device:
            print(f"ğŸ“Œ Diffusion model placed on {diffusion_device} (Main LLM on {device})")

    # æ£€æŸ¥æ˜¯å¦ç¦ç”¨ GNNï¼ˆå½“ use_offline_spans=False æ—¶ï¼Œå®Œå…¨ä¸èµ° GNN è·¯å¾„ï¼‰
    use_offline_spans = cfg.get("train", {}).get("use_offline_spans", False)
    disable_gnn = not use_offline_spans  # å¦‚æœ use_offline_spans=Falseï¼Œåˆ™ç¦ç”¨ GNN
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ LDMolï¼ˆé˜¶æ®µ1 SFT è®­ç»ƒæ—¶å¯ç¦ç”¨ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
    # å¦‚æœ use_diffusion=Falseï¼Œé»˜è®¤ä¹Ÿç¦ç”¨ LDMolï¼ˆå› ä¸º LDMol ä¾èµ– diffusionï¼‰
    use_ldmol = train_conf.get("use_ldmol", use_diffusion)
    # æ˜¯å¦è·³è¿‡ LDMol å†…ç½®çš„ text_encoderï¼ˆè”åˆæ¨ç†æ—¶å¤ç”¨ä¸» Qwen ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
    ldmol_skip_text_encoder = train_conf.get("ldmol_skip_text_encoder", False)
    
    if not use_ldmol:
        print("ğŸ“Œ LDMol disabled (use_ldmol=False)")
    elif ldmol_skip_text_encoder:
        print("ğŸ“Œ LDMol text_encoder skipped (ldmol_skip_text_encoder=True, will reuse main Qwen)")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Layer2ï¼ˆç”¨äºååº”äº§ç‡é¢„æµ‹ï¼‰
    use_layer2 = train_conf.get("use_layer2", False)
    layer2_config = cfg.get("layer2", {}) or {}
    
    if use_layer2:
        print("ğŸ“Œ Layer2 enabled (use_layer2=True)")
    else:
        print("ğŸ“Œ Layer2 disabled (use_layer2=False)")
    
    # åˆå§‹åŒ–ä¸»æ¨¡å‹
    model = MolAwareCausalLM(
        llm=llm,
        tokenizer=tokenizer,
        mol_token=mol_token,
        proxy=cfg.get("network", {}).get("proxy"),
        debug=False,
        diffusion_config=diff_conf,
        diffusion_adapter_config=diff_adp_conf,
        disable_gnn=disable_gnn,  # ä¼ é€’ç¦ç”¨ GNN æ ‡å¿—
        use_ldmol=use_ldmol,  # æ˜¯å¦ä½¿ç”¨ LDMol
        ldmol_skip_text_encoder=ldmol_skip_text_encoder,  # æ˜¯å¦è·³è¿‡ LDMol å†…ç½® text_encoder
        layer2_config=layer2_config,  # Layer2 é…ç½®
        use_layer2=use_layer2,  # æ˜¯å¦ä½¿ç”¨ Layer2
    ).to(device)
    
    # --- æƒé‡åŠ è½½é€»è¾‘ä¼˜åŒ– ---
    checkpoint_dir = cfg.get("paths", {}).get("checkpoint_dir")
    
    # 1. ä¼˜å…ˆä» Checkpoint ç›®å½•æ•´å¥—åŠ è½½
    if checkpoint_dir:
        checkpoint_path = Path(checkpoint_dir)
        if checkpoint_path.exists():
            print(f"ğŸ“‚ Loading weights from checkpoint: {checkpoint_dir}")
            load_model_weights_from_checkpoint_dir(model, checkpoint_dir, device)
        else:
            print(f"âš ï¸ Checkpoint ç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
            print(f"   è·³è¿‡ checkpoint åŠ è½½ï¼Œä½¿ç”¨é»˜è®¤åˆå§‹åŒ–")
    
    # 2. å¦åˆ™ä»åˆ†æ•£è·¯å¾„åŠ è½½ (Legacy / Fine-grained control)
    else:
        # GNN
        gnn_path = cfg.get("paths", {}).get("gnn_state_dict_path")
        if gnn_path and os.path.exists(gnn_path):
            try:
                sd = torch.load(gnn_path, map_location="cpu")
                sd = sd.get("model_state_dict", sd)
                model.gvp_encoder.load_state_dict(clean_state_dict(sd), strict=False)
                print(f"âœ… Loaded GVPEncoder from {gnn_path}")
            except Exception as e:
                print(f"âš ï¸ Load GVP failed: {e}")
        
        # å…¶ä»–é€‚é…å™¨
        load_additional_weights(model, cfg, device)
    
    # åˆå§‹åŒ– GNN ä»»åŠ¡å¤´ (å¦‚æœéœ€è¦)
    use_gnn_tasks = cfg.get("train", {}).get("use_gnn_tasks", False)
    if use_gnn_tasks or (checkpoint_dir and os.path.exists(checkpoint_dir)):
        init_gnn_task_heads(model, cfg, device)
    
    # åº”ç”¨å†»ç»“ç­–ç•¥
    apply_freeze_config(model, cfg)
    
    # æ˜¾å­˜æ¸…ç†
    torch.cuda.empty_cache()
    
    return model


def init_gnn_task_heads(model: MolAwareCausalLM, cfg: Dict[str, Any], device: str):
    """åˆå§‹åŒ–GNNä»»åŠ¡å¤´"""
    if not hasattr(model, "gvp_encoder") or model.gvp_encoder is None:
        return
    
    try:
        train_cfg = cfg.get("train", {})
        model.gvp_encoder.init_task_heads(
            num_reg_tasks=train_cfg.get("gnn_num_reg_tasks", 5),
            num_cls_tasks=train_cfg.get("gnn_num_cls_tasks", 1),
            head_hidden_dim=train_cfg.get("gnn_head_hidden_dim", None),
            head_dropout=float(train_cfg.get("gnn_head_dropout", 0.1)),
        )
        # ç¡®ä¿æ–°åˆå§‹åŒ–çš„å¤´åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        model.gvp_encoder.to(device) 
        print(f"âœ… GNN Task Heads initialized.")
    except Exception as e:
        print(f"âš ï¸ GVP head init failed: {e}")


def load_model_weights_from_checkpoint_dir(model: MolAwareCausalLM, ckpt_dir: str, device: str):
    """
    ä»checkpointç›®å½•åŠ è½½æƒé‡
    ä¼˜åŒ–ç‚¹ï¼šå¤§å¹…ä¼˜åŒ–å¤§æ¨¡å‹åˆ†ç‰‡åŠ è½½çš„å†…å­˜å ç”¨
    
    å¦‚æœ checkpoint ç›®å½•å­˜åœ¨ä½†æ²¡æœ‰ llm/ å­ç›®å½•ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨ split_llm_extras.py è¿›è¡Œæ‹†åˆ†
    """
    ckpt_dir = Path(ckpt_dir)
    
    if not ckpt_dir.exists():
        print(f"âŒ Checkpoint ç›®å½•ä¸å­˜åœ¨: {ckpt_dir}")
        return
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‹†åˆ† checkpoint
    llm_dir = ckpt_dir / "llm"
    
    # å¦‚æœ llm ç›®å½•ä¸å­˜åœ¨ï¼Œä½† checkpoint ç›®å½•ä¸‹æœ‰ pytorch_model.bin æˆ– model.safetensorsï¼Œåˆ™éœ€è¦æ‹†åˆ†
    if not llm_dir.exists():
        bin_path = ckpt_dir / "pytorch_model.bin"
        safetensors_path = ckpt_dir / "model.safetensors"
        if bin_path.exists() or safetensors_path.exists():
            print(f"ğŸ“¦ æ£€æµ‹åˆ°æ··åˆ checkpointï¼Œéœ€è¦æ‹†åˆ†: {ckpt_dir}")
            print(f"   è°ƒç”¨ split_llm_extras.py è¿›è¡Œè‡ªåŠ¨æ‹†åˆ†...")
            try:
                # å¯¼å…¥æ‹†åˆ†å‡½æ•°
                split_script_path = Path(__file__).parent.parent / "split_llm_extras.py"
                if split_script_path.exists():
                    # åŠ¨æ€å¯¼å…¥
                    import importlib.util
                    spec = importlib.util.spec_from_file_location("split_llm_extras", split_script_path)
                    split_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(split_module)
                    
                    # è°ƒç”¨æ‹†åˆ†å‡½æ•°
                    success = split_module.split_checkpoint(str(ckpt_dir), str(ckpt_dir))
                    if success:
                        print(f"âœ… Checkpoint æ‹†åˆ†å®Œæˆ")
                        # é‡æ–°æ£€æŸ¥ llm_dirï¼ˆæ‹†åˆ†ååº”è¯¥å­˜åœ¨äº†ï¼‰
                        llm_dir = ckpt_dir / "llm"
                    else:
                        print(f"âš ï¸ Checkpoint æ‹†åˆ†å¤±è´¥ï¼Œå°è¯•ç»§ç»­åŠ è½½...")
                else:
                    print(f"âš ï¸ æ‰¾ä¸åˆ° split_llm_extras.py: {split_script_path}")
            except Exception as e:
                import traceback
                print(f"âš ï¸ è‡ªåŠ¨æ‹†åˆ†å¤±è´¥: {e}")
                traceback.print_exc()
                print(f"   è¯·æ‰‹åŠ¨è¿è¡Œ: python {split_script_path} æˆ–æ£€æŸ¥ checkpoint è·¯å¾„")
    
    # 1. åŠ è½½ LLM æƒé‡
    if llm_dir.exists():
        try:
            # ä¼˜å…ˆä½¿ç”¨ from_pretrained åŠ è½½ï¼Œå› ä¸ºå®ƒå†…éƒ¨å¤„ç†äº†åˆ†ç‰‡åŠ è½½çš„å†…å­˜ç®¡ç† (low_cpu_mem_usage)
            # æ¯”æ‰‹åŠ¨åˆå¹¶ state_dict æ›´å®‰å…¨ã€æ›´çœå†…å­˜
            print(f"â³ Loading LLM via from_pretrained (memory efficient)...")
            # è¿™é‡Œæˆ‘ä»¬åŠ è½½åˆ°ä¸€ä¸ªä¸´æ—¶æ¨¡å‹ï¼Œç„¶åæå– state_dictï¼Œæˆ–è€…ç›´æ¥è®© model.llm é‡æ–°åŠ è½½
            # ä¸ºäº†æœ€ç¨³å¦¥ï¼Œæˆ‘ä»¬ç›´æ¥è®© model.llm è°ƒç”¨ HF çš„åŠ è½½é€»è¾‘
            # æ³¨æ„ï¼šè¿™éœ€è¦ model.llm æ˜¯æ ‡å‡†çš„ HF æ¨¡å‹å®ä¾‹
            
            # æ–¹æ¡ˆ A: å¦‚æœåªæ˜¯ä¸ºäº†åŠ è½½æƒé‡ï¼Œç”¨ load_state_dict é…åˆ safetensors æœ€å¥½
            # æ–¹æ¡ˆ B (æ¨è): ä½¿ç”¨ transformers çš„ load_sharded_checkpoint å·¥å…·
            from transformers.modeling_utils import load_sharded_checkpoint
            
            # æ£€æŸ¥æ˜¯ safetensors è¿˜æ˜¯ bin
            safetensors_index = llm_dir / "model.safetensors.index.json"
            safetensors_file = llm_dir / "model.safetensors"
            is_safetensors = safetensors_index.exists() or safetensors_file.exists()

            if is_safetensors:
                from safetensors.torch import load_file
                if safetensors_file.exists():
                    # å•æ–‡ä»¶
                    sd = load_file(str(safetensors_file), device="cpu")
                    model.llm.load_state_dict(sd, strict=False)
                else:
                    # åˆ†ç‰‡ safetensors (HF åŸç”Ÿæ”¯æŒè‡ªåŠ¨å¤„ç†)
                    # é‡æ–°åŠ è½½ä¸€éå¯èƒ½æ˜¯æœ€é«˜æ•ˆçš„ï¼Œå› ä¸ºæ‰‹åŠ¨å¤„ç†åˆ†ç‰‡é€»è¾‘å¾ˆå¤æ‚
                    load_sharded_checkpoint(model.llm, str(llm_dir), strict=False, prefer_safe=True)
            else:
                # PyTorch Bin
                bin_file = llm_dir / "pytorch_model.bin"
                bin_index = llm_dir / "pytorch_model.bin.index.json"
                if bin_file.exists():
                    sd = torch.load(str(bin_file), map_location="cpu")
                    model.llm.load_state_dict(sd, strict=False)
                elif bin_index.exists():
                    load_sharded_checkpoint(model.llm, str(llm_dir), strict=False, prefer_safe=False)
            
            print(f"âœ… Loaded LLM weights from {llm_dir}")
            
        except Exception as e:
            print(f"âš ï¸ Optimized LLM load failed: {e}, falling back to legacy...")
            # Fallback (ä½ åŸæ¥çš„é€»è¾‘ï¼Œè™½ç„¶æ…¢ä½†èƒ½ç”¨)
            pass 

    # 2. åŠ è½½ Extras (ä¿æŒä¸å˜ï¼Œä½†ä½¿ç”¨ clean_state_dict)
    extras_dir = ckpt_dir / "extras"
    if extras_dir.exists():
        def _load_component(name, filename):
            path = extras_dir / filename
            comp = getattr(model, name, None)
            if path.exists() and comp is not None:
                try:
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    file_size = path.stat().st_size
                    if file_size == 0:
                        print(f"âš ï¸ Skipping {name}: checkpoint file {filename} is empty (0 bytes)")
                        return
                    
                    sd = torch.load(path, map_location="cpu")
                    if isinstance(sd, dict) and "model_state_dict" in sd: sd = sd["model_state_dict"]
                    elif isinstance(sd, dict) and "state_dict" in sd: sd = sd["state_dict"]
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæƒé‡ï¼ˆå½¢çŠ¶ä¸º [0]ï¼‰
                    empty_keys = []
                    for k, v in sd.items():
                        if isinstance(v, torch.Tensor) and v.numel() == 0:
                            empty_keys.append(k)
                    
                    if empty_keys:
                        print(f"âš ï¸ Skipping {name}: checkpoint contains {len(empty_keys)} empty weights (shape [0])")
                        print(f"   ç¤ºä¾‹: {empty_keys[:3]}...")
                        return
                    
                    comp.load_state_dict(clean_state_dict(sd), strict=False)
                    print(f"âœ… Loaded {name} from {filename}")
                except Exception as e:
                    print(f"âš ï¸ Failed to load {name}: {e}")

        _load_component("gvp_encoder", "gvp_encoder.pt")
        _load_component("mol_adapter", "mol_adapter.pt")
        _load_component("diffusion_adapter", "diffusion_adapter.pt")
        
    # 3. é‡Šæ”¾ CPU å†…å­˜
    gc.collect()
    torch.cuda.empty_cache()


def load_additional_weights(model: MolAwareCausalLM, cfg: Dict[str, Any], device: str):
    """åŠ è½½é¢å¤–çš„æƒé‡ (Unified)"""
    paths = cfg.get("paths", {})
    
    def _load_single(path_key, model_attr, label):
        path = paths.get(path_key)
        comp = getattr(model, model_attr, None)
        if path and os.path.exists(path) and comp is not None:
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = Path(path).stat().st_size
                if file_size == 0:
                    print(f"âš ï¸ Skipping {label}: checkpoint file is empty (0 bytes): {path}")
                    return
                
                sd = torch.load(path, map_location="cpu")
                if isinstance(sd, dict) and "state_dict" in sd: sd = sd["state_dict"]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæƒé‡
                empty_keys = [k for k, v in sd.items() if isinstance(v, torch.Tensor) and v.numel() == 0]
                if empty_keys:
                    print(f"âš ï¸ Skipping {label}: checkpoint contains {len(empty_keys)} empty weights (shape [0])")
                    print(f"   æ–‡ä»¶: {path}")
                    return
                
                comp.load_state_dict(clean_state_dict(sd), strict=False)
                print(f"âœ… Loaded {label} from {path}")
            except Exception as e:
                print(f"âš ï¸ Failed to load {label}: {e}")

    _load_single("gnn_mlp_state_dict_path", "mol_adapter", "mol_adapter")
    _load_single("diffusion_adapter_state_dict_path", "diffusion_adapter", "diffusion_adapter")


def apply_freeze_config(model: MolAwareCausalLM, cfg: Dict[str, Any]):
    """åº”ç”¨å†»ç»“é…ç½®"""
    train_cfg = cfg.get("train", {})
    
    # è¾…åŠ©å‡½æ•°ï¼šå†»ç»“æ¨¡å—
    def _freeze(module, name):
        for p in module.parameters():
            p.requires_grad = False
        print(f"ğŸ”’ Frozen {name}")

    if train_cfg.get("freeze_llm", False):
        for n, p in model.llm.named_parameters():
            if 'embed_tokens' not in n: # é€šå¸¸ä¿ç•™ embedding è®­ç»ƒä»¥é€‚åº”æ–°token
                p.requires_grad = False
        print("ğŸ”’ Frozen LLM (except embed_tokens)")

    if train_cfg.get("freeze_gnn", False) and getattr(model, "gvp_encoder", None):
        _freeze(model.gvp_encoder, "GVP Encoder")

    if train_cfg.get("freeze_mol_adapter", False) and getattr(model, "mol_adapter", None):
        _freeze(model.mol_adapter, "Mol Adapter")

    if train_cfg.get("freeze_diffusion", True) and getattr(model, "diffusion", None):
        _freeze(model.diffusion, "Diffusion Model")
        
    if train_cfg.get("freeze_diffusion_adapter", True) and getattr(model, "diffusion_adapter", None):
        _freeze(model.diffusion_adapter, "Diffusion Adapter")


def init_offline_token_classifier(
    llm: AutoModelForCausalLM,
    mlp_token_classifier_path: Optional[str],
    device: str = "cuda:0",
) -> Optional[nn.Module]:
    """åˆå§‹åŒ–ç¦»çº¿tokenåˆ†ç±»å™¨"""
    if not mlp_token_classifier_path:
        print(f"âš ï¸ mlp_token_classifier_path is not set in config")
        return None
    
    if not os.path.exists(mlp_token_classifier_path):
        print(f"âš ï¸ Token classifier file not found: {mlp_token_classifier_path}")
        return None
    
    try:
        print(f"ğŸ“‚ Loading token classifier from: {mlp_token_classifier_path}")
        # è¿™é‡Œçš„ç»“æ„éœ€è¦ä¸è®­ç»ƒåˆ†ç±»å™¨æ—¶çš„ç»“æ„ä¸€è‡´
        # å¦‚æœèƒ½ä» config è¯»å–æ›´å¥½ï¼Œç›®å‰ä¿æŒé»˜è®¤
        hidden_size = llm.config.hidden_size
        print(f"   Current model hidden_size: {hidden_size}")
        
        # å…ˆæ£€æŸ¥ checkpoint ä¸­çš„ hidden_size
        ckpt_for_check = torch.load(mlp_token_classifier_path, map_location="cpu")
        if isinstance(ckpt_for_check, dict):
            if "state_dict" in ckpt_for_check:
                ckpt_sd = ckpt_for_check["state_dict"]
            elif "model_state_dict" in ckpt_for_check:
                ckpt_sd = ckpt_for_check["model_state_dict"]
            else:
                ckpt_sd = ckpt_for_check
            
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª Linear å±‚çš„æƒé‡æ¥ç¡®å®šåŸå§‹ hidden_size
            for key, value in ckpt_sd.items():
                # ç§»é™¤å¯èƒ½çš„ prefix
                clean_key = key.replace("classifier.", "").replace("token_classifier.", "").replace("module.", "")
                if "weight" in clean_key and len(value.shape) == 2:
                    # ç¬¬ä¸€ä¸ª Linear å±‚çš„è¾“å…¥ç»´åº¦å°±æ˜¯ hidden_size
                    ckpt_hidden_size = value.shape[1]
                    print(f"   Checkpoint hidden_size: {ckpt_hidden_size} (from weight shape: {value.shape})")
                    if ckpt_hidden_size != hidden_size:
                        print(f"   âš ï¸ WARNING: Hidden size mismatch! Checkpoint was trained with {ckpt_hidden_size}, "
                              f"but current model has {hidden_size}. This classifier cannot be used.")
                        return None
                    break
        
        token_head = nn.Sequential(
            nn.Linear(hidden_size, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 2)
        ).to(device)
        
        print(f"   Loading checkpoint...")
        # é‡ç”¨ä¹‹å‰åŠ è½½çš„ checkpointï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        ckpt = ckpt_for_check
        
        # æ£€æŸ¥ checkpoint çš„ç»“æ„
        if isinstance(ckpt, dict):
            print(f"   Checkpoint keys: {list(ckpt.keys())[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªkey
            if "state_dict" in ckpt:
                ckpt = ckpt["state_dict"]
            elif "model_state_dict" in ckpt:
                ckpt = ckpt["model_state_dict"]
        
        # ä½¿ç”¨ clean_state_dict ç§»é™¤ potential module. å‰ç¼€ï¼Œå¹¶è¿‡æ»¤ key
        clean_sd = clean_state_dict(ckpt)
        print(f"   Cleaned state_dict keys (first 10): {list(clean_sd.keys())[:10]}...")
        
        final_sd = OrderedDict()
        for k, v in clean_sd.items():
            # å°è¯•å¤šç§å¯èƒ½çš„ key æ ¼å¼
            if k.startswith("classifier."):
                final_sd[k.replace("classifier.", "")] = v
            elif k.startswith("token_classifier."):
                final_sd[k.replace("token_classifier.", "")] = v
            elif not k.startswith("module.") and not "." in k or k.count(".") <= 1:
                # å¦‚æœ key çœ‹èµ·æ¥åƒæ˜¯åˆ†ç±»å™¨çš„å‚æ•°ï¼ˆæ²¡æœ‰å¤ªå¤šå±‚çº§ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                final_sd[k] = v
        
        print(f"   Final state_dict keys: {list(final_sd.keys())}")
        
        if len(final_sd) == 0:
            print(f"âš ï¸ No matching keys found in checkpoint. Available keys: {list(clean_sd.keys())[:20]}...")
            return None
        
        # å°è¯•åŠ è½½ï¼Œå¦‚æœ strict=False å¤±è´¥ï¼Œå°è¯• strict=False
        try:
            token_head.load_state_dict(final_sd, strict=True)
            print(f"   âœ… Loaded with strict=True")
        except Exception as e:
            print(f"   âš ï¸ Strict loading failed: {e}, trying strict=False...")
            missing_keys, unexpected_keys = token_head.load_state_dict(final_sd, strict=False)
            if missing_keys:
                print(f"   âš ï¸ Missing keys: {missing_keys[:5]}...")
            if unexpected_keys:
                print(f"   âš ï¸ Unexpected keys: {unexpected_keys[:5]}...")
        
        token_head.eval()
        
        # å½»åº•å†»ç»“
        for p in token_head.parameters():
            p.requires_grad = False
        
        print(f"âœ… Loaded offline token classifier successfully")
        return token_head
    except Exception as e:
        print(f"âŒ Failed to load token classifier: {e}")
        import traceback
        traceback.print_exc()
        return None