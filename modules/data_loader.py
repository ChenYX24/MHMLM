"""
æ•°æ®åŠ è½½æ¨¡å—
ç»Ÿä¸€å¤„ç†æ•°æ®åŠ è½½å’Œæ ¼å¼åŒ–
"""
import os
import json
import re
import hashlib
from typing import Optional, List, Dict, Any, Callable, Tuple
from datasets import load_dataset, Dataset
import torch


def safe_to_str(x):
    """å®‰å…¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
    if x is None:
        return ""
    if isinstance(x, (list, tuple)):
        return "\n".join(safe_to_str(xx) for xx in x)
    if isinstance(x, dict):
        return json.dumps(x, ensure_ascii=False)
    return str(x)

WORD_BOUNDARY_CHARS = set(" \n\t.,;:!?()[]{}")

# ==== æ–°å¢ï¼šmol span å¤„ç†ç›¸å…³çš„é€šç”¨å·¥å…· ====

# â€œå•è¯è¾¹ç•Œâ€å­—ç¬¦ï¼š
#   - è¿™äº›å­—ç¬¦ä¼šé˜»æ­¢ span å‘å·¦/å‘å³ç»§ç»­æ‰©å±•
#   - æ³¨æ„ï¼šè¿™é‡Œç‰¹æ„ **ä¸æŠŠ '.'ã€'['ã€']' æ”¾è¿›å»**ï¼Œ
#     æ–¹ä¾¿æŠŠ [Na+].[Cl-]ã€CCO.CS(=O)C è¿™ç§ç¦»å­/æ··åˆç‰©æ‰©æˆä¸€æ•´å—
_MOL_BOUNDARY_CHARS = " \n\t,;:!?{}"
# å»æ‰ [] å’Œ {}ï¼Œé¿å…æŠŠ [Na+].[Cl-] é‡Œçš„æ–¹æ‹¬å·å‰ªæ‰
_MOL_TRIM_CHARS = "'\"`â€œâ€â€˜â€™()"

_MOL_STOPWORDS = {"smiles", "Smiles", "SMILES", "logP", "NSAIDs"}

def _looks_like_molecule(span_text: str) -> bool:
    """
    è½¯è§„åˆ™åˆ¤æ–­ä¸€ä¸ª span çœ‹èµ·æ¥åƒâ€œåˆ†å­ç›¸å…³å®ä½“â€ï¼š
    - å¾ˆçŸ­çš„ç¢ç‰‡ï¼ˆé•¿åº¦ < 2ï¼‰ç›´æ¥ä¸¢æ‰
    - å«æœ‰æ•°å­— or å…¸å‹ SMILES / åŒ–å­¦å¼ç¬¦å·ï¼ˆ= # () [] @ + / -ï¼‰å°±è®¤ä¸ºæ˜¯
    - å¦åˆ™ï¼Œå¦‚æœæœ‰ >=4 ä¸ªå­—æ¯ï¼ˆtoluene, ethanol, ibuprofen ç­‰åŒ–å­¦åï¼‰ä¹Ÿè®¤ä¸ºæ˜¯
    è§„åˆ™æ•…æ„å†™å¾—æ¯”è¾ƒå®½æ¾ï¼Œé¿å…æ¼æ‰çœŸæ­£çš„åŒ–å­¦åã€‚
    """
    if not span_text:
        return False
    
    s = span_text.strip()
    if s in _MOL_STOPWORDS:
        return False
    if len(s) < 2:
        return False

    # å…¸å‹ SMILES / åŒ–å­¦å¼ç‰¹å¾ï¼šæ•°å­—ã€=ã€#ã€æ‹¬å·ã€@ã€+ã€/ã€-
    if any(c.isdigit() for c in s):
        return True
    if any(c in "=#()[]@+/-" for c in s):
        return True

    # å¯¹çº¯å­—æ¯çš„æƒ…å†µï¼šå¦‚æœæœ‰ >=4 ä¸ªå­—æ¯ï¼Œå½“æˆä¸€ä¸ªâ€œåƒåŒ–å­¦åâ€çš„è¯
    letters = [c for c in s if c.isalpha()]
    if len(letters) >= 4:
        return True

    return False


def _expand_and_merge_mol_spans(text: str, spans):
    """
    å¯¹åˆå§‹çš„ (start, end) spans åšç»Ÿä¸€åå¤„ç†ï¼š

    1) å‘å·¦/å‘å³æ‰©å±•åˆ°â€œå•è¯è¾¹ç•Œâ€ï¼ˆ_MOL_BOUNDARY_CHARSï¼‰
    2) å»æ‰ä¸¤ç«¯çš„å¼•å·/æ‹¬å·ï¼ˆ_MOL_TRIM_CHARSï¼‰
    3) åˆå¹¶é‡å æˆ–ç›¸é‚»çš„ spans
    4) åªä¿ç•™çœ‹èµ·æ¥åƒâ€œåˆ†å­ç›¸å…³â€çš„ spanï¼ˆ_looks_like_moleculeï¼‰

    Args:
        text: åŸå§‹å­—ç¬¦ä¸²ï¼ˆå·²ç»å»æ‰æ—§çš„ <mol> æ ‡ç­¾ï¼‰
        spans: List[(start, end)]ï¼Œæ¥è‡ª token offset + é¢„æµ‹ label=1

    Returns:
        List[(start, end)]ï¼šå¤„ç†åçš„ spansï¼ˆæŒ‰èµ·å§‹ä½ç½®æ’åºï¼Œä¸å«ç©º spanï¼‰
    """
    if not spans:
        return []

    expanded = []
    n = len(text)

    for s, e in spans:
        if s is None or e is None:
            continue
        if s >= e:
            continue

        # 1) å‘å·¦æ‰©å±•ï¼Œç›´åˆ°é‡åˆ°è¾¹ç•Œå­—ç¬¦
        while s > 0 and text[s - 1] not in _MOL_BOUNDARY_CHARS:
            s -= 1

        # 2) å‘å³æ‰©å±•ï¼Œç›´åˆ°é‡åˆ°è¾¹ç•Œå­—ç¬¦
        while e < n and text[e] not in _MOL_BOUNDARY_CHARS:
            e += 1

        # 3) ä¿®å‰ªå‰åçš„å¼•å·ã€æ‹¬å·ç­‰
        while s < e and text[s] in _MOL_TRIM_CHARS:
            s += 1
        while e > s and text[e - 1] in _MOL_TRIM_CHARS:
            e -= 1
                # 3.5) å¦‚æœæœ€åä¸€ä¸ªå­—ç¬¦æ˜¯å¥å°¾çš„å°æ•°ç‚¹ï¼Œä¸”åé¢å°±æ˜¯ç©ºç™½ / æ¢è¡Œ / ç»“æŸ / ç‰¹æ®Š tokenï¼Œå»æ‰è¿™ä¸ªç‚¹

        while s < e and text[e - 1] == '.':
            # e == nï¼šå°±æ˜¯å­—ç¬¦ä¸²æœ«å°¾
            # e < n ä¸”åé¢æ˜¯ç©ºç™½ or æ¢è¡Œ or '<'ï¼ˆé€šå¸¸æ˜¯ <|eot_id|> å‰é¢ï¼‰å°±è®¤ä¸ºæ˜¯å¥å°¾
            if e == n or text[e] in " \n\t<":
                e -= 1
            else:
                break

        if s < e:
            expanded.append((s, e))

    if not expanded:
        return []

    # 4) åˆå¹¶é‡å /ç›¸é‚» spans
    expanded.sort()
    merged = []
    for s, e in expanded:
        if not merged or s > merged[-1][1]:
            merged.append([s, e])
        else:
            merged[-1][1] = max(merged[-1][1], e)

    # 5) æœ€åç­›ä¸€éâ€œåƒåˆ†å­â€çš„ spans
    final_spans = []
    for s, e in merged:
        span_text = text[s:e]
        if _looks_like_molecule(span_text):
            final_spans.append((s, e))

    return final_spans

def _expand_spans_to_word_boundaries(
    text: str,
    spans: List[Tuple[int, int]],
) -> List[Tuple[int, int]]:
    """
    å°†å­—ç¬¦çº§ span æ‰©å±•åˆ°â€œè¯è¾¹ç•Œâ€ï¼ˆå‚è€ƒç‹¬ç«‹ MLP æ¨ç†è„šæœ¬çš„é€»è¾‘ï¼‰ï¼š
    - å‘å·¦æ‰©å±•ç›´åˆ°é‡åˆ°ç©ºç™½æˆ–æ ‡ç‚¹
    - å‘å³æ‰©å±•ç›´åˆ°é‡åˆ°ç©ºç™½æˆ–æ ‡ç‚¹
    å¹¶å¯¹æ‰©å±•åçš„ span åšä¸€æ¬¡åˆå¹¶ï¼Œé˜²æ­¢é‡å /åµŒå¥—
    """
    if not spans or not isinstance(text, str):
        return spans

    expanded: List[Tuple[int, int]] = []
    n = len(text)

    for start, end in spans:
        s, e = start, end
        # é˜²å¾¡è¶Šç•Œ
        if s < 0:
            s = 0
        if e > n:
            e = n

        # å‘å·¦æ‰©å±•ï¼Œç›´åˆ°é‡åˆ°â€œè¾¹ç•Œå­—ç¬¦â€
        while s > 0 and text[s - 1] not in WORD_BOUNDARY_CHARS:
            s -= 1
        # å‘å³æ‰©å±•ï¼Œç›´åˆ°é‡åˆ°â€œè¾¹ç•Œå­—ç¬¦â€
        while e < n and text[e] not in WORD_BOUNDARY_CHARS:
            e += 1

        expanded.append((s, e))

    # å¯¹æ‰©å±•åçš„ span å†åˆå¹¶ä¸€æ¬¡ï¼Œé˜²æ­¢é‡å 
    expanded.sort()
    merged: List[List[int]] = []
    for s, e in expanded:
        if not merged or s > merged[-1][1]:
            merged.append([s, e])
        else:
            merged[-1][1] = max(merged[-1][1], e)

    return [tuple(x) for x in merged]

def _save_dataset_to_jsonl(dataset: Dataset, file_path: str, is_tagged: bool = False):
    """ä¿ç•™æ—§æ¥å£ä»¥å…¼å®¹è°ƒç”¨æ–¹ï¼Œä½†ä¸å†ç”¨äºç¼“å­˜ã€‚"""
    os.makedirs(os.path.dirname(file_path) if os.path.dirname(file_path) else ".", exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        for example in dataset:
            f.write(json.dumps(example, ensure_ascii=False) + '\n')


def load_preprocessed_data(
    data_file: str,
    cache_dir: str = "./cache",
    use_cache: bool = True,
    max_samples: Optional[int] = None,
    max_message_chars: Optional[int] = None,
):
    """åŠ è½½é¢„å¤„ç†åçš„æ•°æ®ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ä¸é‡è¯•ï¼Œç›´æ¥è¯»å–æºæ–‡ä»¶ï¼‰
    
    Args:
        data_file: åŸå§‹æ•°æ®è·¯å¾„
        max_message_chars: å¦‚æœæŒ‡å®šï¼Œå¯¹ messages çš„å†…å®¹æ€»å­—ç¬¦æ•°è¶…é™çš„æ ·æœ¬è¿›è¡Œè¿‡æ»¤
    """
    if not os.path.exists(data_file):
        raise FileNotFoundError(f"Data file not found: {data_file}")
    
    file_size = os.path.getsize(data_file)
    print(f"ğŸ“‚ Loading data from: {data_file} (size: {file_size / 1024 / 1024:.2f} MB)")
    if max_samples is not None:
        print(f"ğŸ” DEBUG MODE: Limiting to {max_samples} samples")
    
    data_list = []
    
    def normalize_content(content):
        """å°†contentç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼"""
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if isinstance(item, dict):
                    if item.get("type") == "text":
                        text_parts.append(str(item.get("text", "")))
                    elif "text" in item:
                        text_parts.append(str(item["text"]))
                elif isinstance(item, str):
                    text_parts.append(item)
            return " ".join(text_parts) if text_parts else ""
        if isinstance(content, dict):
            if "text" in content:
                return str(content["text"])
            return json.dumps(content, ensure_ascii=False)
        return str(content) if content is not None else ""
    
    # æ‰‹åŠ¨è¯»å– JSON / JSONLï¼Œé¿å… load_dataset çš„é‡è¯•ä¸ç¼“å­˜
    if data_file.endswith('.jsonl'):
        with open(data_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    if "messages" in data and isinstance(data["messages"], list):
                        for msg in data["messages"]:
                            if "content" in msg:
                                msg["content"] = normalize_content(msg["content"])
                    data_list.append(data)
                except json.JSONDecodeError as je:
                    print(f"âš ï¸  Skipping invalid JSON at line {line_num}: {je}")
                except Exception as ex:
                    print(f"âš ï¸  Error processing line {line_num}: {ex}")
    else:
        with open(data_file, 'r', encoding='utf-8') as f:
            try:
                loaded = json.load(f)
            except Exception as e:
                raise ValueError(f"Failed to load JSON file: {e}") from e
            if isinstance(loaded, list):
                data_iter = loaded
            else:
                data_iter = [loaded]
            for idx, data in enumerate(data_iter, 1):
                if isinstance(data, dict) and "messages" in data and isinstance(data["messages"], list):
                    for msg in data["messages"]:
                        if "content" in msg:
                            msg["content"] = normalize_content(msg["content"])
                data_list.append(data)
    
    if not data_list:
        raise ValueError(f"No valid data loaded from {data_file}")
    
    try:
        raw = Dataset.from_list(data_list)
    except Exception as e2:
        print(f"âš ï¸  Dataset.from_list failed: {e2}")
        normalized_list = []
        for item in data_list:
            normalized_item = {}
            for k, v in item.items():
                if k == "messages" and isinstance(v, list):
                    normalized_item[k] = v
                elif isinstance(v, (dict, list)) and k != "messages":
                    normalized_item[k] = json.dumps(v, ensure_ascii=False)
                else:
                    normalized_item[k] = v
            normalized_list.append(normalized_item)
        raw = Dataset.from_list(normalized_list)
    
    print(f"ğŸ“Š Loaded {len(raw)} raw samples")
    
    if max_samples is not None and len(raw) > max_samples:
        raw = raw.select(range(max_samples))
    
    def _parse_chatml_text_to_messages(text: str):
        """
        å°† ChatML/Qwen é£æ ¼ text:
        <|im_start|>user\n...\n<|im_end|>\n<|im_start|>assistant\n...\n<|im_end|>\n
        è§£ææˆ messages=[{role, content}, ...]
        """
        if not isinstance(text, str) or not text.strip():
            return None

        pattern = r"<\|im_start\|>(user|assistant)\n(.*?)<\|im_end\|>"
        matches = re.findall(pattern, text, flags=re.DOTALL)
        if not matches:
            return None

        msgs = []
        for role, content in matches:
            msgs.append({"role": role, "content": content.strip()})

        # è‡³å°‘è¦æœ‰ user+assistant æ‰èƒ½åš SFTï¼ˆå¦åˆ™æ²¡æœ‰ç›‘ç£ä¿¡å·ï¼‰
        has_user = any(m["role"] == "user" and m["content"] for m in msgs)
        has_asst = any(m["role"] == "assistant" and m["content"] for m in msgs)
        if not (has_user and has_asst):
            return None
        return msgs

    def check_and_preserve_messages(example):
        """
        å…¼å®¹ä¸¤ç§è¾“å…¥ï¼š
        1) messages æ ¼å¼ï¼ˆä½ å½“å‰ loader çš„é»˜è®¤æ ¼å¼ï¼‰
        2) ä»… textï¼ˆChatML/Qwen é£æ ¼ï¼‰ï¼Œä¼šå°è¯•è§£æå› messages

        ç›®æ ‡ï¼š
        - å°½å¯èƒ½ä¿è¯ example["messages"] æ˜¯ list[dict]ï¼Œå¹¶ä¸”å« user+assistant
        - text ç»Ÿä¸€ç½®ä¸º "__MESSAGES_PLACEHOLDER__"ï¼Œè®© load_training_data é‡Œç”¨ tokenizer.apply_chat_template ç”Ÿæˆæœ€ç»ˆ text
        """
        msgs = example.get("messages", None)
        text = example.get("text", "")

        # -------- case 1: å·²æœ‰ messages --------
        if isinstance(msgs, list):
            has_valid_content = False
            for msg in msgs:
                if not isinstance(msg, dict):
                    continue
                content = msg.get("content", "")
                if not isinstance(content, str):
                    content = json.dumps(content, ensure_ascii=False)
                    msg["content"] = content
                if content and content.strip():
                    has_valid_content = True

            example["text"] = "__MESSAGES_PLACEHOLDER__" if has_valid_content else ""
            return example

        # -------- case 2: æ²¡æœ‰ messagesï¼Œä½†æœ‰ textï¼ˆå…¼å®¹ä½  text+meta æ•°æ®ï¼‰--------
        if isinstance(text, str) and text.strip():
            parsed = _parse_chatml_text_to_messages(text)
            if parsed is not None:
                example["messages"] = parsed
                example["text"] = "__MESSAGES_PLACEHOLDER__"
                return example

            # å¦‚æœè§£æä¸äº†ï¼ˆæ¯”å¦‚ä¸æ˜¯ ChatMLï¼‰ï¼Œè¿™é‡Œä¸è¦ raiseï¼Œç›´æ¥ç½®ç©ºè®©åç»­ filter æ‰
            example["text"] = ""
            return example

        # -------- case 3: ä¸¤è€…éƒ½æ²¡æœ‰/ä¸ºç©º --------
        example["text"] = ""
        return example
    
    raw = raw.map(check_and_preserve_messages, num_proc=min(4, os.cpu_count() or 1))
    
    def is_valid(example):
        t = example.get("text", "")
        if t == "__MESSAGES_PLACEHOLDER__":
            return True
        return isinstance(t, str) and len(t.strip()) > 0
    
    processed = raw.filter(is_valid, num_proc=min(4, os.cpu_count() or 1))
    
    # è¿‡æ»¤è¿‡é•¿å¯¹è¯ï¼ˆæŒ‰ messages æ€»å­—ç¬¦æ•°ï¼‰
    if max_message_chars is not None:
        def message_length_ok(example):
            msgs = example.get("messages", [])
            if not isinstance(msgs, list):
                return False
            total = 0
            for msg in msgs:
                if not isinstance(msg, dict):
                    continue
                content = msg.get("content", "")
                if not isinstance(content, str):
                    content = json.dumps(content, ensure_ascii=False)
                total += len(content)
                if total > max_message_chars:
                    return False
            return True
        
        before = len(processed)
        processed = processed.filter(message_length_ok, num_proc=min(4, os.cpu_count() or 1))
        after = len(processed)
        print(f"âœ‚ï¸  Filtered long messages by max_message_chars={max_message_chars}: {before} -> {after}")
    
    print(f"âœ… After filtering: {len(processed)} valid samples")
    
    if len(processed) == 0:
        raise ValueError(
            f"âŒ No valid samples found in {data_file}!\n"
            f"   Please check:\n"
            f"   1. Data file format (should be JSONL with 'text' field)\n"
            f"   2. Text field should not be empty"
        )
    
    def ensure_text_is_string(example):
        text = example.get("text", "")
        if not isinstance(text, str):
            if isinstance(text, list):
                example["text"] = text[0] if len(text) > 0 and isinstance(text[0], str) else ""
            else:
                example["text"] = str(text) if text is not None else ""
        else:
            example["text"] = text
        return example
    
    processed = processed.map(ensure_text_is_string, num_proc=16)
    
    return processed


def format_dataset_with_offline_spans(
    batch: Dict[str, List],
    tag_text_with_classifier: Optional[Callable[[str], str]] = None,
) -> Dict[str, List]:
    """æ ¼å¼åŒ–æ•°æ®é›†ï¼Œå¯é€‰ä½¿ç”¨ç¦»çº¿æ ‡æ³¨"""
    texts = []
    inputs = batch.get("input", [])
    outputs = batch.get("output", [])
    
    for i in range(len(inputs)):
        user = safe_to_str(inputs[i]).strip()
        assistant = safe_to_str(outputs[i]).strip()
        # ä½¿ç”¨é€šç”¨çš„ "User / Assistant" æ–‡æœ¬æ ¼å¼ï¼Œä¸å†ç»‘å®šä»»ä½•ç‰¹å®šæ¨¡å‹çš„ç‰¹æ®Š tokenã€‚
        # å…·ä½“çš„ chat_template ç”± tokenizer åœ¨è®­ç»ƒ/æ¨ç†æ—¶å†³å®šã€‚
        concat = f"User: {user}\n\nAssistant: {assistant}"
        
        if tag_text_with_classifier is not None:
            tagged = tag_text_with_classifier(concat)
            texts.append(tagged)
        else:
            texts.append(concat)
    
    result = {"text": texts}
    
    # ä¿ç•™metaä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    meta_keys = [
        "id", "dataset", "source", "task_type", "smiles", "class_label",
        "property_name", "property_symbol", "property_description",
        "unit", "target_value", "all_targets"
    ]
    for k in meta_keys:
        if k in batch:
            result[k] = batch[k]
    
    return result


def create_tag_text_function(
    tokenizer,
    llm,
    offline_token_head,
    local_rank: int,
    max_length: int = 512,
) -> Optional[Callable[[str], str]]:
    """åˆ›å»ºæ–‡æœ¬æ ‡æ³¨å‡½æ•°ï¼ˆå•æ–‡æœ¬ç‰ˆæœ¬ï¼Œç”¨äºå…¼å®¹ï¼‰"""
    if offline_token_head is None:
        return None
    
    def tag_text_with_classifier(text: str) -> str:
        if not isinstance(text, str) or not text:
            return text
        try:
            # å…ˆæ¸…é™¤å·²æœ‰ <mol> æ ‡ç­¾ï¼Œé¿å…é‡å¤åµŒå¥—
            clean = re.sub(r"</?mol>", "", text)
            enc = tokenizer(
                clean,
                return_tensors="pt",
                return_offsets_mapping=True,
                truncation=True,
                max_length=max_length,
                padding=False,
            )
            input_ids = enc["input_ids"].to(local_rank)
            attn = enc["attention_mask"].to(local_rank)
            offsets = enc["offset_mapping"][0].tolist()
            
            with torch.no_grad():
                out = llm(
                    input_ids=input_ids,
                    attention_mask=attn,
                    output_hidden_states=True,
                    return_dict=True
                )
                hs = out.hidden_states[-1]  # (1, T, H)
                # ç¡®ä¿ dtype åŒ¹é…ï¼šè·å– offline_token_head çš„ dtype
                try:
                    head_dtype = next(offline_token_head.parameters()).dtype
                    if head_dtype != hs.dtype:
                        hs = hs.to(head_dtype)
                except (StopIteration, AttributeError):
                    # å¦‚æœæ²¡æœ‰å‚æ•°æˆ–æ— æ³•è·å–dtypeï¼Œä½¿ç”¨é»˜è®¤çš„float32
                    if hs.dtype != torch.float32:
                        hs = hs.to(torch.float32)
                logits = offline_token_head(hs)  # (1, T, 2)
                preds = torch.argmax(logits, dim=-1)[0].tolist()
            
            # æ”¶é›†è¿ç»­å®ä½“å­—ç¬¦ spanï¼ˆå…ˆæŒ‰ token offset æ‹¼èµ·æ¥ï¼‰
            spans = []
            cur = None
            for p, (s, e) in zip(preds, offsets):
                if s == e:
                    continue
                if p == 1:
                    if cur is None:
                        cur = [s, e]
                    else:
                        cur[1] = e
                else:
                    if cur is not None:
                        spans.append(tuple(cur))
                        cur = None
            if cur is not None:
                spans.append(tuple(cur))

            if not spans:
                return clean
            
            # æ–°å¢ï¼šç”¨ç»Ÿä¸€çš„è§„åˆ™æ‰©å±• + åˆå¹¶ + ä¿®å‰ª + è¿‡æ»¤
            spans = _expand_and_merge_mol_spans(clean, spans)
            if not spans:
                return clean
            
            # === special token ä¿æŠ¤é€»è¾‘ ===
            # è¿™é‡Œåªä¿æŠ¤æˆ‘ä»¬è‡ªå·±æ’å…¥çš„ <mol> æ ‡ç­¾ï¼Œé¿å…é‡å¤åµŒå¥—ï¼›
            # ä¸å†ä¾èµ–ä»»ä½•ç‰¹å®šæ¨¡å‹ï¼ˆå¦‚ Llamaï¼‰çš„å¯¹è¯ header tokenã€‚
            special_tokens = [
                "<mol>", "</mol>",
            ]
            
            # æ‰¾åˆ°æ‰€æœ‰ç‰¹æ®Š token çš„ä½ç½®
            special_token_ranges = []
            for st in special_tokens:
                start = 0
                while True:
                    pos = clean.find(st, start)
                    if pos == -1:
                        break
                    special_token_ranges.append((pos, pos + len(st)))
                    start = pos + 1
            
            # æ£€æŸ¥ç‰¹æ®Š token å¯¹ä¹‹é—´çš„èŒƒå›´ï¼ˆå¦‚ <|start_header_id|>...<|end_header_id|>ï¼‰
            header_pairs = []
            start_pos = 0
            while True:
                start_header = clean.find("<|start_header_id|>", start_pos)
                if start_header == -1:
                    break
                end_header = clean.find("<|end_header_id|>", start_header)
                if end_header != -1:
                    header_pairs.append((start_header, end_header + len("<|end_header_id|>")))
                    start_pos = end_header + len("<|end_header_id|>")
                else:
                    break
            
            # è¿‡æ»¤æ‰ä¸ç‰¹æ®Š token é‡å æˆ–åœ¨ç‰¹æ®Š token å¯¹ä¹‹é—´çš„ spans
            filtered_spans = []
            for s, e in spans:
                is_special = False
                
                # æ£€æŸ¥æ˜¯å¦ä¸ä»»ä½•ç‰¹æ®Š token é‡å 
                for st_start, st_end in special_token_ranges:
                    if not (e <= st_start or s >= st_end):
                        is_special = True
                        break
                
                # æ£€æŸ¥æ˜¯å¦åœ¨ä»»ä½• header å¯¹ä¹‹é—´ï¼ˆåŒ…æ‹¬è¾¹ç•Œï¼‰
                if not is_special:
                    for pair_start, pair_end in header_pairs:
                        if s >= pair_start and e <= pair_end:
                            is_special = True
                            break
                
                if not is_special:
                    filtered_spans.append((s, e))
            
            if not filtered_spans:
                return clean
            
            # ä»åå¾€å‰æ’ <mol></mol>ï¼Œé¿å…ç´¢å¼•åç§»
            tagged = clean
            for s, e in reversed(filtered_spans):
                tagged = tagged[:e] + "</mol>" + tagged[e:]
                tagged = tagged[:s] + "<mol>" + tagged[s:]
            return tagged
        except Exception:
            # å‡ºé”™æ—¶ä¿åº•è¿”å›åŸå§‹æ–‡æœ¬
            return text
    
    return tag_text_with_classifier




def tag_text_with_smiles(text: str, smiles: Optional[str]) -> str:
    """
    åŸºäº SMILES åŒ¹é…åœ¨æ–‡æœ¬ä¸­æ·»åŠ  <mol></mol> æ ‡ç­¾
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        smiles: SMILES å­—ç¬¦ä¸²ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™è¿”å›åŸæ–‡æœ¬ï¼‰
    
    Returns:
        æ ‡æ³¨åçš„æ–‡æœ¬
    """
    if not isinstance(text, str) or not text:
        return text
    
    if not smiles or not isinstance(smiles, str):
        return text
    
    # ç§»é™¤å·²æœ‰çš„ <mol> æ ‡ç­¾
    clean_text = re.sub(r"</?mol>", "", text)
    
    # åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾ SMILES å­—ç¬¦ä¸²
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼Œç¡®ä¿åŒ¹é…å®Œæ•´çš„ SMILESï¼ˆå‰åä¸æ˜¯å­—æ¯æ•°å­—ï¼‰
    # SMILES å¯èƒ½åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œéœ€è¦è½¬ä¹‰
    escaped_smiles = re.escape(smiles)
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…ä½ç½®
    matches = list(re.finditer(escaped_smiles, clean_text))
    
    if not matches:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•ä¸åŒºåˆ†å¤§å°å†™
        matches = list(re.finditer(re.escape(smiles), clean_text, re.IGNORECASE))
    
    if not matches:
        return clean_text
    
    # ä»åå¾€å‰æ’å…¥æ ‡ç­¾ï¼Œé¿å…ç´¢å¼•åç§»
    tagged_text = clean_text
    for match in reversed(matches):
        start, end = match.span()
        # æ£€æŸ¥æ˜¯å¦åœ¨ç‰¹æ®Š token å†…éƒ¨
        # é¿å…åœ¨ç‰¹æ®Š token ä¸­æ’å…¥æ ‡ç­¾
        special_tokens = [
            "<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>",
            "<|user|>", "<|assistant|>",  # å…¼å®¹æ—§æ ¼å¼
        ]
        
        is_special = False
        for st in special_tokens:
            # æ£€æŸ¥åŒ¹é…ä½ç½®æ˜¯å¦ä¸ç‰¹æ®Š token é‡å 
            st_start = clean_text.find(st, max(0, start - len(st)), min(len(clean_text), end + len(st)))
            if st_start != -1:
                st_end = st_start + len(st)
                # å¦‚æœåŒ¹é…ä½ç½®ä¸ç‰¹æ®Š token æœ‰ä»»ä½•é‡å ï¼Œè·³è¿‡
                if not (end <= st_start or start >= st_end):
                    is_special = True
                    break
        
        if not is_special:
            # æ’å…¥æ ‡ç­¾
            tagged_text = tagged_text[:end] + "</mol>" + tagged_text[end:]
            tagged_text = tagged_text[:start] + "<mol>" + tagged_text[start:]
    
    return tagged_text


def create_batch_tag_text_function(
    tokenizer,
    llm,
    offline_token_head,
    local_rank: int,
    max_length: int = 512,
    batch_size: int = 32,  # é»˜è®¤å€¼ä» 32 æ”¹ä¸ºæ›´å°çš„å€¼
) -> Optional[Callable[[List[str]], List[str]]]:
    """åˆ›å»ºæ‰¹é‡æ–‡æœ¬æ ‡æ³¨å‡½æ•°ï¼ˆæ›´å¿«ï¼‰"""
    if offline_token_head is None:
        return None
    
    # ç¡®ä¿ LLM åœ¨ eval æ¨¡å¼ï¼ˆèŠ‚çœå†…å­˜ï¼‰
    original_training_mode = llm.training
    llm.eval()
    # ä¿å­˜åŸå§‹ use_cache è®¾ç½®
    original_use_cache = None
    if hasattr(llm.config, 'use_cache'):
        original_use_cache = llm.config.use_cache
        llm.config.use_cache = False
    
    def tag_texts_batch(texts: List[str]) -> List[str]:
        """æ‰¹é‡å¤„ç†æ–‡æœ¬æ ‡æ³¨"""
        if not texts:
            return texts
        
        results = []
        device = torch.device(f"cuda:{local_rank}" if torch.cuda.is_available() else "cpu")
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            # å…ˆæ¸…ç†å·²æœ‰çš„ <mol> æ ‡ç­¾
            batch_cleaned = [re.sub(r"</?mol>", "", t) if isinstance(t, str) else "" for t in batch_texts]
            
            try:
                # æ¸…ç†å†…å­˜
                torch.cuda.empty_cache()
                
                # æ‰¹é‡ç¼–ç ï¼ˆä½¿ç”¨paddingï¼‰
                enc = tokenizer(
                    batch_cleaned,
                    return_tensors="pt",
                    return_offsets_mapping=True,
                    truncation=True,
                    max_length=max_length,
                    padding=True,
                )
                input_ids = enc["input_ids"].to(device)
                attn = enc["attention_mask"].to(device)
                offsets_list = enc["offset_mapping"]
                
                # ç«‹å³é‡Šæ”¾ CPU ä¸Šçš„ç¼–ç ç»“æœ
                del enc
                
                with torch.no_grad():
                    # ä½¿ç”¨æ›´èŠ‚çœå†…å­˜çš„æ¨ç†æ–¹å¼
                    out = llm(
                        input_ids=input_ids,
                        attention_mask=attn,
                        output_hidden_states=True,
                        return_dict=True,
                        use_cache=False,  # ç¦ç”¨ç¼“å­˜ä»¥èŠ‚çœå†…å­˜
                    )
                    hs = out.hidden_states[-1]  # (B, T, H)
                    # ç¡®ä¿ dtype åŒ¹é…
                    try:
                        head_dtype = next(offline_token_head.parameters()).dtype
                        if head_dtype != hs.dtype:
                            hs = hs.to(head_dtype)
                    except (StopIteration, AttributeError):
                        if hs.dtype != torch.float32:
                            hs = hs.to(torch.float32)
                    logits = offline_token_head(hs)  # (B, T, 2)
                    preds = torch.argmax(logits, dim=-1).cpu().tolist()  # (B, T)
                
                # æ¸…ç† GPU å†…å­˜
                del out, hs, logits, input_ids, attn
                # offsets_list ä¿æŒåœ¨ CPU
                offsets_list_cpu = offsets_list
                torch.cuda.empty_cache()
                
                # å¯¹æ¯ä¸ªæ ·æœ¬å¤„ç†
                for j, (clean_text, pred, offsets) in enumerate(zip(batch_cleaned, preds, offsets_list_cpu)):
                    if not clean_text:
                        results.append(batch_texts[j])
                        continue
                    
                    # æ”¶é›†è¿ç»­å®ä½“å­—ç¬¦ spanï¼ˆtoken offset å±‚ï¼‰
                    spans = []
                    cur = None
                    offsets_items = offsets if isinstance(offsets, list) else offsets.tolist()
                    for p, (s, e) in zip(pred, offsets_items):
                        if s == e:
                            continue
                        if p == 1:
                            if cur is None:
                                cur = [s, e]
                            else:
                                cur[1] = e
                        else:
                            if cur is not None:
                                spans.append(tuple(cur))
                                cur = None
                    if cur is not None:
                        spans.append(tuple(cur))
                    
                    if not spans:
                        results.append(clean_text)
                        continue
                    
                    # æ–°å¢ï¼šç»Ÿä¸€åšæ‰©å±• + åˆå¹¶ + ä¿®å‰ª + è¿‡æ»¤
                    spans = _expand_and_merge_mol_spans(clean_text, spans)
                    if not spans:
                        results.append(clean_text)
                        continue
                    
                    # === special token ä¿æŠ¤é€»è¾‘ï¼ˆä¸å•æ ·æœ¬ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰ ===
                    special_tokens = [
                        "<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>",
                        "<|user|>", "<|assistant|>",  # å…¼å®¹æ—§æ ¼å¼
                    ]
                    
                    # æ‰¾åˆ°æ‰€æœ‰ç‰¹æ®Š token çš„ä½ç½®
                    special_token_ranges = []
                    for st in special_tokens:
                        start = 0
                        while True:
                            pos = clean_text.find(st, start)
                            if pos == -1:
                                break
                            special_token_ranges.append((pos, pos + len(st)))
                            start = pos + 1
                    
                    # æ£€æŸ¥ç‰¹æ®Š token å¯¹ä¹‹é—´çš„èŒƒå›´ï¼ˆå¦‚ <|start_header_id|>...<|end_header_id|>ï¼‰
                    header_pairs = []
                    start_pos = 0
                    while True:
                        start_header = clean_text.find("<|start_header_id|>", start_pos)
                        if start_header == -1:
                            break
                        end_header = clean_text.find("<|end_header_id|>", start_header)
                        if end_header != -1:
                            header_pairs.append((start_header, end_header + len("<|end_header_id|>")))
                            start_pos = end_header + len("<|end_header_id|>")
                        else:
                            break
                    
                    # è¿‡æ»¤æ‰ä¸ç‰¹æ®Š token é‡å æˆ–åœ¨ç‰¹æ®Š token å¯¹ä¹‹é—´çš„ spans
                    filtered_spans = []
                    for s, e in spans:
                        is_special = False
                        
                        # æ£€æŸ¥æ˜¯å¦ä¸ä»»ä½•ç‰¹æ®Š token é‡å 
                        for st_start, st_end in special_token_ranges:
                            if not (e <= st_start or s >= st_end):
                                is_special = True
                                break
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨ä»»ä½• header å¯¹ä¹‹é—´ï¼ˆåŒ…æ‹¬è¾¹ç•Œï¼‰
                        if not is_special:
                            for pair_start, pair_end in header_pairs:
                                if s >= pair_start and e <= pair_end:
                                    is_special = True
                                    break
                        
                        if not is_special:
                            filtered_spans.append((s, e))
                    
                    if not filtered_spans:
                        results.append(clean_text)
                        continue
                    
                    tagged = clean_text
                    for s, e in reversed(filtered_spans):
                        tagged = tagged[:e] + "</mol>" + tagged[e:]
                        tagged = tagged[:s] + "<mol>" + tagged[s:]
                    results.append(tagged)
                
                # æ¯ä¸ª batch æˆåŠŸå¤„ç†åæ¸…ç†å†…å­˜
                torch.cuda.empty_cache()
                    
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å­˜ç›¸å…³é”™è¯¯
                error_msg = str(e).lower()
                is_memory_error = any(keyword in error_msg for keyword in [
                    "cuda out of memory",
                    "out of memory",
                    "cublas",
                    "cudnn",
                    "memory",
                ])
                
                if is_memory_error:
                    # å†…å­˜é”™è¯¯ç›´æ¥æŠ›å‡ºï¼Œä¸è¿›è¡Œfallback
                    print(f"âŒ CUDA memory error during batch tagging (batch {i//batch_size}): {e}")
                    print(f"   Batch size: {batch_size}, Max length: {max_length}")
                    print(f"   Suggestion: Reduce offline_tagging_batch_size in config or reduce max_seq_length")
                    # æ¸…ç†å†…å­˜
                    torch.cuda.empty_cache()
                    raise RuntimeError(f"CUDA out of memory during offline tagging. Original error: {e}") from e
                else:
                    # å…¶ä»–ç±»å‹çš„é”™è¯¯ä¹Ÿç›´æ¥æŠ›å‡ºï¼ˆä¸å†fallbackï¼‰
                    print(f"âŒ Batch tagging failed for batch {i//batch_size}: {e}")
                    torch.cuda.empty_cache()
                    raise RuntimeError(f"Batch tagging failed. Original error: {e}") from e
        
        # æ¢å¤åŸå§‹è®¾ç½®ï¼ˆè™½ç„¶å‡½æ•°ç»“æŸæ—¶å¯èƒ½ä¸éœ€è¦ï¼Œä½†ä¸ºäº†å®‰å…¨ï¼‰
        if original_training_mode:
            llm.train()
        if original_use_cache is not None:
            llm.config.use_cache = original_use_cache
        
        return results
    
    return tag_texts_batch




def load_training_data(
    cfg: Dict[str, Any],
    tokenizer,
    llm,
    offline_token_head: Optional[torch.nn.Module],
    local_rank: int,
) -> tuple:
    """
    åŠ è½½è®­ç»ƒæ•°æ®
    
    Returns:
        train_dataset, eval_dataset
    """
    data_cfg = cfg.get("data", {})
    dataset_path = data_cfg.get("dataset_path") or cfg.get("train", {}).get("dataset_path")
    
    if not dataset_path:
        raise ValueError("dataset_path not found in config")
    
    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºä»£ç ç›®å½•ï¼‰
    if not os.path.isabs(dataset_path):
        # è·å–ä»£ç ç›®å½•ï¼ˆtrain_sft.pyæ‰€åœ¨ç›®å½•ï¼‰
        code_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        dataset_path = os.path.join(code_dir, dataset_path)
        dataset_path = os.path.abspath(dataset_path)
    
    print(f"ğŸ“‚ Using dataset path: {dataset_path}")
    
    # åŠ è½½é¢„å¤„ç†åçš„æ•°æ®ï¼ˆå·²ç»æ˜¯æ ¼å¼åŒ–åçš„ï¼ŒåŒ…å«textå­—æ®µï¼‰
    use_cache = cfg.get("data", {}).get("use_cache", True)
    # æ”¯æŒè°ƒè¯•æ¨¡å¼ï¼šé™åˆ¶æ•°æ®é‡ï¼ˆåœ¨è¿™é‡Œåšã€Œå¸¦ seed çš„éšæœºé‡‡æ ·ã€ï¼‰
    max_samples = cfg.get("data", {}).get("debug_max_samples", None)
    max_tokens = cfg.get("data", {}).get("max_tokens", None)  # æŒ‰ token æ•°è¿‡æ»¤è¶…é•¿æ ·æœ¬
    if max_samples is not None:
        print(f"ğŸ” DEBUG MODE ENABLED: max_samples={max_samples}")
    # æ³¨æ„ï¼šè¿™é‡Œä¸å†æŠŠ max_samples ä¼ ç»™ load_preprocessed_dataï¼Œä»¥é¿å…æ€»æ˜¯å–å‰ N æ¡
    max_message_chars = cfg.get("data", {}).get("max_message_chars", None)
    if max_message_chars is not None:
        print(f"â›” Max message chars: {max_message_chars}")
    processed_dataset = load_preprocessed_data(
        dataset_path,
        cache_dir="./cache",
        use_cache=use_cache,
        max_samples=None,
        max_message_chars=max_message_chars,
    )

    # å¦‚æœé…ç½®äº† debug_max_samplesï¼Œå¹¶ä¸”æ•°æ®é‡å¤§äºè¯¥å€¼ï¼Œåˆ™ä½¿ç”¨ seed åšä¸€æ¬¡å¯å¤ç°çš„éšæœºé‡‡æ ·
    if max_samples is not None and len(processed_dataset) > max_samples:
        base_seed = int(cfg.get("seed", 42))
        rank = int(os.environ.get("RANK", 0))
        # ä¸åŒ rank ä½¿ç”¨ä¸åŒ seedï¼Œé¿å…å¤šå¡é‡‡æ ·å®Œå…¨é‡åˆï¼›ä½†åŒä¸€é…ç½®/èŠ‚ç‚¹ä¸‹æ˜¯å¯å¤ç°çš„
        shuffle_seed = base_seed + rank
        if rank == 0:
            print(f"ğŸ”€ Shuffling dataset with seed={shuffle_seed} and selecting first {max_samples} samples")
        processed_dataset = processed_dataset.shuffle(seed=shuffle_seed)
        processed_dataset = processed_dataset.select(range(max_samples))
        if rank == 0:
            print(f"âœ… After debug sampling: {len(processed_dataset)} samples")
    
    # å¦‚æœæ•°æ®åŒ…å« messages å­—æ®µï¼Œä½¿ç”¨ tokenizer.apply_chat_template è½¬æ¢ä¸º text
    # è¿™æ¯”ç®€å•çš„å­—ç¬¦ä¸²æ‹¼æ¥æ›´å‡†ç¡®ï¼Œèƒ½ä½¿ç”¨æ¨¡å‹ç‰¹å®šçš„ chat template
    if len(processed_dataset) > 0 and "messages" in processed_dataset[0]:
        rank = int(os.environ.get("RANK", 0))
        if rank == 0:
            print("ğŸ”„ Converting messages format to text using tokenizer.apply_chat_template...")
        
        def convert_messages_with_template(example):
            """ä½¿ç”¨ tokenizer.apply_chat_template å°† messages è½¬æ¢ä¸º textã€‚
            æ— è®º example é‡Œæ˜¯å¦å·²æœ‰ textï¼Œåªè¦å­˜åœ¨ messagesï¼Œå°±ç»Ÿä¸€æŒ‰ chat_template é‡æ–°æ¸²æŸ“ï¼Œ
            ä»¥ç¡®ä¿æ ¼å¼ä¸å½“å‰ tokenizerï¼ˆä¾‹å¦‚ Mistral çš„ [INST] æ¨¡æ¿ï¼‰ä¸¥æ ¼ä¸€è‡´ã€‚
            """
            if "messages" in example and isinstance(example["messages"], list):
                messages = example["messages"]
                try:
                    # ä½¿ç”¨ tokenizer çš„ chat_template ç”Ÿæˆå®Œæ•´å¯¹è¯æ–‡æœ¬
                    formatted_text = tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,          # åªæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
                        add_generation_prompt=False,  # è®­ç»ƒæ—¶ä¸åŠ ç”Ÿæˆæç¤º
                    )
                    example["text"] = formatted_text
                    # å¯é€‰ï¼šå¦‚æœæƒ³èŠ‚çœå†…å­˜ï¼Œå¯ä»¥åˆ é™¤ messages å­—æ®µ
                    # del example["messages"]
                except Exception as e:
                    # å¦‚æœ apply_chat_template å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•çš„ User/Assistant æ‹¼æ¥
                    if rank == 0 and len(str(e)) < 200:
                        print(f"âš ï¸  apply_chat_template failed for one sample: {e}, using fallback")
                    text_parts = []
                    for msg in messages:
                        role = msg.get("role", "").lower()
                        content = msg.get("content", "")
                        if content:
                            if role == "system":
                                continue
                            elif role == "user":
                                text_parts.append(f"User: {content}")
                            elif role == "assistant":
                                text_parts.append(f"Assistant: {content}")
                    example["text"] = "\n\n".join(text_parts) if text_parts else ""
            return example
        
        try:
            processed_dataset = processed_dataset.map(
                convert_messages_with_template,
                num_proc=min(4, os.cpu_count() or 1),
                desc="Converting messages to text with chat template"
            )
            if rank == 0:
                print("âœ… Messages converted to text using chat template")
        except Exception as e:
            if rank == 0:
                print(f"âš ï¸  Failed to convert messages with template: {e}")
                print("   Using data as-is (may already have text field)")
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ç°æœ‰æ•°æ®
    
    # è¿‡æ»¤è¶…é•¿ token çš„æ ·æœ¬ï¼ˆåŸºäºç”Ÿæˆåçš„ textï¼‰
    if max_tokens is not None:
        rank = int(os.environ.get("RANK", 0))
        if rank == 0:
            print(f"âœ‚ï¸  Filtering samples longer than {max_tokens} tokens (tokenizer-based)")
        def token_length_ok(example):
            t = example.get("text", "")
            if not isinstance(t, str):
                return False
            # ä¸æˆªæ–­ï¼Œå®Œæ•´è®¡ç®—é•¿åº¦
            ids = tokenizer.encode(t, add_special_tokens=True, truncation=False)
            return len(ids) <= max_tokens
        before = len(processed_dataset)
        processed_dataset = processed_dataset.filter(token_length_ok, num_proc=1)
        after = len(processed_dataset)
        if rank == 0:
            print(f"âœ‚ï¸  Token-length filter: {before} -> {after} samples (max_tokens={max_tokens})")
    
    # æ‰“å°ç¬¬ä¸€æ¡æ•°æ®çš„ç»“æ„
    rank = int(os.environ.get("RANK", 0))
    if rank == 0 and len(processed_dataset) > 0:
        print("\n" + "="*80)
        print("ğŸ“‹ First sample from dataset (after load_preprocessed_data):")
        print("="*80)
        first_sample = processed_dataset[0]
        print(f"Type: {type(first_sample)}")
        print(f"Content: {first_sample}")
        if isinstance(first_sample, dict):
            print(f"Keys: {list(first_sample.keys())}")
            for key, value in first_sample.items():
                print(f"  {key}: type={type(value)}, value={str(value)[:200]}...")
        print("="*80 + "\n")
    
    # åˆ¤æ–­æ˜¯å¦æ˜¯ epoch1ï¼ˆé€šè¿‡æ£€æŸ¥ dataset_path æ˜¯å¦åŒ…å« "epoch1"ï¼‰
    is_epoch1 = "epoch1" in dataset_path.lower()
    
    # åªé€šè¿‡å®é™…æ•°æ®åˆ¤æ–­æ˜¯å¦å·²åŒ…å«æ ‡ç­¾
    has_mol_tags = False
    if len(processed_dataset) > 0:
        sample = processed_dataset[0]
        sample_text = sample.get("text", "")
        has_mol_tags = ("<mol>" in sample_text) and ("</mol>" in sample_text)
        if has_mol_tags:
            print(f"âœ… Data contains <mol> tags, but cache metadata not found")

    # åˆ¤æ–­æ˜¯å¦éœ€è¦æ ‡æ³¨
    is_already_tagged = has_mol_tags
    need_tagging = not is_already_tagged
    tagged_cache_file = None
    rank = int(os.environ.get("RANK", 0))

    if is_already_tagged:
        # æ•°æ®å·²æ ‡æ³¨ï¼šç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦é‡æ–°æ ‡æ³¨
        if rank == 0:
            print("âœ… Data is already tagged, skipping tagging step")
        need_tagging = False
    else:
        # æ•°æ®æœªæ ‡æ³¨ï¼šåˆ é™¤æ‰€æœ‰å¯èƒ½å­˜åœ¨çš„ <mol> / </mol> æ ‡ç­¾ï¼Œå‡†å¤‡æ ‡æ³¨
        if rank == 0:
            print("ğŸ§¹ Removing any existing <mol></mol> tags before tagging")

        def strip_mol_tags(ex):
            try:
                t = ex.get("text", "")
                if isinstance(t, str):
                    ex["text"] = re.sub(r"</?mol>", "", t)
                elif isinstance(t, list):
                    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå¤„ç†æ¯ä¸ªå…ƒç´ 
                    ex["text"] = [re.sub(r"</?mol>", "", str(item)) if isinstance(item, str) else str(item) for item in t]
                else:
                    # å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²åå¤„ç†
                    ex["text"] = re.sub(r"</?mol>", "", str(t)) if t is not None else ""
                return ex
            except Exception as e:
                print(f"âš ï¸  Error in strip_mol_tags: {e}, example keys: {list(ex.keys()) if isinstance(ex, dict) else 'N/A'}")
                return ex

        try:
            processed_dataset = processed_dataset.map(
                strip_mol_tags,
                num_proc=min(16, os.cpu_count() or 1),
                desc="Stripping any existing <mol> tags",
            )
            if rank == 0:
                print(f"âœ… Stripped <mol> tags, dataset size: {len(processed_dataset)}")
        except Exception as e:
            print(f"âŒ Failed to strip <mol> tags: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        # ä¸å†ä½¿ç”¨ tagged ç¼“å­˜
        tagged_cache_file = None
    
    if not need_tagging:
        rank = int(os.environ.get("RANK", 0))
        if rank == 0:
            print("âœ… Data already contains <mol> tags, skipping tagging")
    else:
        # å¯¹äº epoch1ï¼Œä½¿ç”¨ SMILES åŒ¹é…æ–¹æ³•ï¼ˆæ›´å¿«ï¼Œä¸éœ€è¦ LLM æ¨ç†ï¼‰
        if is_epoch1:
            rank = int(os.environ.get("RANK", 0))
            if rank == 0:
                print("ğŸ”„ Applying SMILES-based tagging for epoch1 data...")
            
            def tag_with_smiles(example):
                """ä½¿ç”¨ SMILES åŒ¹é…æ·»åŠ æ ‡ç­¾"""
                text = example.get("text", "")
                smiles = example.get("smiles", None)
                if text and smiles:
                    example["text"] = tag_text_with_smiles(text, smiles)
                return example
            
            # æ‰¹é‡å¤„ç†ï¼ˆä½¿ç”¨ mapï¼Œå¯ä»¥å¹¶è¡Œï¼‰
            processed_dataset = processed_dataset.map(
                tag_with_smiles,
                num_proc=min(4, os.cpu_count() or 1),
                desc="Tagging with SMILES"
            )
            
            if rank == 0:
                print("âœ… SMILES-based tagging completed")
                # ä¿å­˜taggedæ•°æ®åˆ°ç¼“å­˜ï¼ˆæ ‡è®°ä¸ºå·²æ ‡æ³¨ï¼‰
                if use_cache and tagged_cache_file:
                    print(f"ğŸ’¾ Saving tagged data to cache: {tagged_cache_file}")
                    os.makedirs(os.path.dirname(tagged_cache_file) if os.path.dirname(tagged_cache_file) else ".", exist_ok=True)
                    _save_dataset_to_jsonl(processed_dataset, tagged_cache_file, is_tagged=True)
                    print(f"âœ… Tagged cache saved ({len(processed_dataset)} samples, is_tagged=True)")
        
        # å¯¹äº epoch2 æˆ–å…¶ä»–æƒ…å†µï¼Œä½¿ç”¨ç¦»çº¿æ ‡æ³¨ï¼ˆLLM + token classifierï¼‰
        elif cfg.get("train", {}).get("use_offline_spans", False):
            if offline_token_head is None:
                if rank == 0:
                    print("âš ï¸  use_offline_spans=True but offline_token_head is None")
                    print("   This might happen if token classifier failed to load")
                    print("   Will skip offline tagging and use data as-is")
                # å¦‚æœ offline_token_head ä¸º Noneï¼Œè·³è¿‡æ ‡æ³¨ï¼Œç›´æ¥ä½¿ç”¨æ•°æ®
                need_tagging = False
            else:
                # æ£€æŸ¥æ˜¯å¦åœ¨ DDP ç¯å¢ƒä¸‹
                rank = int(os.environ.get("RANK", 0))
                world_size = int(os.environ.get("WORLD_SIZE", 1))
                is_distributed = world_size > 1
                # åœ¨ DDP ç¯å¢ƒä¸‹ï¼Œå°†æ•°æ®åˆ†ç‰‡ï¼Œæ¯ä¸ªè¿›ç¨‹å¤„ç†è‡ªå·±çš„åˆ†ç‰‡ï¼Œç„¶ååŒæ­¥
                # ä½¿ç”¨æ›´å°çš„ max_length ç”¨äº offline tagging ä»¥èŠ‚çœå†…å­˜ï¼ˆå¯ä»¥å°äºè®­ç»ƒæ—¶çš„ max_seq_lengthï¼‰
                training_max_length = cfg.get("train", {}).get("max_seq_length", 2048)
                # offline tagging æ—¶ä½¿ç”¨æ›´å°çš„é•¿åº¦ä»¥èŠ‚çœå†…å­˜ï¼ˆé»˜è®¤æ˜¯è®­ç»ƒé•¿åº¦çš„ä¸€åŠï¼Œæœ€å° 512ï¼‰
                max_length = cfg.get("train", {}).get("offline_tagging_max_length", None)
                if max_length is None:
                    max_length = max(512, training_max_length // 2)  # é»˜è®¤ä½¿ç”¨è®­ç»ƒé•¿åº¦çš„ä¸€åŠï¼Œæœ€å° 512
                batch_size = cfg.get("train", {}).get("offline_tagging_batch_size", 32)
                
                if is_distributed:
                    # è®¡ç®—æ¯ä¸ªè¿›ç¨‹çš„æ•°æ®åˆ†ç‰‡
                    total_size = len(processed_dataset)
                    chunk_size = total_size // world_size
                    start_idx = rank * chunk_size
                    end_idx = start_idx + chunk_size if rank < world_size - 1 else total_size
                    
                    print(f"ğŸ”„ Applying offline tagging to add <mol> tags... (rank {rank}/{world_size-1}, processing samples {start_idx}-{end_idx-1})")
                    print(f"   Using max_length={max_length} (training max_length={training_max_length}), batch_size={batch_size}")
                    
                    # ç¡®ä¿ LLM åœ¨ eval æ¨¡å¼å¹¶æ¸…ç†å†…å­˜
                    llm.eval()
                    torch.cuda.empty_cache()
                    
                    # é€‰æ‹©å½“å‰è¿›ç¨‹çš„æ•°æ®åˆ†ç‰‡
                    processed_dataset_shard = processed_dataset.select(range(start_idx, end_idx))
                    
                    # ä½¿ç”¨æ‰¹é‡å¤„ç†å‡½æ•°ï¼ˆçœŸæ­£çš„æ‰¹é‡æ¨ç†ï¼Œæ›´å¿«ï¼‰
                    batch_tag_func = create_batch_tag_text_function(
                        tokenizer, llm, offline_token_head, local_rank, max_length, batch_size
                    )
                    
                    if batch_tag_func is not None:
                        def apply_tagging_batch(batch):
                            """æ‰¹é‡å¤„ç†æ ‡æ³¨ï¼ˆçœŸæ­£çš„æ‰¹é‡æ¨ç†ï¼‰"""
                            texts = batch.get("text", [])
                            if not texts:
                                return batch
                            
                            # ç¡®ä¿ texts æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
                            if isinstance(texts, list) and len(texts) > 0:
                                # æ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦æ˜¯å­—ç¬¦ä¸²
                                if not isinstance(texts[0], str):
                                    # å¦‚æœä¸æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢
                                    texts = [str(t) if t is not None else "" for t in texts]
                            elif not isinstance(texts, list):
                                # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                                texts = [str(texts)] if texts else []
                            
                            # æ‰¹é‡å¤„ç†
                            tagged_texts = batch_tag_func(texts)
                            # ç¡®ä¿è¿”å›çš„æ˜¯åˆ—è¡¨
                            if not isinstance(tagged_texts, list):
                                tagged_texts = [tagged_texts] if tagged_texts else []
                            batch["text"] = tagged_texts
                            return batch
                        
                        print(f"   Using batch size: {batch_size} (batch inference enabled)")
                        processed_dataset_shard = processed_dataset_shard.map(
                            apply_tagging_batch,
                            batched=True,
                            batch_size=batch_size,
                            num_proc=1,  # å•è¿›ç¨‹é¿å…CUDAé—®é¢˜
                        )
                        # æ¸…ç† GPU å†…å­˜
                        torch.cuda.empty_cache()
                        print(f"âœ… Offline tagging completed for shard {rank} ({len(processed_dataset_shard)} samples)")
                    
                    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼Œç¡®ä¿æ‰€æœ‰åˆ†ç‰‡éƒ½å¤„ç†å®Œæˆ
                    import torch.distributed as dist
                    if dist.is_initialized():
                        dist.barrier()
                        print(f"âœ… All processes completed offline tagging (rank {rank})")
                    
                    # æ¯ä¸ªè¿›ç¨‹ä¿å­˜è‡ªå·±çš„åˆ†ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åè®© rank 0 åˆå¹¶ä¿å­˜åˆ°ç¼“å­˜
                    if use_cache and tagged_cache_file:
                        # æ¯ä¸ªè¿›ç¨‹ä¿å­˜è‡ªå·±çš„åˆ†ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶
                        shard_cache_file = tagged_cache_file.replace(".jsonl", f"_shard_{rank}.jsonl")
                        os.makedirs(os.path.dirname(shard_cache_file) if os.path.dirname(shard_cache_file) else ".", exist_ok=True)
                        _save_dataset_to_jsonl(processed_dataset_shard, shard_cache_file, is_tagged=True)
                        print(f"ğŸ’¾ Rank {rank}: Saved shard to {shard_cache_file} ({len(processed_dataset_shard)} samples)")
                        
                        # åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰åˆ†ç‰‡éƒ½å·²ä¿å­˜
                        if dist.is_initialized():
                            dist.barrier()
                        
                        # rank 0 è´Ÿè´£æ”¶é›†æ‰€æœ‰åˆ†ç‰‡å¹¶åˆå¹¶ä¿å­˜åˆ°æœ€ç»ˆçš„ tagged cache
                        if rank == 0:
                            print(f"ğŸ’¾ Rank 0: Collecting all shards and saving to cache...")
                            all_shards = []
                            for r in range(world_size):
                                shard_file = tagged_cache_file.replace(".jsonl", f"_shard_{r}.jsonl")
                                if os.path.exists(shard_file):
                                    shard_dataset = load_dataset("json", data_files=shard_file, cache_dir="./cache", split="train", streaming=False)
                                    all_shards.append(shard_dataset)
                                    print(f"   Loaded shard {r}: {len(shard_dataset)} samples")
                            
                            if all_shards:
                                # åˆå¹¶æ‰€æœ‰åˆ†ç‰‡
                                from datasets import concatenate_datasets
                                merged_dataset = concatenate_datasets(all_shards)
                                print(f"   Merged {len(merged_dataset)} samples from {len(all_shards)} shards")
                                
                                # ä¿å­˜åˆ°æœ€ç»ˆçš„ tagged cache
                                _save_dataset_to_jsonl(merged_dataset, tagged_cache_file, is_tagged=True)
                                print(f"âœ… Tagged cache saved: {tagged_cache_file} ({len(merged_dataset)} samples, is_tagged=True)")
                                
                                # æ¸…ç†ä¸´æ—¶åˆ†ç‰‡æ–‡ä»¶
                                for r in range(world_size):
                                    shard_file = tagged_cache_file.replace(".jsonl", f"_shard_{r}.jsonl")
                                    if os.path.exists(shard_file):
                                        try:
                                            os.remove(shard_file)
                                            meta_file = shard_file + ".meta"
                                            if os.path.exists(meta_file):
                                                os.remove(meta_file)
                                        except Exception as e:
                                            print(f"âš ï¸  Failed to remove shard file {shard_file}: {e}")
                        
                        # å†æ¬¡åŒæ­¥ï¼Œç¡®ä¿ rank 0 å®Œæˆä¿å­˜
                        if dist.is_initialized():
                            dist.barrier()
                        
                        # å¦‚æœç¼“å­˜å·²ä¿å­˜ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½ä»ç¼“å­˜åŠ è½½å®Œæ•´æ•°æ®é›†
                        # è¿™æ ·æ¯ä¸ªè¿›ç¨‹éƒ½èƒ½çœ‹åˆ°å®Œæ•´çš„æ•°æ®ï¼Œè®­ç»ƒæ­¥æ•°æ‰ä¼šæ­£ç¡®
                        if os.path.exists(tagged_cache_file):
                            print(f"ğŸ“‚ Reloading full tagged dataset from cache for all processes... (rank {rank})")
                            cached_full = load_dataset("json", data_files=tagged_cache_file, cache_dir="./cache", split="train", streaming=False)
                            print(f"âœ… Loaded full dataset from cache: {len(cached_full)} samples (rank {rank})")
                            processed_dataset = cached_full
                        else:
                            # å¦‚æœç¼“å­˜ä¿å­˜å¤±è´¥ï¼Œä½¿ç”¨åˆ†ç‰‡ï¼ˆfallbackï¼‰
                            print(f"âš ï¸  Cache file not found after saving, using shard for rank {rank} ({len(processed_dataset_shard)} samples)")
                            processed_dataset = processed_dataset_shard
                    else:
                        # å¦‚æœæ²¡æœ‰ä¿å­˜ç¼“å­˜ï¼Œä½¿ç”¨åˆ†ç‰‡
                        print(f"âœ… Using processed shard for rank {rank} ({len(processed_dataset_shard)} samples)")
                        print(f"   Note: Cache not saved, using shard. DataLoader will handle data distribution.")
                        processed_dataset = processed_dataset_shard
                else:
                    # å•è¿›ç¨‹æ¨¡å¼ï¼Œå¤„ç†å…¨éƒ¨æ•°æ®
                    print(f"ğŸ”„ Applying offline tagging to add <mol> tags...")
                    
                    # ç¡®ä¿ LLM åœ¨ eval æ¨¡å¼å¹¶æ¸…ç†å†…å­˜
                    llm.eval()
                    torch.cuda.empty_cache()
                    
                    # ä½¿ç”¨æ‰¹é‡å¤„ç†å‡½æ•°ï¼ˆçœŸæ­£çš„æ‰¹é‡æ¨ç†ï¼Œæ›´å¿«ï¼‰
                    batch_tag_func = create_batch_tag_text_function(
                        tokenizer, llm, offline_token_head, local_rank, max_length, batch_size
                    )
                    
                    if batch_tag_func is not None:
                        def apply_tagging_batch(batch):
                            """æ‰¹é‡å¤„ç†æ ‡æ³¨ï¼ˆçœŸæ­£çš„æ‰¹é‡æ¨ç†ï¼‰"""
                            texts = batch.get("text", [])
                            if not texts:
                                return batch
                            
                            # ç¡®ä¿ texts æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
                            if isinstance(texts, list) and len(texts) > 0:
                                # æ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦æ˜¯å­—ç¬¦ä¸²
                                if not isinstance(texts[0], str):
                                    # å¦‚æœä¸æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢
                                    texts = [str(t) if t is not None else "" for t in texts]
                            elif not isinstance(texts, list):
                                # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                                texts = [str(texts)] if texts else []
                            
                            # æ‰¹é‡å¤„ç†
                            tagged_texts = batch_tag_func(texts)
                            # ç¡®ä¿è¿”å›çš„æ˜¯åˆ—è¡¨
                            if not isinstance(tagged_texts, list):
                                tagged_texts = [tagged_texts] if tagged_texts else []
                            batch["text"] = tagged_texts
                            return batch
                        
                        print(f"   Using batch size: {batch_size} (batch inference enabled)")
                        processed_dataset = processed_dataset.map(
                            apply_tagging_batch,
                            batched=True,
                            batch_size=batch_size,
                            num_proc=1,  # å•è¿›ç¨‹é¿å…CUDAé—®é¢˜
                        )
                        print("âœ… Offline tagging completed")
                        # ä¿å­˜taggedæ•°æ®åˆ°ç¼“å­˜ï¼ˆæ ‡è®°ä¸ºå·²æ ‡æ³¨ï¼‰
                        if use_cache and tagged_cache_file:
                            print(f"ğŸ’¾ Saving tagged data to cache: {tagged_cache_file}")
                            os.makedirs(os.path.dirname(tagged_cache_file) if os.path.dirname(tagged_cache_file) else ".", exist_ok=True)
                            _save_dataset_to_jsonl(processed_dataset, tagged_cache_file, is_tagged=True)
                            print(f"âœ… Tagged cache saved ({len(processed_dataset)} samples, is_tagged=True)")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    eval_split = cfg.get("train", {}).get("eval_split", 0.05)
    split = processed_dataset.train_test_split(
        test_size=eval_split,
        seed=cfg.get("seed", 42)
    )
    
    train_size = len(split["train"])
    eval_size = len(split["test"])
    print(f"ğŸ“ˆ Dataset split: {train_size} train, {eval_size} eval (split={eval_split})")
    
    if train_size == 0:
        raise ValueError(
            f"âŒ Training dataset is empty after splitting!\n"
            f"   Total samples: {len(processed_dataset)}\n"
            f"   Eval split: {eval_split}\n"
            f"   This might happen if eval_split is too large or dataset is too small"
        )
    
    # æ‰“å°æœ€ç»ˆè¿”å›çš„train_datasetçš„ç¬¬ä¸€æ¡æ•°æ®
    rank = int(os.environ.get("RANK", 0))
    if rank == 0 and len(split["train"]) > 0:
        print("\n" + "="*80)
        print("ğŸ“‹ First sample from train_dataset (final, before return):")
        print("="*80)
        first_train_sample = split["train"][0]
        print(f"Type: {type(first_train_sample)}")
        print(f"Content: {first_train_sample}")
        if isinstance(first_train_sample, dict):
            print(f"Keys: {list(first_train_sample.keys())}")
            for key, value in first_train_sample.items():
                if key == "text":
                    print(f"  {key}: type={type(value)}, length={len(str(value))}, preview={str(value)[:200]}...")
                else:
                    print(f"  {key}: type={type(value)}, value={str(value)[:200]}...")
        print("="*80 + "\n")
    
    return split["train"], split["test"]


def compute_qm9_stats_from_dataset(dataset) -> tuple:
    """ä»æ•°æ®é›†ä¸­è®¡ç®—QM9ç»Ÿè®¡ä¿¡æ¯"""
    tasks = ["mu", "alpha", "homo", "lumo", "gap"]
    sums = [0.0] * len(tasks)
    sqs = [0.0] * len(tasks)
    cnt = 0
    
    for ex in dataset:
        if ex.get("dataset") != "QM9" or ex.get("task_type") != "regression":
            continue
        at = ex.get("all_targets")
        if at is None:
            continue
        cnt += 1
        for i, t in enumerate(tasks):
            val = float(at.get(t, 0.0))
            sums[i] += val
            sqs[i] += val ** 2
    
    if cnt == 0:
        return None, None
    
    means = [s / cnt for s in sums]
    vars_ = [sq / cnt - m ** 2 for sq, m in zip(sqs, means)]
    stds = [max(1e-8, v) ** 0.5 for v in vars_]
    
    return means, stds


def clean_cached_data(cache_file: str, output_file: Optional[str] = None):
    """
    æ¸…ç†ç¼“å­˜æ•°æ®ä¸­çš„é”™è¯¯æ ‡æ³¨
    
    ä¿®å¤ä»¥ä¸‹é—®é¢˜ï¼š
    1. ç§»é™¤ç‰¹æ®Š token ä¸­çš„ <mol> æ ‡ç­¾ï¼ˆå¦‚ <|start_header_id|><mol>assistant</mol><|end_header_id|>ï¼‰
    2. æ¸…ç† "the question is" å’Œ "the answer is" å‰ç¼€ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    
    Args:
        cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™è¦†ç›–åŸæ–‡ä»¶ï¼‰
    """
    if not os.path.exists(cache_file):
        print(f"âŒ Cache file not found: {cache_file}")
        return

    print(f"ğŸ“‚ Loading cache file: {cache_file}")
    dataset = load_dataset("json", data_files=cache_file, cache_dir="./cache", split="train", streaming=False)
    print(f"ğŸ“Š Loaded {len(dataset)} samples")
    
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ä¸­çš„é”™è¯¯æ ‡æ³¨"""
        if not isinstance(text, str):
            return text
        
        # 1. ç§»é™¤ç‰¹æ®Š token ä¸­çš„ <mol> æ ‡ç­¾ï¼ˆä»…å¯¹æ—§çš„ Llama 3.2 æ ¼å¼ç¼“å­˜ç”Ÿæ•ˆï¼‰
        # ä¿®å¤ <|start_header_id|><mol>assistant</mol><|end_header_id|> -> <|start_header_id|>assistant<|end_header_id|>
        if "<|start_header_id|>" in text and "<|end_header_id|>" in text:
            text = re.sub(
                r'<\|start_header_id\|><mol>(assistant|user)</mol><\|end_header_id\|>',
                r'<|start_header_id|>\1<|end_header_id|>',
                text
            )
        
        # 2. æ¸…ç† "the question is" å’Œ "the answer is" å‰ç¼€
        # å¦‚æœæ–‡æœ¬ä¸­å·²ç»åŒ…å«æ ‡å‡†æ ¼å¼ï¼Œä½†è¿˜æœ‰è¿™äº›å‰ç¼€ï¼Œéœ€è¦ç§»é™¤å‰ç¼€å¹¶ä¿ç•™å®é™…å†…å®¹
        if "<|start_header_id|>assistant<|end_header_id|>" in text:
            # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼ŒæŸ¥æ‰¾ assistant éƒ¨åˆ†ä¸­çš„ "the question is" å’Œ "the answer is"
            # æå– assistant éƒ¨åˆ†
            assistant_match = re.search(
                r'<\|start_header_id\|>assistant<\|end_header_id\|>\s*\n\s*\n(.*?)(?:\s*<\|eot_id\|>|$)',
                text,
                re.DOTALL
            )
            if assistant_match:
                assistant_content = assistant_match.group(1)
                # æ£€æŸ¥æ˜¯å¦åŒ…å« "the question is" å’Œ "the answer is"
                pattern = r'the\s+question\s+is\s+(.+?)\s*,\s*the\s+answer\s+is\s+(.+?)(?:\s*<\|eot_id\|>|$)'
                match = re.search(pattern, assistant_content, re.IGNORECASE | re.DOTALL)
                if match:
                    # æå–å®é™…çš„ answer å†…å®¹ï¼ˆå¿½ç•¥ question éƒ¨åˆ†ï¼Œå› ä¸º question å·²ç»åœ¨ user éƒ¨åˆ†äº†ï¼‰
                    answer = match.group(2).strip()
                    # æ›¿æ¢ assistant éƒ¨åˆ†ï¼Œç§»é™¤ "the question is ... , the answer is" å‰ç¼€ï¼Œåªä¿ç•™ answer
                    # ä½¿ç”¨å­—ç¬¦ä¸²æ›¿æ¢è€Œä¸æ˜¯æ­£åˆ™è¡¨è¾¾å¼ï¼Œé¿å…è½¬ä¹‰é—®é¢˜
                    start_marker = "<|start_header_id|>assistant<|end_header_id|>\n\n"
                    end_marker = "<|eot_id|>"
                    start_idx = text.find(start_marker)
                    if start_idx != -1:
                        start_idx += len(start_marker)
                        end_idx = text.find(end_marker, start_idx)
                        if end_idx != -1:
                            # æ›¿æ¢ assistant å†…å®¹
                            text = text[:start_idx] + answer + text[end_idx:]
        else:
            # å¦‚æœæ²¡æœ‰æ ‡å‡†æ ¼å¼ï¼Œå°è¯•ä» "the question is" å’Œ "the answer is" æ„å»ºæ ‡å‡†æ ¼å¼
            pattern = r'the\s+question\s+is\s+(.+?)\s*,\s*the\s+answer\s+is\s+(.+?)(?:\s*<\|eot_id\|>|$)'
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                question = match.group(1).strip()
                answer = match.group(2).strip()
                # é‡æ–°æ ¼å¼åŒ–ä¸ºæ ‡å‡†çš„ Llama 3.2 æ ¼å¼
                text = f"<|start_header_id|>user<|end_header_id|>\n\n{question}<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n\n{answer}<|eot_id|>"
        
        return text
    
    def clean_example(example: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…ç†å•ä¸ªæ ·æœ¬"""
        if "text" in example:
            example["text"] = clean_text(example["text"])
        return example
    
    print("ğŸ§¹ Cleaning cached data...")
    cleaned_dataset = dataset.map(clean_example, num_proc=min(4, os.cpu_count() or 1))
    
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®
    if output_file is None:
        output_file = cache_file
    
    print(f"ğŸ’¾ Saving cleaned data to: {output_file}")
    _save_dataset_to_jsonl(cleaned_dataset, output_file, is_tagged=True)
    print(f"âœ… Cleaned cache saved ({len(cleaned_dataset)} samples)")


if __name__ == "__main__":
    """
    å‘½ä»¤è¡Œå·¥å…·ï¼šæ¸…ç†ç¼“å­˜æ•°æ®
    
    ç”¨æ³•ï¼š
        python -m modules.data_loader <cache_file> [output_file]
    """
    import sys
    if len(sys.argv) < 2:
        print("Usage: python -m modules.data_loader <cache_file> [output_file]")
        print("Example: python -m modules.data_loader ./cache/epoch2_preprocessed_tagged_offline_fa392044.jsonl")
        sys.exit(1)
    
    cache_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    clean_cached_data(cache_file, output_file)

